{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\deniz\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deniz\\AppData\\Local\\Temp\\ipykernel_12268\\386886616.py:22: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.fft import rfft\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import kerastuner as kt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "      <th>activity</th>\n",
       "      <th>patient_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-05-07 12:00:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-05-07 12:01:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>143</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-05-07 12:02:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-05-07 12:03:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>20</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-05-07 12:04:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>166</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571701</th>\n",
       "      <td>2003-12-01 12:53:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>7</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571702</th>\n",
       "      <td>2003-12-01 12:54:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>7</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571703</th>\n",
       "      <td>2003-12-01 12:55:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571704</th>\n",
       "      <td>2003-12-01 12:56:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571705</th>\n",
       "      <td>2003-12-01 12:57:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>7</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1571706 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp        date  activity patient_name  label\n",
       "0        2003-05-07 12:00:00  2003-05-07         0  condition_1      1\n",
       "1        2003-05-07 12:01:00  2003-05-07       143  condition_1      1\n",
       "2        2003-05-07 12:02:00  2003-05-07         0  condition_1      1\n",
       "3        2003-05-07 12:03:00  2003-05-07        20  condition_1      1\n",
       "4        2003-05-07 12:04:00  2003-05-07       166  condition_1      1\n",
       "...                      ...         ...       ...          ...    ...\n",
       "1571701  2003-12-01 12:53:00  2003-12-01         7    control_9      0\n",
       "1571702  2003-12-01 12:54:00  2003-12-01         7    control_9      0\n",
       "1571703  2003-12-01 12:55:00  2003-12-01         5    control_9      0\n",
       "1571704  2003-12-01 12:56:00  2003-12-01         5    control_9      0\n",
       "1571705  2003-12-01 12:57:00  2003-12-01         7    control_9      0\n",
       "\n",
       "[1571706 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_conditions = 'data\\depresjon\\condition'\n",
    "conditions = glob.glob(path_conditions + \"/*.csv\")\n",
    "\n",
    "path_controls = 'data\\depresjon\\control'\n",
    "controls = glob.glob(path_controls + \"/*.csv\")\n",
    "\n",
    "df_scores = pd.read_csv(\"data\\depresjon\\scores.csv\")\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "all_list = []\n",
    "\n",
    "# Loop through the list of csv files\n",
    "for filename in conditions:\n",
    "    # Read the current CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    
    "    name = os.path.splitext(os.path.basename(filename))[0]  
    "    df['patient_name'] = name  # Add patient_name as a new column\n",
    "    df['label'] = 1\n",
    "    # Append the DataFrame to the list\n",
    "    all_list.append(df)\n",
    "\n",
    "for filename in controls:\n",
    "    # Read the current CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    
    "    name = os.path.splitext(os.path.basename(filename))[0]  
    "    df['patient_name'] = name  # Add patient_name as a new column\n",
    "    df['label'] = 0\n",
    "    # Append the DataFrame to the list\n",
    "    all_list.append(df)\n",
    "\n",
    "\n",
    "# Combine all DataFrames in the list into a single DataFrame\n",
    "combined_df = pd.concat(all_list, ignore_index=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the dataset\n",
    "# Ensuring no NaN/infinite values in the dataset\n",
    "combined_df.fillna(0, inplace=True)\n",
    "combined_df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df, combined_df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to segment the data with 48-hour segments\n",
    "def segment_data(df, segment_size_minutes=2880):\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for name in df['patient_name'].unique():\n",
    "        patient_data = df[df['patient_name'] == name]\n",
    "        for start_pos in range(0, len(patient_data) - segment_size_minutes + 1, segment_size_minutes):\n",
    "            segment = patient_data['activity'].iloc[start_pos:start_pos + segment_size_minutes].values\n",
    "            label = patient_data['label'].iloc[start_pos]  # Assuming label is constant within each segment\n",
    "            segments.append(segment)\n",
    "            labels.append(label)\n",
    "    return np.array(segments), np.array(labels)\n",
    "\n",
    "# Segment the training and testing data separately\n",
    "X_train_segments, y_train_segments = segment_data(X_train)\n",
    "X_test_segments, y_test_segments = segment_data(X_test)\n",
    "\n",
    "# Normalize the data after segmentation\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_segments)\n",
    "X_test_normalized = scaler.transform(X_test_segments)\n",
    "\n",
    "# Adjusting the shape for CNN input\n",
    "X_train_processed = X_train_normalized.reshape((-1, X_train_normalized.shape[1], 1))\n",
    "X_test_processed = X_test_normalized.reshape((-1, X_test_normalized.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE for addressing class imbalance\n",
    "# Flatten the X_train_processed array\n",
    "X_train_flattened = X_train_processed.reshape(X_train_processed.shape[0], -1)\n",
    "smote = SMOTE()\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_flattened, y_train_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\intro_to_kt\\tuner0.json\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 2s 32ms/step - loss: 0.6463 - accuracy: 0.6145 - precision_1: 0.2727 - recall_1: 0.0188 - val_loss: 0.8440 - val_accuracy: 0.0000e+00 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.5820 - accuracy: 0.6636 - precision_1: 1.0000 - recall_1: 0.1000 - val_loss: 0.7239 - val_accuracy: 0.4630 - val_precision_1: 1.0000 - val_recall_1: 0.4630\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.5437 - accuracy: 0.6939 - precision_1: 0.9394 - recall_1: 0.1937 - val_loss: 0.5539 - val_accuracy: 0.9630 - val_precision_1: 1.0000 - val_recall_1: 0.9630\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.5006 - accuracy: 0.7687 - precision_1: 0.9296 - recall_1: 0.4125 - val_loss: 0.6113 - val_accuracy: 0.8426 - val_precision_1: 1.0000 - val_recall_1: 0.8426\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.4496 - accuracy: 0.8645 - precision_1: 0.9636 - recall_1: 0.6625 - val_loss: 0.4607 - val_accuracy: 0.9815 - val_precision_1: 1.0000 - val_recall_1: 0.9815\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "Accuracy: 0.654320987654321\n",
      "Precision: 0.4642857142857143\n",
      "Recall: 0.5\n",
      "F1 Score: 0.4814814814814815\n",
      "Confusion Matrix:\n",
      " [[40 15]\n",
      " [13 13]]\n"
     ]
    }
   ],
   "source": [
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=hp.Int('filters1', min_value=16, max_value=64, step=16),\n",
    "                     kernel_size=hp.Choice('kernel_size1', values=[3, 5]),\n",
    "                     activation='relu',\n",
    "                     input_shape=(X_train_processed.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(rate=hp.Float('dropout1', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Additional convolutional layer\n",
    "    model.add(Conv1D(filters=hp.Int('filters2', min_value=16, max_value=64, step=16),\n",
    "                     kernel_size=hp.Choice('kernel_size2', values=[3, 5]),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(rate=hp.Float('dropout2', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Additional convolutional layer\n",
    "    model.add(Conv1D(filters=hp.Int('filters3', min_value=16, max_value=64, step=16),\n",
    "                     kernel_size=hp.Choice('kernel_size3', values=[3, 5]),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(rate=hp.Float('dropout3', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Include gradient clipping in the optimizer\n",
    "    optimizer = Adam(\n",
    "        hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]),\n",
    "        clipnorm=hp.Choice('clipnorm', values=[0.5, 1.0, 1.5])\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', Precision(), Recall()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=5,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "# Create a callback to stop training early after reaching a certain value for the validation loss\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Execute the hyperparameter search\n",
    "tuner.search(X_train_balanced, y_train_balanced, epochs=5, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Retrieve the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model\n",
    "history = model.fit(X_train_balanced, y_train_balanced, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_pred_prob = model.predict(X_test_processed)\n",
    "\n",
    "# Convert probabilities to binary predictions based on a threshold\n",
    "threshold = 0.5  # You can adjust this threshold if needed\n",
    "y_pred = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test_segments, y_pred)\n",
    "precision = precision_score(y_test_segments, y_pred)\n",
    "recall = recall_score(y_test_segments, y_pred)\n",
    "f1 = f1_score(y_test_segments, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test_segments, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.46153846153846156\n",
      "Specificity: 0.7636363636363637\n",
      "AUC: 0.6475524475524477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_test_segments, y_pred_prob)\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "fpr, tpr, thresholds = roc_curve(y_test_segments, y_pred_prob)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "y_pred_binary = (y_pred_prob > optimal_threshold).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_segments, y_pred_binary).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "print(\"AUC:\", auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
