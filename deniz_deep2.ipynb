{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\deniz\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deniz\\AppData\\Local\\Temp\\ipykernel_9404\\3594687158.py:21: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.fft import rfft\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import kerastuner as kt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "      <th>activity</th>\n",
       "      <th>patient_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-05-07 12:00:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-05-07 12:01:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>143</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-05-07 12:02:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-05-07 12:03:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>20</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-05-07 12:04:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>166</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571701</th>\n",
       "      <td>2003-12-01 12:53:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>7</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571702</th>\n",
       "      <td>2003-12-01 12:54:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>7</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571703</th>\n",
       "      <td>2003-12-01 12:55:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571704</th>\n",
       "      <td>2003-12-01 12:56:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571705</th>\n",
       "      <td>2003-12-01 12:57:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>7</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1571706 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp        date  activity patient_name  label\n",
       "0        2003-05-07 12:00:00  2003-05-07         0  condition_1      1\n",
       "1        2003-05-07 12:01:00  2003-05-07       143  condition_1      1\n",
       "2        2003-05-07 12:02:00  2003-05-07         0  condition_1      1\n",
       "3        2003-05-07 12:03:00  2003-05-07        20  condition_1      1\n",
       "4        2003-05-07 12:04:00  2003-05-07       166  condition_1      1\n",
       "...                      ...         ...       ...          ...    ...\n",
       "1571701  2003-12-01 12:53:00  2003-12-01         7    control_9      0\n",
       "1571702  2003-12-01 12:54:00  2003-12-01         7    control_9      0\n",
       "1571703  2003-12-01 12:55:00  2003-12-01         5    control_9      0\n",
       "1571704  2003-12-01 12:56:00  2003-12-01         5    control_9      0\n",
       "1571705  2003-12-01 12:57:00  2003-12-01         7    control_9      0\n",
       "\n",
       "[1571706 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_conditions = 'data\\depresjon\\condition'\n",
    "conditions = glob.glob(path_conditions + \"/*.csv\")\n",
    "\n",
    "path_controls = 'data\\depresjon\\control'\n",
    "controls = glob.glob(path_controls + \"/*.csv\")\n",
    "\n",
    "df_scores = pd.read_csv(\"data\\depresjon\\scores.csv\")\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "all_list = []\n",
    "\n",
    "# Loop through the list of csv files\n",
    "for filename in conditions:\n",
    "    # Read the current CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Optionally, extract patient_name or other identifiers from filename if needed\n",
    "    # For example, if the filename contains the patient_name:\n",
    "    name = os.path.splitext(os.path.basename(filename))[0]  # Adjust based on your file naming convention and operating system\n",
    "    df['patient_name'] = name  # Add patient_name as a new column\n",
    "    df['label'] = 1\n",
    "    # Append the DataFrame to the list\n",
    "    all_list.append(df)\n",
    "\n",
    "for filename in controls:\n",
    "    # Read the current CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Optionally, extract patient_name or other identifiers from filename if needed\n",
    "    # For example, if the filename contains the patient_name:\n",
    "    name = os.path.splitext(os.path.basename(filename))[0]  # Adjust based on your file naming convention and operating system\n",
    "    df['patient_name'] = name  # Add patient_name as a new column\n",
    "    df['label'] = 0\n",
    "    # Append the DataFrame to the list\n",
    "    all_list.append(df)\n",
    "\n",
    "\n",
    "# Combine all DataFrames in the list into a single DataFrame\n",
    "combined_df = pd.concat(all_list, ignore_index=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the dataset\n",
    "# Ensuring no NaN/infinite values in the dataset\n",
    "combined_df.fillna(0, inplace=True)\n",
    "combined_df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df, combined_df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to segment the data with 48-hour segments\n",
    "def segment_data(df, segment_size_minutes=2880):\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for name in df['patient_name'].unique():\n",
    "        patient_data = df[df['patient_name'] == name]\n",
    "        for start_pos in range(0, len(patient_data) - segment_size_minutes + 1, segment_size_minutes):\n",
    "            segment = patient_data['activity'].iloc[start_pos:start_pos + segment_size_minutes].values\n",
    "            label = patient_data['label'].iloc[start_pos]  # Assuming label is constant within each segment\n",
    "            segments.append(segment)\n",
    "            labels.append(label)\n",
    "    return np.array(segments), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the training and testing data separately\n",
    "X_train_segments, y_train_segments = segment_data(X_train)\n",
    "X_test_segments, y_test_segments = segment_data(X_test)\n",
    "\n",
    "# Normalize the data after segmentation\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_segments)\n",
    "X_test_normalized = scaler.transform(X_test_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Fourier Transform of each segment\n",
    "def calculate_fourier_transform(segment):\n",
    "    # Compute the real Fast Fourier Transform\n",
    "    fft_values = rfft(segment)\n",
    "    # Compute the absolute values (magnitudes) of the FFT\n",
    "    fft_magnitude = np.abs(fft_values)\n",
    "    return fft_magnitude\n",
    "\n",
    "# Function to extract statistical features from each segment\n",
    "def extract_statistical_features(segment):\n",
    "    features = {\n",
    "        'mean': np.mean(segment),\n",
    "        'std': np.std(segment),\n",
    "        'skew': skew(segment),\n",
    "        'kurtosis': kurtosis(segment)\n",
    "    }\n",
    "    return np.array(list(features.values()))\n",
    "\n",
    "# Function to process each segment, combining FFT and statistical features\n",
    "def process_segment(segment):\n",
    "    fft_magnitude = calculate_fourier_transform(segment)\n",
    "    statistical_features = extract_statistical_features(segment)\n",
    "    # Combine FFT magnitudes with statistical features\n",
    "    combined_features = np.concatenate([fft_magnitude, statistical_features])\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature extraction to each segment\n",
    "X_train_processed = np.array([process_segment(segment) for segment in X_train_normalized])\n",
    "X_test_processed = np.array([process_segment(segment) for segment in X_test_normalized])\n",
    "\n",
    "# Adjusting the shape for CNN input\n",
    "X_train_processed = X_train_processed.reshape((-1, X_train_processed.shape[1], 1))\n",
    "X_test_processed = X_test_processed.reshape((-1, X_test_processed.shape[1], 1))\n",
    "\n",
    "# At this point, the data is ready to be fed into a CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to address imbalance\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=hp.Int('filters', min_value=16, max_value=64, step=16),\n",
    "                     kernel_size=hp.Choice('kernel_size', values=[3, 5]),\n",
    "                     activation='relu',\n",
    "                     input_shape=(X_train_processed.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Include gradient clipping in the optimizer\n",
    "    optimizer = Adam(\n",
    "        hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]),\n",
    "        clipnorm=hp.Choice('clipnorm', values=[0.5, 1.0, 1.5])\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', Precision(), Recall()])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\intro_to_kt\\tuner0.json\n",
      "WARNING:tensorflow:From c:\\Users\\deniz\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\deniz\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\deniz\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\deniz\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "524/524 [==============================] - 3s 4ms/step - loss: 0.7399 - accuracy: 0.4637 - precision: 0.3480 - recall: 0.5940 - val_loss: 0.6903 - val_accuracy: 0.6570 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/5\n",
      "524/524 [==============================] - 2s 4ms/step - loss: 0.6941 - accuracy: 0.4720 - precision: 0.3433 - recall: 0.5427 - val_loss: 0.6924 - val_accuracy: 0.6570 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/5\n",
      "524/524 [==============================] - 3s 6ms/step - loss: 0.6941 - accuracy: 0.4622 - precision: 0.3532 - recall: 0.6293 - val_loss: 0.6966 - val_accuracy: 0.3430 - val_precision: 0.3430 - val_recall: 1.0000\n",
      "Epoch 4/5\n",
      "524/524 [==============================] - 3s 6ms/step - loss: 0.6939 - accuracy: 0.4729 - precision: 0.3559 - recall: 0.6089 - val_loss: 0.6842 - val_accuracy: 0.6570 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/5\n",
      "524/524 [==============================] - 3s 5ms/step - loss: 0.6939 - accuracy: 0.4949 - precision: 0.3591 - recall: 0.5485 - val_loss: 0.7028 - val_accuracy: 0.3430 - val_precision: 0.3430 - val_recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=5,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "# Create a callback to stop training early after reaching a certain value for the validation loss\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Execute the hyperparameter search\n",
    "tuner.search(X_train_processed, y_train, epochs=5, validation_split=0.2, callbacks=[stop_early], class_weight=class_weight_dict)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model\n",
    "history = model.fit(X_train_processed, y_train, epochs=10, validation_split=0.2, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.3482262703739214\n",
      "Precision: 0.3482262703739214\n",
      "Recall: 1.0\n",
      "F1 Score: 0.5165694780258854\n",
      "Confusion Matrix:\n",
      " [[   0 3399]\n",
      " [   0 1816]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_pred_prob = model.predict(X_test_processed)\n",
    "\n",
    "# Convert probabilities to binary predictions based on a threshold\n",
    "threshold = 0.5  # You can adjust this threshold if needed\n",
    "y_pred = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test_segments, y_pred)\n",
    "precision = precision_score(y_test_segments, y_pred)\n",
    "recall = recall_score(y_test_segments, y_pred)\n",
    "f1 = f1_score(y_test_segments, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test_segments, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the probability scores for each class and then convert them to binary predictions based on a threshold. use the threshold of 0.5 to convert the probability scores to binary predictions. Adjust the threshold according to your preference or based on the trade-off between precision and recall that you want to achieve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
