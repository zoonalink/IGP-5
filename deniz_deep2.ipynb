{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\deniz\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deniz\\AppData\\Local\\Temp\\ipykernel_21600\\3594687158.py:21: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.fft import rfft\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import kerastuner as kt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "      <th>activity</th>\n",
       "      <th>patient_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-05-07 12:00:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-05-07 12:01:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>143</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-05-07 12:02:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-05-07 12:03:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>20</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-05-07 12:04:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>166</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571701</th>\n",
       "      <td>2003-12-01 12:53:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>7</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571702</th>\n",
       "      <td>2003-12-01 12:54:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>7</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571703</th>\n",
       "      <td>2003-12-01 12:55:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571704</th>\n",
       "      <td>2003-12-01 12:56:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571705</th>\n",
       "      <td>2003-12-01 12:57:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>7</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1571706 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp        date  activity patient_name  label\n",
       "0        2003-05-07 12:00:00  2003-05-07         0  condition_1      1\n",
       "1        2003-05-07 12:01:00  2003-05-07       143  condition_1      1\n",
       "2        2003-05-07 12:02:00  2003-05-07         0  condition_1      1\n",
       "3        2003-05-07 12:03:00  2003-05-07        20  condition_1      1\n",
       "4        2003-05-07 12:04:00  2003-05-07       166  condition_1      1\n",
       "...                      ...         ...       ...          ...    ...\n",
       "1571701  2003-12-01 12:53:00  2003-12-01         7    control_9      0\n",
       "1571702  2003-12-01 12:54:00  2003-12-01         7    control_9      0\n",
       "1571703  2003-12-01 12:55:00  2003-12-01         5    control_9      0\n",
       "1571704  2003-12-01 12:56:00  2003-12-01         5    control_9      0\n",
       "1571705  2003-12-01 12:57:00  2003-12-01         7    control_9      0\n",
       "\n",
       "[1571706 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_conditions = 'data\\depresjon\\condition'\n",
    "conditions = glob.glob(path_conditions + \"/*.csv\")\n",
    "\n",
    "path_controls = 'data\\depresjon\\control'\n",
    "controls = glob.glob(path_controls + \"/*.csv\")\n",
    "\n",
    "df_scores = pd.read_csv(\"data\\depresjon\\scores.csv\")\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "all_list = []\n",
    "\n",
    "# Loop through the list of csv files\n",
    "for filename in conditions:\n",
    "    # Read the current CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Optionally, extract patient_name or other identifiers from filename if needed\n",
    "    # For example, if the filename contains the patient_name:\n",
    "    name = os.path.splitext(os.path.basename(filename))[0]  # Adjust based on your file naming convention and operating system\n",
    "    df['patient_name'] = name  # Add patient_name as a new column\n",
    "    df['label'] = 1\n",
    "    # Append the DataFrame to the list\n",
    "    all_list.append(df)\n",
    "\n",
    "for filename in controls:\n",
    "    # Read the current CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Optionally, extract patient_name or other identifiers from filename if needed\n",
    "    # For example, if the filename contains the patient_name:\n",
    "    name = os.path.splitext(os.path.basename(filename))[0]  # Adjust based on your file naming convention and operating system\n",
    "    df['patient_name'] = name  # Add patient_name as a new column\n",
    "    df['label'] = 0\n",
    "    # Append the DataFrame to the list\n",
    "    all_list.append(df)\n",
    "\n",
    "\n",
    "# Combine all DataFrames in the list into a single DataFrame\n",
    "combined_df = pd.concat(all_list, ignore_index=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the dataset\n",
    "# Ensuring no NaN/infinite values in the dataset\n",
    "combined_df.fillna(0, inplace=True)\n",
    "combined_df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df, combined_df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to segment the data with 48-hour segments\n",
    "def segment_data(df, segment_size_minutes=2880):\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for name in df['patient_name'].unique():\n",
    "        patient_data = df[df['patient_name'] == name]\n",
    "        for start_pos in range(0, len(patient_data) - segment_size_minutes + 1, segment_size_minutes):\n",
    "            segment = patient_data['activity'].iloc[start_pos:start_pos + segment_size_minutes].values\n",
    "            label = patient_data['label'].iloc[start_pos]  # Assuming label is constant within each segment\n",
    "            segments.append(segment)\n",
    "            labels.append(label)\n",
    "    return np.array(segments), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the training and testing data separately\n",
    "X_train_segments, y_train_segments = segment_data(X_train)\n",
    "X_test_segments, y_test_segments = segment_data(X_test)\n",
    "\n",
    "# Normalize the data after segmentation\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_segments)\n",
    "X_test_normalized = scaler.transform(X_test_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the shape for CNN input\n",
    "X_train_processed = X_train_normalized.reshape((-1, X_train_normalized.shape[1], 1))\n",
    "X_test_processed = X_test_normalized.reshape((-1, X_test_normalized.shape[1], 1))\n",
    "\n",
    "# At this point, the data is ready to be fed into a CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to address imbalance\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=hp.Int('filters', min_value=16, max_value=64, step=16),\n",
    "                     kernel_size=hp.Choice('kernel_size', values=[3, 5]),\n",
    "                     activation='relu',\n",
    "                     input_shape=(X_train_processed.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Include gradient clipping in the optimizer\n",
    "    optimizer = Adam(\n",
    "        hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]),\n",
    "        clipnorm=hp.Choice('clipnorm', values=[0.5, 1.0, 1.5])\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', Precision(), Recall()])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\intro_to_kt\\tuner0.json\n",
      "WARNING:tensorflow:From c:\\Users\\deniz\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\deniz\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\deniz\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\deniz\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "11/11 [==============================] - 3s 148ms/step - loss: 10.3211 - accuracy: 0.4862 - precision: 0.3108 - recall: 0.4107 - val_loss: 1.2235 - val_accuracy: 0.4146 - val_precision: 0.4146 - val_recall: 1.0000\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 2.0860 - accuracy: 0.5229 - precision: 0.3706 - recall: 0.5625 - val_loss: 1.4327 - val_accuracy: 0.5854 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 1s 128ms/step - loss: 0.4867 - accuracy: 0.7584 - precision: 0.6341 - recall: 0.6964 - val_loss: 0.9348 - val_accuracy: 0.4390 - val_precision: 0.4143 - val_recall: 0.8529\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 1s 128ms/step - loss: 0.2010 - accuracy: 0.9358 - precision: 0.8824 - recall: 0.9375 - val_loss: 1.1141 - val_accuracy: 0.5854 - val_precision: 0.5000 - val_recall: 0.0588\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.0673 - accuracy: 0.9878 - precision: 0.9821 - recall: 0.9821 - val_loss: 1.3266 - val_accuracy: 0.5488 - val_precision: 0.4000 - val_recall: 0.1765\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=5,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "# Create a callback to stop training early after reaching a certain value for the validation loss\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Execute the hyperparameter search\n",
    "tuner.search(X_train_processed, y_train, epochs=5, validation_split=0.2, callbacks=[stop_early], class_weight=class_weight_dict)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model\n",
    "history = model.fit(X_train_processed, y_train, epochs=5, validation_split=0.2, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 14ms/step\n",
      "Accuracy: 0.6296296296296297\n",
      "Precision: 0.3333333333333333\n",
      "Recall: 0.15384615384615385\n",
      "F1 Score: 0.21052631578947367\n",
      "Confusion Matrix:\n",
      " [[47  8]\n",
      " [22  4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_pred_prob = model.predict(X_test_processed)\n",
    "\n",
    "# Convert probabilities to binary predictions based on a threshold\n",
    "threshold = 0.5  # You can adjust this threshold if needed\n",
    "y_pred = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test_segments, y_pred)\n",
    "precision = precision_score(y_test_segments, y_pred)\n",
    "recall = recall_score(y_test_segments, y_pred)\n",
    "f1 = f1_score(y_test_segments, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test_segments, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
