{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\deniz\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deniz\\AppData\\Local\\Temp\\ipykernel_19680\\3594687158.py:21: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.fft import rfft\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import kerastuner as kt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "      <th>activity</th>\n",
       "      <th>patient_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-05-07 12:00:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-05-07 12:01:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>143</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-05-07 12:02:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>0</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-05-07 12:03:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>20</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-05-07 12:04:00</td>\n",
       "      <td>2003-05-07</td>\n",
       "      <td>166</td>\n",
       "      <td>condition_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571701</th>\n",
       "      <td>2003-12-01 12:53:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>7</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571702</th>\n",
       "      <td>2003-12-01 12:54:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>7</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571703</th>\n",
       "      <td>2003-12-01 12:55:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571704</th>\n",
       "      <td>2003-12-01 12:56:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571705</th>\n",
       "      <td>2003-12-01 12:57:00</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>7</td>\n",
       "      <td>control_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1571706 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp        date  activity patient_name  label\n",
       "0        2003-05-07 12:00:00  2003-05-07         0  condition_1      1\n",
       "1        2003-05-07 12:01:00  2003-05-07       143  condition_1      1\n",
       "2        2003-05-07 12:02:00  2003-05-07         0  condition_1      1\n",
       "3        2003-05-07 12:03:00  2003-05-07        20  condition_1      1\n",
       "4        2003-05-07 12:04:00  2003-05-07       166  condition_1      1\n",
       "...                      ...         ...       ...          ...    ...\n",
       "1571701  2003-12-01 12:53:00  2003-12-01         7    control_9      0\n",
       "1571702  2003-12-01 12:54:00  2003-12-01         7    control_9      0\n",
       "1571703  2003-12-01 12:55:00  2003-12-01         5    control_9      0\n",
       "1571704  2003-12-01 12:56:00  2003-12-01         5    control_9      0\n",
       "1571705  2003-12-01 12:57:00  2003-12-01         7    control_9      0\n",
       "\n",
       "[1571706 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_conditions = 'data\\depresjon\\condition'\n",
    "conditions = glob.glob(path_conditions + \"/*.csv\")\n",
    "\n",
    "path_controls = 'data\\depresjon\\control'\n",
    "controls = glob.glob(path_controls + \"/*.csv\")\n",
    "\n",
    "df_scores = pd.read_csv(\"data\\depresjon\\scores.csv\")\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "all_list = []\n",
    "\n",
    "# Loop through the list of csv files\n",
    "for filename in conditions:\n",
    "    # Read the current CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Optionally, extract patient_name or other identifiers from filename if needed\n",
    "    # For example, if the filename contains the patient_name:\n",
    "    name = os.path.splitext(os.path.basename(filename))[0]  # Adjust based on your file naming convention and operating system\n",
    "    df['patient_name'] = name  # Add patient_name as a new column\n",
    "    df['label'] = 1\n",
    "    # Append the DataFrame to the list\n",
    "    all_list.append(df)\n",
    "\n",
    "for filename in controls:\n",
    "    # Read the current CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Optionally, extract patient_name or other identifiers from filename if needed\n",
    "    # For example, if the filename contains the patient_name:\n",
    "    name = os.path.splitext(os.path.basename(filename))[0]  # Adjust based on your file naming convention and operating system\n",
    "    df['patient_name'] = name  # Add patient_name as a new column\n",
    "    df['label'] = 0\n",
    "    # Append the DataFrame to the list\n",
    "    all_list.append(df)\n",
    "\n",
    "\n",
    "# Combine all DataFrames in the list into a single DataFrame\n",
    "combined_df = pd.concat(all_list, ignore_index=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "Feature engineering for time-series or activity data before applying a CNN can involve several steps, aiming to highlight aspects of the data that are indicative of the outcome you're trying to predict (in this case, depression). Here are some common feature engineering techniques for time-series data:\n",
    "\n",
    "**Normalization/Standardization:** As CNNs can be sensitive to the scale of the input data, it's standard practice to normalize or standardize your features so that they're on a similar scale. This usually means subtracting the mean and dividing by the standard deviation (Z-score normalization).\n",
    "\n",
    "**Windowing:** If your dataset consists of continuous time-series data, you might want to split it into smaller fixed-size sequences or windows. This is especially relevant if you're dealing with long sequences.\n",
    "\n",
    "**Denoising:** If the data is noisy, applying a smoothing filter or a denoising algorithm can help to reduce noise and make patterns more discernible.\n",
    "\n",
    "**Feature Extraction:** Extracting statistical features from time windows can be useful, especially if there's a risk of losing important information through windowing alone. Common features include the mean, standard deviation, skewness, kurtosis, and higher-order moments of the data within a window.\n",
    "\n",
    "**Fourier Transforms or Spectral Analysis:** For data with periodic features, applying a Fourier transform can help to identify dominant frequency components, which might be relevant for distinguishing between different states, such as depressed vs. non-depressed.\n",
    "\n",
    "**Wavelet Transforms:** These can be used to decompose time-series data into time-frequency space, capturing both temporal and frequency information.\n",
    "\n",
    "**Domain-Specific Features:** Depending on the specifics of your activity data, there may be particular measures that are relevant to depression. For instance, the amount of activity or the variability of activity levels could be significant.\n",
    "\n",
    "**Dimensionality Reduction:** Techniques like PCA (Principal Component Analysis) can be applied to reduce the dimensionality of the feature space while retaining most of the variability in the data.\n",
    "\n",
    "**Correlation Analysis:** Analyzing the correlation between different features can provide insights and help in selecting the most relevant features for your model.\n",
    "\n",
    "**Balancing the Dataset:** If the dataset is imbalanced with respect to the target classes, you might want to consider resampling techniques to balance the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the dataset\n",
    "# Ensuring no NaN/infinite values in the dataset\n",
    "combined_df.fillna(0, inplace=True)\n",
    "combined_df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "# Normalize the activity data\n",
    "scaler = StandardScaler()\n",
    "combined_df['activity_normalized'] = scaler.fit_transform(combined_df[['activity']])\n",
    "\n",
    "# Function to segment the data\n",
    "def segment_data(df, segment_size=60):\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for name in df['patient_name'].unique():\n",
    "        patient_data = df[df['patient_name'] == name]\n",
    "        for start_pos in range(0, len(patient_data) - segment_size + 1, segment_size):\n",
    "            segment = patient_data['activity_normalized'].iloc[start_pos:start_pos + segment_size].values\n",
    "            label = patient_data['label'].iloc[start_pos]  # Assuming label is constant within each segment\n",
    "            segments.append(segment)\n",
    "            labels.append(label)\n",
    "    return np.array(segments), np.array(labels)\n",
    "\n",
    "# Segmenting the data\n",
    "segment_size = 60  # Example segment size\n",
    "segments, labels = segment_data(combined_df, segment_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26166, 35)\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the Fourier Transform of each segment\n",
    "def calculate_fourier_transform(segment):\n",
    "    # Compute the real Fast Fourier Transform\n",
    "    fft_values = rfft(segment)\n",
    "    # Compute the absolute values (magnitudes) of the FFT\n",
    "    fft_magnitude = np.abs(fft_values)\n",
    "    return fft_magnitude\n",
    "\n",
    "# Function to extract statistical features from each segment\n",
    "def extract_statistical_features(segment):\n",
    "    features = {\n",
    "        'mean': np.mean(segment),\n",
    "        'std': np.std(segment),\n",
    "        'skew': skew(segment),\n",
    "        'kurtosis': kurtosis(segment)\n",
    "    }\n",
    "    return np.array(list(features.values()))\n",
    "\n",
    "# Function to process each segment, combining FFT and statistical features\n",
    "def process_segment(segment):\n",
    "    fft_magnitude = calculate_fourier_transform(segment)\n",
    "    statistical_features = extract_statistical_features(segment)\n",
    "    # Combine FFT magnitudes with statistical features\n",
    "    combined_features = np.concatenate([fft_magnitude, statistical_features])\n",
    "    return combined_features\n",
    "\n",
    "# Apply feature extraction to each segment\n",
    "features = np.array([process_segment(segment) for segment in segments])\n",
    "\n",
    "# Since the feature extraction might change the shape, let's check and adjust the data shape for CNN input\n",
    "print(features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Adjusting the shape for CNN input\n",
    "X_train = X_train.reshape((-1, X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((-1, X_test.shape[1], 1))\n",
    "\n",
    "# At this point, the data is ready to be fed into a CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\intro_to_kt\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights to address imbalance\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=hp.Int('filters', min_value=16, max_value=64, step=16),\n",
    "                     kernel_size=hp.Choice('kernel_size', values=[3, 5]),\n",
    "                     activation='relu',\n",
    "                     input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Include gradient clipping in the optimizer\n",
    "    optimizer = Adam(\n",
    "        hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]),\n",
    "        clipnorm=hp.Choice('clipnorm', values=[0.5, 1.0, 1.5])\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', Precision(), Recall()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=5,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "# Create a callback to stop training early after reaching a certain value for the validation loss\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "524/524 [==============================] - 3s 4ms/step - loss: nan - accuracy: 0.6484 - precision_7: 0.2759 - recall_7: 0.0014 - val_loss: nan - val_accuracy: 0.6556 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00\n",
      "Epoch 2/5\n",
      "524/524 [==============================] - 2s 4ms/step - loss: nan - accuracy: 0.6491 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: nan - val_accuracy: 0.6556 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00\n",
      "Epoch 3/5\n",
      "524/524 [==============================] - 2s 4ms/step - loss: nan - accuracy: 0.6491 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: nan - val_accuracy: 0.6556 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00\n",
      "Epoch 4/5\n",
      "524/524 [==============================] - 3s 5ms/step - loss: nan - accuracy: 0.6491 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: nan - val_accuracy: 0.6556 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00\n",
      "Epoch 5/5\n",
      "524/524 [==============================] - 2s 4ms/step - loss: nan - accuracy: 0.6491 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: nan - val_accuracy: 0.6556 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Execute the hyperparameter search\n",
    "tuner.search(X_train, y_train, epochs=5, validation_split=0.2, callbacks=[stop_early], class_weight=class_weight_dict)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_split=0.2, class_weight=class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.6437 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00\n",
      "[test loss, test accuracy, test precision, test recall, test AUC]: [nan, 0.6436759829521179, 0.0, 0.0]\n",
      "164/164 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.6436759648452427\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      1.00      0.78      3369\n",
      "           1       0.00      0.00      0.00      1865\n",
      "\n",
      "    accuracy                           0.64      5234\n",
      "   macro avg       0.32      0.50      0.39      5234\n",
      "weighted avg       0.41      0.64      0.50      5234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_result = model.evaluate(X_test, y_test)\n",
    "print(\"[test loss, test accuracy, test precision, test recall, test AUC]:\", eval_result)\n",
    "\n",
    "# Predicting and evaluating using custom thresholds if necessary\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Computing success metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
