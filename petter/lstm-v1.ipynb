{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LSTM Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* objectives:\n",
    "  * Fit a RNN (LSTM) model \n",
    "  * \n",
    "  \n",
    "* plan: \n",
    "\n",
    "1. libraries and functions\n",
    "2. load data\n",
    "3. preprocess data\n",
    "   - [x] full days only\n",
    "   - [ ] resample\n",
    "   - [ ] normalise activity\n",
    "4. feature engineering\n",
    "   - [ ] `mean`, `std`, `min`, `max`, `sum`\n",
    "   - [ ] `%0 active`\n",
    "5. train/test split\n",
    "6. modelling\n",
    "   - [ ] import libraries\n",
    "   - [ ] model selection\n",
    "   - [ ] model evaluation\n",
    "7. interpretation / visualisation\n",
    "   \n",
    "Try to recreate: \n",
    "\n",
    "**Bibliography:** Arora, A., Chakraborty, P. and Bhatia, M.P.S. (2023) Identifying digital biomarkers in actigraph based sequential motor activity data for assessment of depression: a model evaluating SVM in LSTM extracted feature space. International Journal of Information Technology [online]. 15 (2), pp. 797â€“802. Available from: https://doi.org/10.1007/s41870-023-01162-5 [Accessed 17 February 2024].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions\n",
    "- [x] extract depresjon from folder\n",
    "- [x] extract full days (1440 rows) records and minimum full records\n",
    "- [ ] resample, e.g. to hourly\n",
    "- [] normalise data (mean = 0, std = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_folder(folderpath, add_scores=False, downsample=None):\n",
    "    \"\"\"\n",
    "    Extract CSV data from folder and subfolders into a dataframe.\n",
    "\n",
    "    Args:\n",
    "      folderpath (str): Path to the folder containing CSV files.\n",
    "      add_scores (bool, optional): Boolean to add scores.csv to the dataframe. Defaults to False.\n",
    "      downsample (int, optional): Number of rows to downsample CSVs to. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "      pandas.DataFrame: DataFrame of concatenated CSV data.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Dict to store dataframes by condition  \n",
    "    dfs = {'control': [], 'condition': []}\n",
    "\n",
    "    try:\n",
    "        # Handle top-level scores CSV\n",
    "        if add_scores and 'scores.csv' in os.listdir(folderpath):\n",
    "            scores_path = os.path.join(folderpath, 'scores.csv')  \n",
    "            dfs['scores'] = pd.read_csv(scores_path)\n",
    "\n",
    "        # Get subfolders\n",
    "        subfolders = [f for f in os.listdir(folderpath) if os.path.isdir(os.path.join(folderpath, f))]\n",
    "\n",
    "        for subfolder in subfolders:\n",
    "            subfolderpath = os.path.join(folderpath, subfolder)  \n",
    "\n",
    "            # Get list of CSV files\n",
    "            files = os.listdir(subfolderpath)\n",
    "\n",
    "            for file in files:\n",
    "                filepath = os.path.join(subfolderpath, file)\n",
    "\n",
    "                # Extract ID from filename \n",
    "                id = file.split('.')[0]\n",
    "\n",
    "                df = pd.read_csv(filepath)\n",
    "\n",
    "                # Downsample if needed\n",
    "                if downsample:\n",
    "                    df = df.sample(downsample)\n",
    "\n",
    "                # Add ID column - this is the filename without the extension\n",
    "                df['id'] = id\n",
    "\n",
    "                # Add 'condition' column\n",
    "                df['condition'] = subfolder\n",
    "\n",
    "                # Convert 'timestamp' and 'date' to datetime\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "                # Append to dict by condition\n",
    "                if subfolder == 'control':\n",
    "                    dfs['control'].append(df)\n",
    "                else:  \n",
    "                    dfs['condition'].append(df)\n",
    "\n",
    "    except OSError:\n",
    "        print(f\"Error reading folder: {folderpath}\")\n",
    "\n",
    "    # concatenate dfs for each condition\n",
    "    dfs['control'] = pd.concat(dfs['control'])\n",
    "    dfs['condition'] = pd.concat(dfs['condition'])\n",
    "\n",
    "    # Reset index on the final df\n",
    "    df = pd.concat([dfs['control'], dfs['condition']]).reset_index(drop=True)\n",
    "\n",
    "    # add label column\n",
    "    df['label'] = 0\n",
    "    df.loc[df['condition'] == 'condition', 'label'] = 1\n",
    "    \n",
    "    # remove old 'condition' column\n",
    "    df.drop('condition', axis=1, inplace=True)\n",
    "\n",
    "    # Final concat\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = extract_folder('../data/depresjon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract full days only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_full_days(df):\n",
    "    \"\"\"\n",
    "    Extracts full days from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame containing only full days (1440 rows per day).\n",
    "\n",
    "    \"\"\"\n",
    "    # group by id and date, count rows, and filter where count equals 1440\n",
    "    full_days_df = df.groupby(['id', 'date']).filter(lambda x: len(x) == 1440)\n",
    "    \n",
    "    # print id and date combinations that don't have 1440 rows\n",
    "    not_full_days = df.groupby(['id', 'date']).size().reset_index(name='count').query('count != 1440')\n",
    "    print(\"id and date combinations that don't have 1440 rows:\")\n",
    "    print(not_full_days)\n",
    "    \n",
    "    return full_days_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id and date combinations that don't have 1440 rows:\n",
      "                id       date  count\n",
      "0      condition_1 2003-05-07    720\n",
      "16     condition_1 2003-05-23    924\n",
      "17    condition_10 2004-08-31    900\n",
      "32    condition_10 2004-09-15    495\n",
      "33    condition_11 2004-09-28    870\n",
      "...            ...        ...    ...\n",
      "1101     control_7 2003-04-23    610\n",
      "1102     control_8 2003-11-04    900\n",
      "1122     control_8 2003-11-24    658\n",
      "1123     control_9 2003-11-11    900\n",
      "1143     control_9 2003-12-01    778\n",
      "\n",
      "[115 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "full_df = extract_full_days(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481760, 5)\n",
      "              timestamp       date  activity         id  label\n",
      "540 2003-03-19 00:00:00 2003-03-19         0  control_1      0\n",
      "541 2003-03-19 00:01:00 2003-03-19         0  control_1      0\n",
      "542 2003-03-19 00:02:00 2003-03-19         0  control_1      0\n",
      "543 2003-03-19 00:03:00 2003-03-19         0  control_1      0\n",
      "544 2003-03-19 00:04:00 2003-03-19       175  control_1      0\n"
     ]
    }
   ],
   "source": [
    "# print shape of the full days dataframe\n",
    "print(full_df.shape)\n",
    "\n",
    "# print the first few rows of the full days dataframe\n",
    "print(full_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample to hourly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_data(df, freq, agg_func='mean'):\n",
    "    \"\"\"\n",
    "    Resamples the given DataFrame based on the specified frequency and aggregation function.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame.\n",
    "    - freq (str): The frequency at which to resample the dat. (e.g., H for hourly, D for daily, 5T for every 5 minutes),\n",
    "    - agg_func (str, optional): The aggregation function to apply during resampling. Defaults to 'mean'.\n",
    "\n",
    "    Returns:\n",
    "    - df_resampled (DataFrame): The resampled DataFrame.\n",
    "    \"\"\"\n",
    "    df_resampled = df.set_index('timestamp').groupby('id').resample(freq).agg(agg_func)\n",
    "    df_resampled.reset_index(inplace=True)\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIXME future warning - non-numeric cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the full days dataframe to hourly frequency\n",
    "resample_df = resample_data(full_df, 'h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise 'activity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_data(df, columns_to_normalise):\n",
    "    \"\"\"\n",
    "    Normalise the specified columns in the df using StandardScaler.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to be normalised.\n",
    "    - columns_to_normalise (list): A list of column names to be normalised.\n",
    "\n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): The DataFrame with the specified columns normalised.\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df[columns_to_normalise] = scaler.fit_transform(df[columns_to_normalise])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise 'activity'\n",
    "norm_df = normalise_data(resample_df, ['activity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 24 hour sequences\n",
    "\n",
    "The general idea - I think - is to create 24-hour long sequences from the hourly resampled dataframe so that *daily patterns* can be captured.  Then the `LSTM` model learns from the full day sequences.\n",
    "\n",
    "The sliding window effectively means taking a fixed-size window and liding across the data in steps: \n",
    "\n",
    "* window size = 24; hop-size = 12:\n",
    "  * 24 rows = first window\n",
    "  * slide window 12 rows down and take rows 12-36 as the next window\n",
    "  * etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(full_df, window_size, hop_size):\n",
    "    \"\"\"\n",
    "    Create sequences from a given dataframe.\n",
    "\n",
    "    Args:\n",
    "        full_df (pandas.DataFrame): df with full days\n",
    "        window_size (int): The size of each window/sequence.\n",
    "        hop_size (int): The number of steps to move the window.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sequences/windows created from the dataframe.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "\n",
    "    for i in range(0, len(full_df), hop_size):\n",
    "        window = full_df.iloc[i:i+window_size]\n",
    "        sequences.append(window)\n",
    "\n",
    "    return sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for full days df\n",
    "window_size = 24\n",
    "hop_size = 12\n",
    "sequences = create_sequences(norm_df, window_size, hop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2068\n",
      "(24, 5)\n",
      "              id           timestamp       date  activity  label\n",
      "24720  control_9 2003-11-27 00:00:00 2003-11-27 -0.704176    0.0\n",
      "24721  control_9 2003-11-27 01:00:00 2003-11-27 -0.704176    0.0\n",
      "24722  control_9 2003-11-27 02:00:00 2003-11-27 -0.704176    0.0\n",
      "24723  control_9 2003-11-27 03:00:00 2003-11-27 -0.704176    0.0\n",
      "24724  control_9 2003-11-27 04:00:00 2003-11-27 -0.704176    0.0\n",
      "24725  control_9 2003-11-27 05:00:00 2003-11-27 -0.704176    0.0\n",
      "24726  control_9 2003-11-27 06:00:00 2003-11-27 -0.704176    0.0\n",
      "24727  control_9 2003-11-27 07:00:00 2003-11-27 -0.704176    0.0\n",
      "24728  control_9 2003-11-27 08:00:00 2003-11-27 -0.704176    0.0\n",
      "24729  control_9 2003-11-27 09:00:00 2003-11-27 -0.704176    0.0\n",
      "24730  control_9 2003-11-27 10:00:00 2003-11-27 -0.704176    0.0\n",
      "24731  control_9 2003-11-27 11:00:00 2003-11-27 -0.704176    0.0\n",
      "24732  control_9 2003-11-27 12:00:00 2003-11-27 -0.681956    0.0\n",
      "24733  control_9 2003-11-27 13:00:00 2003-11-27 -0.703437    0.0\n",
      "24734  control_9 2003-11-27 14:00:00 2003-11-27 -0.704176    0.0\n",
      "24735  control_9 2003-11-27 15:00:00 2003-11-27 -0.703639    0.0\n",
      "24736  control_9 2003-11-27 16:00:00 2003-11-27 -0.700819    0.0\n",
      "24737  control_9 2003-11-27 17:00:00 2003-11-27 -0.700148    0.0\n",
      "24738  control_9 2003-11-27 18:00:00 2003-11-27 -0.698537    0.0\n",
      "24739  control_9 2003-11-27 19:00:00 2003-11-27 -0.696590    0.0\n",
      "24740  control_9 2003-11-27 20:00:00 2003-11-27 -0.696926    0.0\n",
      "24741  control_9 2003-11-27 21:00:00 2003-11-27 -0.696120    0.0\n",
      "24742  control_9 2003-11-27 22:00:00 2003-11-27 -0.696120    0.0\n",
      "24743  control_9 2003-11-27 23:00:00 2003-11-27 -0.696120    0.0\n"
     ]
    }
   ],
   "source": [
    "# print the number of sequences\n",
    "print(len(sequences))\n",
    "\n",
    "# print the shape of the first sequence\n",
    "print(sequences[0].shape)\n",
    "\n",
    "# print first sequence\n",
    "print(sequences[2060])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[              id           timestamp       date  activity  label\n",
      "24804  control_9 2003-11-30 12:00:00 2003-11-30  -0.69612    0.0\n",
      "24805  control_9 2003-11-30 13:00:00 2003-11-30  -0.69612    0.0\n",
      "24806  control_9 2003-11-30 14:00:00 2003-11-30  -0.69612    0.0\n",
      "24807  control_9 2003-11-30 15:00:00 2003-11-30  -0.69612    0.0\n",
      "24808  control_9 2003-11-30 16:00:00 2003-11-30  -0.69612    0.0\n",
      "24809  control_9 2003-11-30 17:00:00 2003-11-30  -0.69612    0.0\n",
      "24810  control_9 2003-11-30 18:00:00 2003-11-30  -0.69612    0.0\n",
      "24811  control_9 2003-11-30 19:00:00 2003-11-30  -0.69612    0.0\n",
      "24812  control_9 2003-11-30 20:00:00 2003-11-30  -0.69612    0.0\n",
      "24813  control_9 2003-11-30 21:00:00 2003-11-30  -0.69612    0.0\n",
      "24814  control_9 2003-11-30 22:00:00 2003-11-30  -0.69612    0.0\n",
      "24815  control_9 2003-11-30 23:00:00 2003-11-30  -0.69612    0.0]\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(seq) for seq in sequences]\n",
    "#print(set(lengths))\n",
    "\n",
    "# print sequences with length not equal to window_size\n",
    "print([seq for seq in sequences if len(seq) != window_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove partial sequence for `control_9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove sequences with length not equal to window_size\n",
    "full_seqs = [s for s in sequences if len(s) == window_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# print length of sequences\n",
    "print([seq for seq in full_seqs if len(seq) != window_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(sequences, activity_column='activity', label_column='label'):\n",
    "  \"\"\"\n",
    "  Create a dataset from a list of sequences.\n",
    "\n",
    "  Parameters:\n",
    "  sequences (list): A list of sequences.\n",
    "  activity_column (str): The name of the column containing activity data in each sequence. Default is 'activity'.\n",
    "  label_column (str): The name of the column containing label data in each sequence. Default is 'label'.\n",
    "\n",
    "  Returns:\n",
    "  inputs (numpy.ndarray): An array of input sequences.\n",
    "  targets (numpy.ndarray): An array of target sequences.\n",
    "  \"\"\"\n",
    "\n",
    "  import numpy as np\n",
    "  inputs = []\n",
    "  targets = []\n",
    "\n",
    "  for seq in sequences:\n",
    "    # Extract just activity column\n",
    "    input_arr = seq[activity_column].values\n",
    "\n",
    "    # Extract label column\n",
    "    target_arr = seq[label_column].values\n",
    "\n",
    "    inputs.append(input_arr)\n",
    "    targets.append(target_arr)\n",
    "\n",
    "  inputs = np.array(inputs)\n",
    "  targets = np.array(targets)\n",
    "\n",
    "  return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from full sequences\n",
    "X_train, y_train = create_dataset(full_seqs, 'label')\n",
    "# Reshape X_train for the model\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2067, 24, 1) (2067, 24)\n"
     ]
    }
   ],
   "source": [
    "# print the shape of X_train and y_train\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "Article (Arora, 2023) states that they extracted high level features from the fifth LSTM layer with this configuration: \n",
    "\n",
    "* num_timesteps = 24 \n",
    "* num_features = 1  \n",
    "* num_layers = 5 \n",
    "* units = [125, 100, 75, 50, 25] \n",
    "* dropout = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### side step - set up separate environment - too many issues instlling tensorfloww\n",
    "\n",
    "1. gpu drivers\n",
    "2. cuda toolkit, cuDNN\n",
    "3. running in wsl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 19:21:38.303993: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-18 19:21:38.642894: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-18 19:21:38.643180: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-18 19:21:38.698760: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-18 19:21:38.809205: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-18 19:21:39.594029: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 19:21:42.184298: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-18 19:21:42.499748: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-18 19:21:42.499797: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if TensorFlow sees any GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    # If GPUs are available, print the number of GPUs\n",
    "    print(\"Num GPUs Available:\", len(gpus))\n",
    "else:\n",
    "    print(\"No GPUs Available\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 19:21:55.410077: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-18 19:21:55.410155: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-18 19:21:55.410173: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-18 19:21:55.794423: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-18 19:21:55.795118: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-18 19:21:55.795136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-02-18 19:21:55.795209: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-18 19:21:55.795277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3600 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-02-18 19:21:58.438058: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-18 19:22:00.605888: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 5s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# configuration\n",
    "num_timesteps = 24 # X data shape \n",
    "num_features = 1  # Number of features\n",
    "num_layers = 5 \n",
    "units = [125, 100, 75, 50, 25] # Units in each LSTM layer\n",
    "dropout = 0.1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#  LSTM layers  \n",
    "for i in range(num_layers):\n",
    "  model.add(LSTM(units[i], return_sequences=True, dropout=dropout)) \n",
    "\n",
    "#  connected output layer  \n",
    "model.add(Dense(25)) \n",
    "\n",
    "# compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam') \n",
    "\n",
    "# generate features\n",
    "features = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 7.91813727e-06 -1.46745761e-05  2.23000934e-05 ... -2.13794374e-05\n",
      "    1.99195210e-06 -6.26087467e-06]\n",
      "  [ 2.96882499e-05 -6.89646549e-05  1.03898288e-04 ... -1.01119571e-04\n",
      "    1.17164618e-05 -3.10999167e-05]\n",
      "  [ 6.27766058e-05 -1.88223479e-04  2.80895241e-04 ... -2.78581749e-04\n",
      "    3.94541566e-05 -9.17305224e-05]\n",
      "  ...\n",
      "  [-7.48083787e-03 -4.97863395e-03  3.54944705e-03 ... -1.01685189e-02\n",
      "    1.10634314e-02 -1.96319968e-02]\n",
      "  [-8.14214442e-03 -4.97901067e-03  3.14453384e-03 ... -1.01871649e-02\n",
      "    1.17522776e-02 -2.08753459e-02]\n",
      "  [-8.78571905e-03 -4.98929992e-03  2.74188560e-03 ... -1.01795997e-02\n",
      "    1.24003105e-02 -2.20377594e-02]]\n",
      "\n",
      " [[ 7.91813727e-06 -1.46745761e-05  2.23000934e-05 ... -2.13794374e-05\n",
      "    1.99195210e-06 -6.26087467e-06]\n",
      "  [ 2.96882499e-05 -6.89646549e-05  1.03898288e-04 ... -1.01119571e-04\n",
      "    1.17164618e-05 -3.10999167e-05]\n",
      "  [ 6.27766058e-05 -1.88223479e-04  2.80895241e-04 ... -2.78581749e-04\n",
      "    3.94541566e-05 -9.17305224e-05]\n",
      "  ...\n",
      "  [-7.48083787e-03 -4.97863395e-03  3.54944705e-03 ... -1.01685189e-02\n",
      "    1.10634314e-02 -1.96319968e-02]\n",
      "  [-8.14214442e-03 -4.97901067e-03  3.14453384e-03 ... -1.01871649e-02\n",
      "    1.17522776e-02 -2.08753459e-02]\n",
      "  [-8.78571905e-03 -4.98929992e-03  2.74188560e-03 ... -1.01795997e-02\n",
      "    1.24003105e-02 -2.20377594e-02]]\n",
      "\n",
      " [[ 7.91813727e-06 -1.46745761e-05  2.23000934e-05 ... -2.13794374e-05\n",
      "    1.99195210e-06 -6.26087467e-06]\n",
      "  [ 2.96882499e-05 -6.89646549e-05  1.03898288e-04 ... -1.01119571e-04\n",
      "    1.17164618e-05 -3.10999167e-05]\n",
      "  [ 6.27766058e-05 -1.88223479e-04  2.80895241e-04 ... -2.78581749e-04\n",
      "    3.94541566e-05 -9.17305224e-05]\n",
      "  ...\n",
      "  [-7.48083787e-03 -4.97863395e-03  3.54944705e-03 ... -1.01685189e-02\n",
      "    1.10634314e-02 -1.96319968e-02]\n",
      "  [-8.14214442e-03 -4.97901067e-03  3.14453384e-03 ... -1.01871649e-02\n",
      "    1.17522776e-02 -2.08753459e-02]\n",
      "  [-8.78571905e-03 -4.98929992e-03  2.74188560e-03 ... -1.01795997e-02\n",
      "    1.24003105e-02 -2.20377594e-02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "# print features\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "Process:\n",
    "* ~~Downsample to hourly averages~~\n",
    "* ~~create 24 hour sequences using overlapping sliding windows with 12 hour hop~~\n",
    "* ~~each 24 hour sequence will be one data example~~\n",
    "\n",
    "LSTM features:\n",
    "* LSTM network with 5 layers (125, 100, 75, 50, 25 nodes)\n",
    "* use tanh activations and 0.1 dropout\n",
    "* feed 24 hour sequence and extract features from last layer\n",
    "\n",
    "Stat features:\n",
    "* men, std, %zero activities\n",
    "* use overlapping window\n",
    "\n",
    "Train SVM classifier\n",
    "* linear SVM with c=0.1\n",
    "\n",
    "Eval model\n",
    "* 10 fold cross validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models (broken into sets -not sure of time needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results - summary - out of the box models: \n",
    "\n",
    "* **Accuracy** - proportion of total predictions correct ->    `gradient boost`\n",
    "\n",
    "$$\\frac{{\\text{{True Positive}} + \\text{{True Negative}}}}{{\\text{{Total Prediction}}}}$$\n",
    "\n",
    "* **Precision**: proportion of positive prediction that are actually correct (Positive Predictive value) -> `neural network`\n",
    "  \n",
    "$$\\frac{{\\text{{True Positive}}}}{{\\text{{True Positives}}+ \\text{{False Positives}}}}$$\n",
    "\n",
    "* **Recall**: proportion of actual positives that are correctly identified (aka Sensitivity) -> `Naive Bayes`\n",
    "\n",
    "$$\\frac{{\\text{{True Positive}}}}{{\\text{{True Positives}} + \\text{{False Negatives}}}}$$\n",
    "\n",
    "* **F1**: harmonic mean of Precision and Recall -> `gradient boost`\n",
    "\n",
    "$$\\frac{{{{2}} * \\text{{(Precision}} * \\text{{Recall)}}}}{{\\text{{Precision}} + \\text{{Recall}}}}$$\n",
    "\n",
    "* **MCC**: measure of quality of binary classifications - considered a balanced measure ->  `gradient boost`\n",
    "\n",
    "$$\\frac{{\\text{{(TP * TN - FP *FN)}}}}{{\\text{{sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))}}}}$$\n",
    "\n",
    "* **Quickest**: time ->  `Naive Bayes`\n",
    "\n",
    "\n",
    "\n",
    "#### Reminder: \n",
    "\n",
    "* `True Positives (TP)`:  model predicted positive, and the truth is also positive.\n",
    "* `True Negatives (TN)`:  model predicted negative, and the truth is also negative.\n",
    "* `False Positives (FP)`: model predicted positive, and the truth is negative.\n",
    "* `False Negatives (FN)`: model predicted negative, and the truth is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
