{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic classification models\n",
    "\n",
    "* objectives:\n",
    "  * Fit a RNN (LSTM) model \n",
    "  * \n",
    "  \n",
    "* plan: \n",
    "\n",
    "1. libraries and functions\n",
    "2. load data\n",
    "3. preprocess data\n",
    "   - [ ] full days only\n",
    "   - [ ] normalise activity\n",
    "   - [ ] resample\n",
    "4. feature engineering\n",
    "   - [ ] `mean`, `std`, `min`, `max`, `sum`\n",
    "   - [ ] `%0 active`\n",
    "5. train/test split\n",
    "6. modelling\n",
    "   - [ ] import libraries\n",
    "   - [ ] model selection\n",
    "   - [ ] model evaluation\n",
    "7. interpretation / visualisation\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions\n",
    "- [x] extract depresjon from folder\n",
    "- [x] extract full days (1440 rows) records and minimum full records\n",
    "- [ ] resample, e.g. to hourly\n",
    "- [] normalise data (mean = 0, std = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_folder(folderpath, add_scores=False, downsample=None):\n",
    "    \"\"\"\n",
    "    Extract CSV data from folder and subfolders into a dataframe.\n",
    "\n",
    "    Args:\n",
    "      folderpath (str): Path to the folder containing CSV files.\n",
    "      add_scores (bool, optional): Boolean to add scores.csv to the dataframe. Defaults to False.\n",
    "      downsample (int, optional): Number of rows to downsample CSVs to. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "      pandas.DataFrame: DataFrame of concatenated CSV data.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Dict to store dataframes by condition  \n",
    "    dfs = {'control': [], 'condition': []}\n",
    "\n",
    "    try:\n",
    "        # Handle top-level scores CSV\n",
    "        if add_scores and 'scores.csv' in os.listdir(folderpath):\n",
    "            scores_path = os.path.join(folderpath, 'scores.csv')  \n",
    "            dfs['scores'] = pd.read_csv(scores_path)\n",
    "\n",
    "        # Get subfolders\n",
    "        subfolders = [f for f in os.listdir(folderpath) if os.path.isdir(os.path.join(folderpath, f))]\n",
    "\n",
    "        for subfolder in subfolders:\n",
    "            subfolderpath = os.path.join(folderpath, subfolder)  \n",
    "\n",
    "            # Get list of CSV files\n",
    "            files = os.listdir(subfolderpath)\n",
    "\n",
    "            for file in files:\n",
    "                filepath = os.path.join(subfolderpath, file)\n",
    "\n",
    "                # Extract ID from filename \n",
    "                id = file.split('.')[0]\n",
    "\n",
    "                df = pd.read_csv(filepath)\n",
    "\n",
    "                # Downsample if needed\n",
    "                if downsample:\n",
    "                    df = df.sample(downsample)\n",
    "\n",
    "                # Add ID column - this is the filename without the extension\n",
    "                df['id'] = id\n",
    "\n",
    "                # Add 'condition' column\n",
    "                df['condition'] = subfolder\n",
    "\n",
    "                # Convert 'timestamp' and 'date' to datetime\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "                # Append to dict by condition\n",
    "                if subfolder == 'control':\n",
    "                    dfs['control'].append(df)\n",
    "                else:  \n",
    "                    dfs['condition'].append(df)\n",
    "\n",
    "    except OSError:\n",
    "        print(f\"Error reading folder: {folderpath}\")\n",
    "\n",
    "    # concatenate dfs for each condition\n",
    "    dfs['control'] = pd.concat(dfs['control'])\n",
    "    dfs['condition'] = pd.concat(dfs['condition'])\n",
    "\n",
    "    # Reset index on the final df\n",
    "    df = pd.concat([dfs['control'], dfs['condition']]).reset_index(drop=True)\n",
    "\n",
    "    # add label column\n",
    "    df['label'] = 0\n",
    "    df.loc[df['condition'] == 'condition', 'label'] = 1\n",
    "    \n",
    "    # remove old 'condition' column\n",
    "    df.drop('condition', axis=1, inplace=True)\n",
    "\n",
    "    # Final concat\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = extract_folder('../data/depresjon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract full days only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_full_days(df):\n",
    "    # group by id and date, count rows, and filter where count equals 1440\n",
    "    full_days_df = df.groupby(['id', 'date']).filter(lambda x: len(x) == 1440)\n",
    "    \n",
    "    # print id and date combinations that don't have 1440 rows\n",
    "    not_full_days = df.groupby(['id', 'date']).size().reset_index(name='count').query('count != 1440')\n",
    "    print(\"id and date combinations that don't have 1440 rows:\")\n",
    "    print(not_full_days)\n",
    "    \n",
    "    return full_days_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id and date combinations that don't have 1440 rows:\n",
      "                id       date  count\n",
      "0      condition_1 2003-05-07    720\n",
      "16     condition_1 2003-05-23    924\n",
      "17    condition_10 2004-08-31    900\n",
      "32    condition_10 2004-09-15    495\n",
      "33    condition_11 2004-09-28    870\n",
      "...            ...        ...    ...\n",
      "1101     control_7 2003-04-23    610\n",
      "1102     control_8 2003-11-04    900\n",
      "1122     control_8 2003-11-24    658\n",
      "1123     control_9 2003-11-11    900\n",
      "1143     control_9 2003-12-01    778\n",
      "\n",
      "[115 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "full_df = extract_full_days(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "Process:\n",
    "* Downsample to hourly averages\n",
    "* create 24 hour sequences using overlapping sliding windows with 12 hour hop\n",
    "* each 24 hour sequence will be one data example\n",
    "\n",
    "LSTM features:\n",
    "* LSTM network with 5 layers (125, 100, 75, 50, 25 nodes)\n",
    "* use tanh activations and 0.1 dropout\n",
    "* feed 24 hour sequence and extract features from last layer\n",
    "\n",
    "Stat features:\n",
    "* men, std, %zero activities\n",
    "* use overlapping window\n",
    "\n",
    "Train SVM classifier\n",
    "* linear SVM with c=0.1\n",
    "\n",
    "Eval model\n",
    "* 10 fold cross validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample - \n",
    "\n",
    "* I am skipping this optional step for now...I want to see if it runs on the complete df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models (broken into sets -not sure of time needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results - summary - out of the box models: \n",
    "\n",
    "* **Accuracy** - proportion of total predictions correct ->    `gradient boost`\n",
    "\n",
    "$$\\frac{{\\text{{True Positive}} + \\text{{True Negative}}}}{{\\text{{Total Prediction}}}}$$\n",
    "\n",
    "* **Precision**: proportion of positive prediction that are actually correct (Positive Predictive value) -> `neural network`\n",
    "  \n",
    "$$\\frac{{\\text{{True Positive}}}}{{\\text{{True Positives}}+ \\text{{False Positives}}}}$$\n",
    "\n",
    "* **Recall**: proportion of actual positives that are correctly identified (aka Sensitivity) -> `Naive Bayes`\n",
    "\n",
    "$$\\frac{{\\text{{True Positive}}}}{{\\text{{True Positives}} + \\text{{False Negatives}}}}$$\n",
    "\n",
    "* **F1**: harmonic mean of Precision and Recall -> `gradient boost`\n",
    "\n",
    "$$\\frac{{{{2}} * \\text{{(Precision}} * \\text{{Recall)}}}}{{\\text{{Precision}} + \\text{{Recall}}}}$$\n",
    "\n",
    "* **MCC**: measure of quality of binary classifications - considered a balanced measure ->  `gradient boost`\n",
    "\n",
    "$$\\frac{{\\text{{(TP * TN - FP *FN)}}}}{{\\text{{sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))}}}}$$\n",
    "\n",
    "* **Quickest**: time ->  `Naive Bayes`\n",
    "\n",
    "\n",
    "\n",
    "#### Reminder: \n",
    "\n",
    "* `True Positives (TP)`:  model predicted positive, and the truth is also positive.\n",
    "* `True Negatives (TN)`:  model predicted negative, and the truth is also negative.\n",
    "* `False Positives (FP)`: model predicted positive, and the truth is negative.\n",
    "* `False Negatives (FN)`: model predicted negative, and the truth is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
