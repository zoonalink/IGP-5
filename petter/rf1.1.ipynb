{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - baseline\n",
    "\n",
    "* Referencing: \n",
    "> **Bibliography:** Zanella-Calzada, L.A., Galván-Tejada, C.E., Chávez-Lamas, N.M., Gracia-Cortés, M. del C., Magallanes-Quintanar, R., Celaya-Padilla, J.M., Galván-Tejada, J.I. and Gamboa-Rosales, H. (2019) Feature Extraction in Motor Activity Signal: Towards a Depression Episodes Detection in Unipolar and Bipolar Patients. Diagnostics [online]. 9 (1), p. 8. Available from: https://www.mdpi.com/2075-4418/9/1/8 [Accessed 28 November 2023].\n",
    "* [article notes](../literature/Zanella-FeatureExtraction.md)\n",
    "\n",
    "Objective: \n",
    "\n",
    "* To fit a modified Random Forest model, taking inspiration from the above article\n",
    "\n",
    "## Plan \n",
    "\n",
    "1. Load and process `depresjon`\n",
    "   * load into pandas df\n",
    "   * select `control` and `condition` -> it seems that they used first 4 control and first 5 condition participants\n",
    "   * normalise data (mean = 0, std = 1)\n",
    "   * remove incomplete cases\n",
    "   * take only first value of each hour??  maybe mean for each hour (**I don't understand this**)\n",
    "\n",
    "2. Extract features - 14 features\n",
    "   * mean\n",
    "   * standard deviation\n",
    "   * variance\n",
    "   * trimmed mean\n",
    "   * coefficient of variation\n",
    "   * inversse coefficient of variation\n",
    "   * kurtosis\n",
    "   * skewness\n",
    "   * quantailes (1, 5, 25, 75, 95, 99)\n",
    "\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis, mstats\n",
    "from statsmodels.robust import mad\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data\n",
    "\n",
    "### Load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_folder(folderpath, add_scores=False, downsample=None):\n",
    "    \"\"\"\n",
    "    Extract CSV data from folder and subfolders into a dataframe.\n",
    "\n",
    "    Args:\n",
    "      folderpath (str): Path to the folder containing CSV files.\n",
    "      add_scores (bool, optional): Boolean to add scores.csv to the dataframe. Defaults to False.\n",
    "      downsample (int, optional): Number of rows to downsample CSVs to. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "      pandas.DataFrame: DataFrame of concatenated CSV data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dict to store dataframes by condition  \n",
    "    dfs = {'control': [], 'condition': []}\n",
    "\n",
    "    try:\n",
    "        # Handle top-level scores CSV\n",
    "        if add_scores and 'scores.csv' in os.listdir(folderpath):\n",
    "            scores_path = os.path.join(folderpath, 'scores.csv')  \n",
    "            dfs['scores'] = pd.read_csv(scores_path)\n",
    "\n",
    "        # Get subfolders\n",
    "        subfolders = [f for f in os.listdir(folderpath) if os.path.isdir(os.path.join(folderpath, f))]\n",
    "\n",
    "        for subfolder in subfolders:\n",
    "            subfolderpath = os.path.join(folderpath, subfolder)  \n",
    "\n",
    "            # Get list of CSV files\n",
    "            files = os.listdir(subfolderpath)\n",
    "\n",
    "            for file in files:\n",
    "                filepath = os.path.join(subfolderpath, file)\n",
    "\n",
    "                # Extract ID from filename \n",
    "                id = file.split('.')[0]\n",
    "\n",
    "                df = pd.read_csv(filepath)\n",
    "\n",
    "                # Downsample if needed\n",
    "                if downsample:\n",
    "                    df = df.sample(downsample)\n",
    "\n",
    "                # Add ID column - this is the filename without the extension\n",
    "                df['id'] = id\n",
    "\n",
    "                # Add 'condition' column\n",
    "                df['condition'] = subfolder\n",
    "\n",
    "                # Convert 'timestamp' and 'date' to datetime\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "                # Append to dict by condition\n",
    "                if subfolder == 'control':\n",
    "                    dfs['control'].append(df)\n",
    "                else:  \n",
    "                    dfs['condition'].append(df)\n",
    "\n",
    "    except OSError:\n",
    "        print(f\"Error reading folder: {folderpath}\")\n",
    "\n",
    "    # concatenate dfs for each condition\n",
    "    dfs['control'] = pd.concat(dfs['control'])\n",
    "    dfs['condition'] = pd.concat(dfs['condition'])\n",
    "\n",
    "    # Reset index on the final df\n",
    "    df = pd.concat([dfs['control'], dfs['condition']]).reset_index(drop=True)\n",
    "\n",
    "    # add label column\n",
    "    df['label'] = 0\n",
    "    df.loc[df['condition'] == 'condition', 'label'] = 1\n",
    "    \n",
    "    # remove old 'condition' column\n",
    "    df.drop('condition', axis=1, inplace=True)\n",
    "\n",
    "    # Final concat\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  timestamp       date  activity           id  label\n",
      "0       2003-03-18 15:00:00 2003-03-18        60    control_1      0\n",
      "1       2003-03-18 15:01:00 2003-03-18         0    control_1      0\n",
      "2       2003-03-18 15:02:00 2003-03-18       264    control_1      0\n",
      "3       2003-03-18 15:03:00 2003-03-18       662    control_1      0\n",
      "4       2003-03-18 15:04:00 2003-03-18       293    control_1      0\n",
      "...                     ...        ...       ...          ...    ...\n",
      "1571696 2004-06-10 14:58:00 2004-06-10         0  condition_9      1\n",
      "1571697 2004-06-10 14:59:00 2004-06-10         0  condition_9      1\n",
      "1571698 2004-06-10 15:00:00 2004-06-10         0  condition_9      1\n",
      "1571699 2004-06-10 15:01:00 2004-06-10         5  condition_9      1\n",
      "1571700 2004-06-10 15:02:00 2004-06-10         0  condition_9      1\n",
      "\n",
      "[1571701 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# set folder path\n",
    "folderpath = '../data/depresjon/'\n",
    "# extract all files\n",
    "all_files = extract_folder(folderpath)\n",
    "# print rows 21-24\n",
    "#print(all_files.iloc[21:25])\n",
    "print(all_files.head(-5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select subset\n",
    "\n",
    "As per article - picking first 4 control & first 5 condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp       date  activity         id  label\n",
      "0 2003-03-18 15:00:00 2003-03-18        60  control_1      0\n",
      "1 2003-03-18 15:01:00 2003-03-18         0  control_1      0\n",
      "2 2003-03-18 15:02:00 2003-03-18       264  control_1      0\n",
      "3 2003-03-18 15:03:00 2003-03-18       662  control_1      0\n",
      "4 2003-03-18 15:04:00 2003-03-18       293  control_1      0 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 306813 entries, 0 to 1488480\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype         \n",
      "---  ------     --------------   -----         \n",
      " 0   timestamp  306813 non-null  datetime64[ns]\n",
      " 1   date       306813 non-null  datetime64[ns]\n",
      " 2   activity   306813 non-null  int64         \n",
      " 3   id         306813 non-null  object        \n",
      " 4   label      306813 non-null  int64         \n",
      "dtypes: datetime64[ns](2), int64(2), object(1)\n",
      "memory usage: 14.0+ MB\n",
      "None \n",
      "\n",
      "                  timestamp       date  activity           id  label\n",
      "330295  2002-10-20 02:20:00 2002-10-20         0    control_2      0\n",
      "1474504 2003-06-17 15:46:00 2003-06-17       590  condition_5      1\n",
      "24986   2003-04-05 00:26:00 2003-04-05         0    control_1      0\n",
      "1484626 2003-06-24 16:28:00 2003-06-24        40  condition_5      1\n",
      "803840  2002-11-12 22:13:00 2002-11-12         3    control_4      0\n",
      "39022   2003-04-14 18:22:00 2003-04-14         0    control_1      0\n",
      "36462   2003-04-12 23:42:00 2003-04-12         0    control_1      0\n",
      "1450696 2003-06-07 03:43:00 2003-06-07       382  condition_4      1\n",
      "1020832 2003-05-08 02:02:00 2003-05-08         0  condition_1      1\n",
      "1447134 2003-06-04 16:21:00 2003-06-04       286  condition_4      1 \n",
      "\n",
      "control_3      65407\n",
      "control_1      51611\n",
      "condition_2    38926\n",
      "control_2      31473\n",
      "control_4      31455\n",
      "condition_1    23244\n",
      "condition_3    21648\n",
      "condition_4    21556\n",
      "condition_5    21493\n",
      "Name: id, dtype: int64 \n",
      "\n",
      "0    179946\n",
      "1    126867\n",
      "Name: label, dtype: int64\n",
      "0    0.586501\n",
      "1    0.413499\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "control_subjects = ['control_1', 'control_2', 'control_3', 'control_4']\n",
    "condition_subjects = ['condition_1', 'condition_2', 'condition_3', 'condition_4', 'condition_5']\n",
    "\n",
    "# Filter for control subjects\n",
    "control_df = all_files[all_files['id'].isin(control_subjects)] \n",
    "\n",
    "# Filter for condition subjects\n",
    "condition_df = all_files[all_files['id'].isin(condition_subjects)]\n",
    "\n",
    "# Concatenate \n",
    "subset_df = pd.concat([control_df, condition_df])\n",
    "\n",
    "# print the first 5 rows\n",
    "print(subset_df.head(5), '\\n')\n",
    "\n",
    "# print info\n",
    "print(subset_df.info(), '\\n')\n",
    "\n",
    "# print random 10 rows\n",
    "print(subset_df.sample(10), '\\n')\n",
    "\n",
    "# print number of rows by 'id'\n",
    "print(subset_df['id'].value_counts(), '\\n')\n",
    "\n",
    "# print number of rows by 'label'\n",
    "print(subset_df['label'].value_counts())\n",
    "\n",
    "# print proportion of 'label' column\n",
    "print(subset_df['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise subset (z-score)\n",
    "\n",
    "Could use: \n",
    "\n",
    "* `sklearn.preprocessing.scale` - Standardises features by removing the mean and scaling to unit variance (similar to manual z-score normalisation)\n",
    "* `sklearn.preprocessing.minmax_scale` - Transforms features to a given range (often 0-1 for minmax scaling)\n",
    "* `sklearn.preprocessing.normalize` - L2 vector normalisation\n",
    "\n",
    "* pandas -> df.normalize(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            activity          label  activity_norm\n",
      "count  306813.000000  306813.000000   3.068130e+05\n",
      "mean      168.451369       0.413499  -2.501153e-17\n",
      "std       356.899613       0.492462   1.000000e+00\n",
      "min         0.000000       0.000000  -4.719853e-01\n",
      "25%         0.000000       0.000000  -4.719853e-01\n",
      "50%         3.000000       0.000000  -4.635796e-01\n",
      "75%       160.000000       1.000000  -2.367996e-02\n",
      "max      6776.000000       1.000000   1.851375e+01\n"
     ]
    }
   ],
   "source": [
    "# calculate mean and standard deviation\n",
    "mu = subset_df['activity'].mean()\n",
    "sigma = subset_df['activity'].std()\n",
    "\n",
    "# normalise\n",
    "normal_df = subset_df.copy()\n",
    "normal_df['activity_norm'] = (normal_df['activity'] - mu)/sigma\n",
    "\n",
    "# print summary statistics\n",
    "print(normal_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False \n",
      "\n",
      "timestamp        0\n",
      "date             0\n",
      "activity         0\n",
      "id               0\n",
      "label            0\n",
      "activity_norm    0\n",
      "dtype: int64 \n",
      "\n",
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# Check if dataframe has any NaN \n",
    "print(normal_df.isnull().values.any(), '\\n')\n",
    "\n",
    "# Count number of NaN per column\n",
    "print(normal_df.isnull().sum(), '\\n')\n",
    "\n",
    "# See indices of NaN values \n",
    "print(normal_df[normal_df.isnull().any(axis=1)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 306813 entries, 0 to 1488480\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype         \n",
      "---  ------         --------------   -----         \n",
      " 0   timestamp      306813 non-null  datetime64[ns]\n",
      " 1   date           306813 non-null  datetime64[ns]\n",
      " 2   activity       306813 non-null  int64         \n",
      " 3   id             306813 non-null  object        \n",
      " 4   label          306813 non-null  int64         \n",
      " 5   activity_norm  306813 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(1), int64(2), object(1)\n",
      "memory usage: 16.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# drop NaN values rows\n",
    "normal_df.dropna(inplace=True)\n",
    "\n",
    "# Check if dataframe has any NaN\n",
    "print(normal_df.isnull().values.any())\n",
    "\n",
    "# print info\n",
    "print(normal_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features (14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Mean**:\n",
    "\n",
    "$\\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i$\n",
    "\n",
    "* Calculation: Average value of the activity data.\n",
    "* Significance: Represents the central tendency of the data.\n",
    "\n",
    "**Trimmed Standard Deviation:**\n",
    "\n",
    "$\\text{sd} = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\mu)^2}$\n",
    "\n",
    "\n",
    "* Calculation: A measure of the amount of variation in the data, with a specified percentage of outliers removed.\n",
    "* Significance: More robust to outliers than the regular standard deviation.\n",
    "\n",
    "**Trimmed Variance:**\n",
    "\n",
    "$\\text{sd}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2$\n",
    "\n",
    "* Calculation: Similar to trimmed standard deviation but squared, providing a measure of the spread of the data.\n",
    "* Significance: Robust measure of data spread with specified outliers removed.\n",
    "\n",
    "**Quantiles (1st, 5th, 25th, 75th, 95th, 99th):**\n",
    "\n",
    "$Q[i](p) = (1 - \\gamma)x[j] + \\gamma x[j + 1]$\n",
    "\n",
    "* Calculation: Values below which a given percentage of the data falls.\n",
    "* Significance: Describes the distribution of the data and helps identify potential outliers.\n",
    "\n",
    "**Skewness:**\n",
    "\n",
    "$S = \\frac{\\mu - \\upsilon}{\\sigma}$\n",
    "\n",
    "* Calculation: A measure of the asymmetry of the data distribution.\n",
    "* Significance: Positive skewness indicates a right-skewed distribution (tail to the right), negative skewness indicates a left-skewed distribution (tail to the left).\n",
    "\n",
    "**Kurtosis:**\n",
    "\n",
    "$K = \\frac{\\mu}{\\sigma}$\n",
    "\n",
    "* Calculation: A measure of the \"tailedness\" of the data distribution.\n",
    "* Significance: High kurtosis indicates heavy tails and more outliers compared to a normal distribution. Low kurtosis indicates light tails.\n",
    "\n",
    "**Coefficient of Variation (Coef_Var):**\n",
    "\n",
    "$\\text{CV} = \\frac{\\text{sd}}{\\mu}$\n",
    "\n",
    "* Calculation: Standard deviation divided by the mean, expressing the relative variability of the data.\n",
    "* Significance: Useful for comparing the variability of datasets with different units or scales.\n",
    "\n",
    "**Inverse Coefficient of Variation (Inverse_Coef_Var):**\n",
    "\n",
    "$\\text{ICV} = \\frac{\\mu}{\\text{sd}}$\n",
    "\n",
    "* Calculation: The reciprocal of the coefficient of variation.\n",
    "* Significance: Measures the efficiency of data representation; a higher value suggests more efficient data representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy df to avoid changing the original\n",
    "normal_feature_df = normal_df.copy()\n",
    "\n",
    "# 'timestamp' as index\n",
    "normal_feature_df = normal_feature_df.set_index('timestamp')\n",
    "normal_df = normal_df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean, standard deviation, and variance for each hour\n",
    "mean_value_hourly = normal_feature_df.resample('H')['activity_norm'].mean()\n",
    "std_value_hourly = normal_feature_df.resample('H')['activity_norm'].std()\n",
    "variance_value_hourly = normal_feature_df.resample('H')['activity_norm'].var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimmed Mean (using 5% trimming??) for each hour\n",
    "def calculate_trimmed_mean(x):\n",
    "    if len(x) > 0:\n",
    "        return np.mean(x[(x >= np.percentile(x, 5)) & (x <= np.percentile(x, 95))])\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "trimmed_mean_value_hourly = normal_feature_df.resample('H')['activity_norm'].apply(calculate_trimmed_mean)\n",
    "\n",
    "# Coefficient of Variation and Inverse Coefficient of Variation for each hour\n",
    "coef_var_value_hourly = std_value_hourly / mean_value_hourly\n",
    "inverse_coef_var_value_hourly = mean_value_hourly / std_value_hourly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_32272\\514108838.py:15: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  kurtosis_value_hourly = normal_feature_df.resample('H')['activity_norm'].apply(lambda x: kurtosis(x) if len(x) > 0 else np.nan)\n",
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_32272\\514108838.py:16: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  skewness_value_hourly = normal_feature_df.resample('H')['activity_norm'].apply(lambda x: skew(x) if len(x) > 0 else np.nan)\n"
     ]
    }
   ],
   "source": [
    "# Kurtosis and Skewness for each hour\n",
    "def calculate_kurtosis(x):\n",
    "    if len(x) > 0 and np.std(x) != 0:\n",
    "        return kurtosis(x)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def calculate_skewness(x):\n",
    "    if len(x) > 0 and np.std(x) != 0:\n",
    "        return skew(x)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Kurtosis and Skewness for each hour\n",
    "kurtosis_value_hourly = normal_feature_df.resample('H')['activity_norm'].apply(lambda x: kurtosis(x) if len(x) > 0 else np.nan)\n",
    "skewness_value_hourly = normal_feature_df.resample('H')['activity_norm'].apply(lambda x: skew(x) if len(x) > 0 else np.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantiles (1%, 5%, 25%, 75%, 95%, 99%) for each hour\n",
    "quantiles_value_hourly = normal_feature_df.resample('H')['activity_norm'].quantile([0.01, 0.05, 0.25, 0.75, 0.95, 0.99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'label' from normal_df with resampling\n",
    "label_hourly = normal_df.resample('H')['label'].first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align labels with features\n",
    "features_hourly = pd.DataFrame({\n",
    "    'mean': mean_value_hourly,\n",
    "    'std': std_value_hourly,\n",
    "    'variance': variance_value_hourly,\n",
    "    'trimmed_mean': trimmed_mean_value_hourly,\n",
    "    'coef_var': coef_var_value_hourly,\n",
    "    'inverse_coef_var': inverse_coef_var_value_hourly,\n",
    "    'kurtosis': kurtosis_value_hourly,\n",
    "    'skewness': skewness_value_hourly,\n",
    "    'label': label_hourly  # Add 'label' column\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Quantiles\n",
    "features_hourly = pd.concat([features_hourly, quantiles_value_hourly.unstack(level=-1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         mean       std  variance  trimmed_mean   coef_var  \\\n",
      "timestamp                                                                    \n",
      "2002-10-02 15:00:00  1.226905  1.213061  1.471516      1.122097   0.988716   \n",
      "2002-10-02 16:00:00  1.574949  1.221473  1.491997      1.543573   0.775564   \n",
      "2002-10-02 17:00:00  2.149760  1.191689  1.420123      2.106185   0.554336   \n",
      "2002-10-02 18:00:00  0.310868  1.101726  1.213801      0.145472   3.544034   \n",
      "2002-10-02 19:00:00  0.318620  1.047779  1.097842      0.157313   3.288494   \n",
      "...                       ...       ...       ...           ...        ...   \n",
      "2003-06-27 04:00:00 -0.471985  0.000000  0.000000     -0.471985  -0.000000   \n",
      "2003-06-27 05:00:00 -0.471985  0.000000  0.000000     -0.471985  -0.000000   \n",
      "2003-06-27 06:00:00 -0.471985  0.000000  0.000000     -0.471985  -0.000000   \n",
      "2003-06-27 07:00:00 -0.043854  0.710295  0.504518     -0.170510 -16.196911   \n",
      "2003-06-27 08:00:00 -0.433736  0.184898  0.034187     -0.471985  -0.426290   \n",
      "\n",
      "                     inverse_coef_var   kurtosis  skewness  label      0.01  \\\n",
      "timestamp                                                                     \n",
      "2002-10-02 15:00:00          1.011413   0.913590  0.818144    0.0 -0.360834   \n",
      "2002-10-02 16:00:00          1.289384  -0.851444  0.374490    0.0 -0.239315   \n",
      "2002-10-02 17:00:00          1.803961   0.655764  0.403232    0.0 -0.128135   \n",
      "2002-10-02 18:00:00          0.282164   5.408604  2.321854    0.0 -0.471985   \n",
      "2002-10-02 19:00:00          0.304091   1.795890  1.574117    0.0 -0.471985   \n",
      "...                               ...        ...       ...    ...       ...   \n",
      "2003-06-27 04:00:00              -inf        NaN       NaN    1.0 -0.471985   \n",
      "2003-06-27 05:00:00              -inf        NaN       NaN    1.0 -0.471985   \n",
      "2003-06-27 06:00:00              -inf        NaN       NaN    1.0 -0.471985   \n",
      "2003-06-27 07:00:00         -0.061740  14.558287  3.348301    1.0 -0.471985   \n",
      "2003-06-27 08:00:00         -2.345818  24.242867  4.976884    1.0 -0.471985   \n",
      "\n",
      "                         0.05      0.25      0.75      0.95      0.99  \n",
      "timestamp                                                              \n",
      "2002-10-02 15:00:00 -0.275852  0.222187  2.080553  3.160409  4.394593  \n",
      "2002-10-02 16:00:00 -0.095129  0.659285  2.425188  3.633651  4.009415  \n",
      "2002-10-02 17:00:00  0.285931  1.441718  2.923928  4.069208  5.264558  \n",
      "2002-10-02 18:00:00 -0.464000 -0.309475  0.325158  2.071027  4.346597  \n",
      "2002-10-02 19:00:00 -0.471985 -0.450271  0.775564  2.642056  3.481732  \n",
      "...                       ...       ...       ...       ...       ...  \n",
      "2003-06-27 04:00:00 -0.471985 -0.471985 -0.471985 -0.471985 -0.471985  \n",
      "2003-06-27 05:00:00 -0.471985 -0.471985 -0.471985 -0.471985 -0.471985  \n",
      "2003-06-27 06:00:00 -0.471985 -0.471985 -0.471985 -0.471985 -0.471985  \n",
      "2003-06-27 07:00:00 -0.471985 -0.471985  0.158444  0.921124  2.616166  \n",
      "2003-06-27 08:00:00 -0.471985 -0.471985 -0.471985 -0.471985  0.393636  \n",
      "\n",
      "[6426 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(features_hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN and Inf Check:\n",
      "mean                2488\n",
      "std                 2488\n",
      "variance            2488\n",
      "trimmed_mean        2488\n",
      "coef_var            2488\n",
      "inverse_coef_var    2488\n",
      "kurtosis            3665\n",
      "skewness            3665\n",
      "label               2488\n",
      "0.01                2488\n",
      "0.05                2488\n",
      "0.25                2488\n",
      "0.75                2488\n",
      "0.95                2488\n",
      "0.99                2488\n",
      "mean                   0\n",
      "std                    0\n",
      "variance               0\n",
      "trimmed_mean           0\n",
      "coef_var               0\n",
      "inverse_coef_var    1177\n",
      "kurtosis               0\n",
      "skewness               0\n",
      "label                  0\n",
      "0.01                   0\n",
      "0.05                   0\n",
      "0.25                   0\n",
      "0.75                   0\n",
      "0.95                   0\n",
      "0.99                   0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_32272\\2716595254.py:2: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  nan_inf_check = features_hourly.isnull().sum().append(features_hourly.isin([np.inf, -np.inf]).sum())\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN or inf values\n",
    "nan_inf_check = features_hourly.isnull().sum().append(features_hourly.isin([np.inf, -np.inf]).sum())\n",
    "print(\"NaN and Inf Check:\")\n",
    "print(nan_inf_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3938, 15)\n",
      "NaN and Inf Check:\n",
      "mean                   0\n",
      "std                    0\n",
      "variance               0\n",
      "trimmed_mean           0\n",
      "coef_var               0\n",
      "inverse_coef_var       0\n",
      "kurtosis            1177\n",
      "skewness            1177\n",
      "label                  0\n",
      "0.01                   0\n",
      "0.05                   0\n",
      "0.25                   0\n",
      "0.75                   0\n",
      "0.95                   0\n",
      "0.99                   0\n",
      "mean                   0\n",
      "std                    0\n",
      "variance               0\n",
      "trimmed_mean           0\n",
      "coef_var               0\n",
      "inverse_coef_var    1177\n",
      "kurtosis               0\n",
      "skewness               0\n",
      "label                  0\n",
      "0.01                   0\n",
      "0.05                   0\n",
      "0.25                   0\n",
      "0.75                   0\n",
      "0.95                   0\n",
      "0.99                   0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_32272\\198874199.py:7: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  nan_inf_check = features_hourly_2.isnull().sum().append(features_hourly_2.isin([np.inf, -np.inf]).sum())\n"
     ]
    }
   ],
   "source": [
    "# drop rows with missing labels\n",
    "features_hourly_2 = features_hourly.dropna(subset=['label'])\n",
    "\n",
    "#print shape\n",
    "print(features_hourly_2.shape)\n",
    "\n",
    "nan_inf_check = features_hourly_2.isnull().sum().append(features_hourly_2.isin([np.inf, -np.inf]).sum())\n",
    "print(\"NaN and Inf Check:\")\n",
    "print(nan_inf_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows with NaN kurtosis and skewness and -inf inverse_coef_var...\n",
    "\n",
    "**Decided to impute to mean for now.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_32272\\1054803192.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features_hourly_2['kurtosis'].fillna(features_hourly_2['kurtosis'].mean(), inplace=True)\n",
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_32272\\1054803192.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features_hourly_2['skewness'].fillna(features_hourly_2['skewness'].mean(), inplace=True)\n",
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_32272\\1054803192.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features_hourly_2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_32272\\1054803192.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features_hourly_2['inverse_coef_var'].fillna(features_hourly_2['inverse_coef_var'].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values with mean or median\n",
    "features_hourly_2['kurtosis'].fillna(features_hourly_2['kurtosis'].mean(), inplace=True)\n",
    "features_hourly_2['skewness'].fillna(features_hourly_2['skewness'].mean(), inplace=True)\n",
    "\n",
    "# Handle infinite values in 'inverse_coef_var'\n",
    "features_hourly_2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "features_hourly_2['inverse_coef_var'].fillna(features_hourly_2['inverse_coef_var'].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN and Inf Check:\n",
      "mean                0\n",
      "std                 0\n",
      "variance            0\n",
      "trimmed_mean        0\n",
      "coef_var            0\n",
      "inverse_coef_var    0\n",
      "kurtosis            0\n",
      "skewness            0\n",
      "label               0\n",
      "0.01                0\n",
      "0.05                0\n",
      "0.25                0\n",
      "0.75                0\n",
      "0.95                0\n",
      "0.99                0\n",
      "mean                0\n",
      "std                 0\n",
      "variance            0\n",
      "trimmed_mean        0\n",
      "coef_var            0\n",
      "inverse_coef_var    0\n",
      "kurtosis            0\n",
      "skewness            0\n",
      "label               0\n",
      "0.01                0\n",
      "0.05                0\n",
      "0.25                0\n",
      "0.75                0\n",
      "0.95                0\n",
      "0.99                0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_32272\\1165047148.py:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  nan_inf_check = features_hourly_2.isnull().sum().append(features_hourly_2.isin([np.inf, -np.inf]).sum())\n"
     ]
    }
   ],
   "source": [
    "nan_inf_check = features_hourly_2.isnull().sum().append(features_hourly_2.isin([np.inf, -np.inf]).sum())\n",
    "print(\"NaN and Inf Check:\")\n",
    "print(nan_inf_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "### Train / Test split\n",
    "\n",
    "* test = 0.2\n",
    "* random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the data into training and testing sets.\n",
    "\n",
    "Parameters:\n",
    "    X : pandas.DataFrame\n",
    "        The feature matrix.\n",
    "    y : pandas.Series\n",
    "        The target variable.\n",
    "    test_size : float, optional\n",
    "        The proportion of the dataset to include in the test split. Default is 0.2.\n",
    "    random_state : int, optional\n",
    "        The seed used by the random number generator. Default is 42.\n",
    "\n",
    "Returns:\n",
    "    X_train : pandas.DataFrame\n",
    "        The training feature matrix.\n",
    "    X_test : pandas.DataFrame\n",
    "        The testing feature matrix.\n",
    "    y_train : pandas.Series\n",
    "        The training target variable.\n",
    "    y_test : pandas.Series\n",
    "        The testing target variable.\n",
    "\"\"\"\n",
    "X = features_hourly_2.drop('label', axis=1)\n",
    "y = features_hourly_2['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "* estimators - number of trees in the forest; how many trees before deciding\n",
    "* max_depth - max depth of tree - levels\n",
    "* min_samples_split - min number of samples required to split an internal node\n",
    "* min_samples_leaf - min number of samples at leaf node\n",
    "\n",
    "Trains best model using grid search to find best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Convert column names to strings\n",
    "X_train.columns = X_train.columns.astype(str)\n",
    "\n",
    "# hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "# grid search\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "Train on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=200,\n",
       "                       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=200,\n",
       "                       random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=200,\n",
       "                       random_state=42)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to strings for X_test\n",
    "X_test.columns = X_test.columns.astype(str)\n",
    "\n",
    "y_pred = best_rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8388324873096447\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.91      0.89       554\n",
      "         1.0       0.76      0.67      0.71       234\n",
      "\n",
      "    accuracy                           0.84       788\n",
      "   macro avg       0.81      0.79      0.80       788\n",
      "weighted avg       0.83      0.84      0.84       788\n",
      "\n",
      "Confusion Matrix:\n",
      " [[505  49]\n",
      " [ 78 156]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", classification_report)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(best_rf_model, X_train, y_train, cv=5)\n",
    "mean_cv_score = np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82063492 0.80793651 0.84285714 0.81587302 0.81428571]\n",
      "0.8203174603174602\n"
     ]
    }
   ],
   "source": [
    "# print cross-validation scores\n",
    "print(cv_scores)\n",
    "\n",
    "# print mean cross-validation score\n",
    "print(mean_cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRAPS - to be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         mean       std  variance  trimmed_mean   coef_var  \\\n",
      "timestamp                                                                    \n",
      "2002-10-02 15:00:00  1.226905  1.213061  1.471516      1.122097   0.988716   \n",
      "2002-10-02 16:00:00  1.574949  1.221473  1.491997      1.543573   0.775564   \n",
      "2002-10-02 17:00:00  2.149760  1.191689  1.420123      2.106185   0.554336   \n",
      "2002-10-02 18:00:00  0.310868  1.101726  1.213801      0.145472   3.544034   \n",
      "2002-10-02 19:00:00  0.318620  1.047779  1.097842      0.157313   3.288494   \n",
      "...                       ...       ...       ...           ...        ...   \n",
      "2003-06-27 04:00:00 -0.471985  0.000000  0.000000     -0.471985  -0.000000   \n",
      "2003-06-27 05:00:00 -0.471985  0.000000  0.000000     -0.471985  -0.000000   \n",
      "2003-06-27 06:00:00 -0.471985  0.000000  0.000000     -0.471985  -0.000000   \n",
      "2003-06-27 07:00:00 -0.043854  0.710295  0.504518     -0.170510 -16.196911   \n",
      "2003-06-27 08:00:00 -0.433736  0.184898  0.034187     -0.471985  -0.426290   \n",
      "\n",
      "                     inverse_coef_var   kurtosis             skewness  \\\n",
      "timestamp                                                               \n",
      "2002-10-02 15:00:00          1.011413    0.91359    0.818143897401409   \n",
      "2002-10-02 16:00:00          1.289384  -0.851444  0.37448986646001314   \n",
      "2002-10-02 17:00:00          1.803961   0.655764   0.4032319566283226   \n",
      "2002-10-02 18:00:00          0.282164   5.408604    2.321853664834024   \n",
      "2002-10-02 19:00:00          0.304091    1.79589   1.5741168925565419   \n",
      "...                               ...        ...                  ...   \n",
      "2003-06-27 04:00:00              -inf       -3.0                  0.0   \n",
      "2003-06-27 05:00:00              -inf       -3.0                  0.0   \n",
      "2003-06-27 06:00:00              -inf       -3.0                  0.0   \n",
      "2003-06-27 07:00:00         -0.061740  14.558287   3.3483008819450073   \n",
      "2003-06-27 08:00:00         -2.345818  24.242867    4.976883629003061   \n",
      "\n",
      "                     quantile_01  quantile_05  quantile_25  quantile_75  \\\n",
      "timestamp                                                                 \n",
      "2002-10-02 15:00:00    -0.360834    -0.275852     0.222187     2.080553   \n",
      "2002-10-02 16:00:00    -0.239315    -0.095129     0.659285     2.425188   \n",
      "2002-10-02 17:00:00    -0.128135     0.285931     1.441718     2.923928   \n",
      "2002-10-02 18:00:00    -0.471985    -0.464000    -0.309475     0.325158   \n",
      "2002-10-02 19:00:00    -0.471985    -0.471985    -0.450271     0.775564   \n",
      "...                          ...          ...          ...          ...   \n",
      "2003-06-27 04:00:00    -0.471985    -0.471985    -0.471985    -0.471985   \n",
      "2003-06-27 05:00:00    -0.471985    -0.471985    -0.471985    -0.471985   \n",
      "2003-06-27 06:00:00    -0.471985    -0.471985    -0.471985    -0.471985   \n",
      "2003-06-27 07:00:00    -0.471985    -0.471985    -0.471985     0.158444   \n",
      "2003-06-27 08:00:00    -0.471985    -0.471985    -0.471985    -0.471985   \n",
      "\n",
      "                     quantile_95  quantile_99  label  \n",
      "timestamp                                             \n",
      "2002-10-02 15:00:00     3.160409     4.394593    0.0  \n",
      "2002-10-02 16:00:00     3.633651     4.009415    0.0  \n",
      "2002-10-02 17:00:00     4.069208     5.264558    0.0  \n",
      "2002-10-02 18:00:00     2.071027     4.346597    0.0  \n",
      "2002-10-02 19:00:00     2.642056     3.481732    0.0  \n",
      "...                          ...          ...    ...  \n",
      "2003-06-27 04:00:00    -0.471985    -0.471985    1.0  \n",
      "2003-06-27 05:00:00    -0.471985    -0.471985    1.0  \n",
      "2003-06-27 06:00:00    -0.471985    -0.471985    1.0  \n",
      "2003-06-27 07:00:00     0.921124     2.616166    1.0  \n",
      "2003-06-27 08:00:00    -0.471985     0.393636    1.0  \n",
      "\n",
      "[6426 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# 'timestamp' as index\n",
    "normal_df = normal_df.set_index('timestamp')\n",
    "\n",
    "# extract 'label' from normal_df\n",
    "label_hourly = normal_df.resample('H')['label'].first()\n",
    "\n",
    "# merge features_hourly with label_hourly\n",
    "merged_hourly_df = pd.merge(features_hourly, label_hourly, left_index=True, right_index=True)\n",
    "\n",
    "# Print\n",
    "print(merged_hourly_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'label' from normal_df with resampling\n",
    "label_hourly = normal_df.resample('H')['label'].first()\n",
    "\n",
    "# Align labels with features\n",
    "merged_hourly_df['label'] = label_hourly.loc[merged_hourly_df.index]\n",
    "\n",
    "# Drop rows with NaN labels\n",
    "merged_hourly_df = merged_hourly_df.dropna(subset=['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_12084\\3898015565.py:32: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  kurtosis_value_hourly = normmal_feature_df.resample('H')['activity_norm'].apply(lambda x: kurtosis(x) if len(x) > 0 else np.nan)\n",
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_12084\\3898015565.py:33: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  skewness_value_hourly = normmal_feature_df.resample('H')['activity_norm'].apply(lambda x: skew(x) if len(x) > 0 else np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         mean       std  variance  trimmed_mean   coef_var  \\\n",
      "timestamp                                                                    \n",
      "2002-10-02 15:00:00  1.226905  1.213061  1.471516      1.122097   0.988716   \n",
      "2002-10-02 16:00:00  1.574949  1.221473  1.491997      1.543573   0.775564   \n",
      "2002-10-02 17:00:00  2.149760  1.191689  1.420123      2.106185   0.554336   \n",
      "2002-10-02 18:00:00  0.310868  1.101726  1.213801      0.145472   3.544034   \n",
      "2002-10-02 19:00:00  0.318620  1.047779  1.097842      0.157313   3.288494   \n",
      "...                       ...       ...       ...           ...        ...   \n",
      "2003-06-27 04:00:00 -0.471985  0.000000  0.000000     -0.471985  -0.000000   \n",
      "2003-06-27 05:00:00 -0.471985  0.000000  0.000000     -0.471985  -0.000000   \n",
      "2003-06-27 06:00:00 -0.471985  0.000000  0.000000     -0.471985  -0.000000   \n",
      "2003-06-27 07:00:00 -0.043854  0.710295  0.504518     -0.170510 -16.196911   \n",
      "2003-06-27 08:00:00 -0.433736  0.184898  0.034187     -0.471985  -0.426290   \n",
      "\n",
      "                     inverse_coef_var   kurtosis  skewness  quantile_01  \\\n",
      "timestamp                                                                 \n",
      "2002-10-02 15:00:00          1.011413   0.913590  0.818144          0.0   \n",
      "2002-10-02 16:00:00          1.289384  -0.851444  0.374490          0.0   \n",
      "2002-10-02 17:00:00          1.803961   0.655764  0.403232          0.0   \n",
      "2002-10-02 18:00:00          0.282164   5.408604  2.321854          0.0   \n",
      "2002-10-02 19:00:00          0.304091   1.795890  1.574117          0.0   \n",
      "...                               ...        ...       ...          ...   \n",
      "2003-06-27 04:00:00              -inf        NaN       NaN          1.0   \n",
      "2003-06-27 05:00:00              -inf        NaN       NaN          1.0   \n",
      "2003-06-27 06:00:00              -inf        NaN       NaN          1.0   \n",
      "2003-06-27 07:00:00         -0.061740  14.558287  3.348301          1.0   \n",
      "2003-06-27 08:00:00         -2.345818  24.242867  4.976884          1.0   \n",
      "\n",
      "                     quantile_05  quantile_25  quantile_75  quantile_95  \\\n",
      "timestamp                                                                 \n",
      "2002-10-02 15:00:00    -0.360834    -0.275852     0.222187     2.080553   \n",
      "2002-10-02 16:00:00    -0.239315    -0.095129     0.659285     2.425188   \n",
      "2002-10-02 17:00:00    -0.128135     0.285931     1.441718     2.923928   \n",
      "2002-10-02 18:00:00    -0.471985    -0.464000    -0.309475     0.325158   \n",
      "2002-10-02 19:00:00    -0.471985    -0.471985    -0.450271     0.775564   \n",
      "...                          ...          ...          ...          ...   \n",
      "2003-06-27 04:00:00    -0.471985    -0.471985    -0.471985    -0.471985   \n",
      "2003-06-27 05:00:00    -0.471985    -0.471985    -0.471985    -0.471985   \n",
      "2003-06-27 06:00:00    -0.471985    -0.471985    -0.471985    -0.471985   \n",
      "2003-06-27 07:00:00    -0.471985    -0.471985    -0.471985     0.158444   \n",
      "2003-06-27 08:00:00    -0.471985    -0.471985    -0.471985    -0.471985   \n",
      "\n",
      "                     quantile_99     label  \n",
      "timestamp                                   \n",
      "2002-10-02 15:00:00     3.160409  4.394593  \n",
      "2002-10-02 16:00:00     3.633651  4.009415  \n",
      "2002-10-02 17:00:00     4.069208  5.264558  \n",
      "2002-10-02 18:00:00     2.071027  4.346597  \n",
      "2002-10-02 19:00:00     2.642056  3.481732  \n",
      "...                          ...       ...  \n",
      "2003-06-27 04:00:00    -0.471985 -0.471985  \n",
      "2003-06-27 05:00:00    -0.471985 -0.471985  \n",
      "2003-06-27 06:00:00    -0.471985 -0.471985  \n",
      "2003-06-27 07:00:00     0.921124  2.616166  \n",
      "2003-06-27 08:00:00    -0.471985  0.393636  \n",
      "\n",
      "[6426 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.stats import mstats\n",
    "\n",
    "# Copy df to avoid changing the original\n",
    "normmal_feature_df = normal_df.copy()\n",
    "\n",
    "# 'timestamp' as index\n",
    "normmal_feature_df = normmal_feature_df.set_index('timestamp')\n",
    "normal_df = normal_df.set_index('timestamp')\n",
    "\n",
    "# Mean, standard deviation, and variance for each hour\n",
    "mean_value_hourly = normmal_feature_df.resample('H')['activity_norm'].mean()\n",
    "std_value_hourly = normmal_feature_df.resample('H')['activity_norm'].std()\n",
    "variance_value_hourly = normmal_feature_df.resample('H')['activity_norm'].var()\n",
    "\n",
    "# Trimmed Mean (using 5% trimming??) for each hour\n",
    "def calculate_trimmed_mean(x):\n",
    "    if len(x) > 0:\n",
    "        return np.mean(x[(x >= np.percentile(x, 5)) & (x <= np.percentile(x, 95))])\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "trimmed_mean_value_hourly = normmal_feature_df.resample('H')['activity_norm'].apply(calculate_trimmed_mean)\n",
    "\n",
    "# Coefficient of Variation and Inverse Coefficient of Variation for each hour\n",
    "coef_var_value_hourly = std_value_hourly / mean_value_hourly\n",
    "inverse_coef_var_value_hourly = mean_value_hourly / std_value_hourly\n",
    "\n",
    "# Kurtosis and Skewness for each hour\n",
    "kurtosis_value_hourly = normmal_feature_df.resample('H')['activity_norm'].apply(lambda x: kurtosis(x) if len(x) > 0 else np.nan)\n",
    "skewness_value_hourly = normmal_feature_df.resample('H')['activity_norm'].apply(lambda x: skew(x) if len(x) > 0 else np.nan)\n",
    "\n",
    "# Quantiles (1%, 5%, 25%, 75%, 95%, 99%) for each hour\n",
    "quantiles_value_hourly = normmal_feature_df.resample('H')['activity_norm'].quantile([0.01, 0.05, 0.25, 0.75, 0.95, 0.99])\n",
    "\n",
    "# Extract 'label' from normal_df with resampling\n",
    "label_hourly = normal_df.resample('H')['label'].first()\n",
    "\n",
    "# Align labels with features\n",
    "features_hourly = pd.DataFrame({\n",
    "    'mean': mean_value_hourly,\n",
    "    'std': std_value_hourly,\n",
    "    'variance': variance_value_hourly,\n",
    "    'trimmed_mean': trimmed_mean_value_hourly,\n",
    "    'coef_var': coef_var_value_hourly,\n",
    "    'inverse_coef_var': inverse_coef_var_value_hourly,\n",
    "    'kurtosis': kurtosis_value_hourly,\n",
    "    'skewness': skewness_value_hourly,\n",
    "    'label': label_hourly  # Add 'label' column\n",
    "})\n",
    "\n",
    "# Add Quantiles\n",
    "features_hourly = pd.concat([features_hourly, quantiles_value_hourly.unstack(level=-1)], axis=1)\n",
    "\n",
    "# Rename columns\n",
    "features_hourly.columns = ['mean', 'std', 'variance', 'trimmed_mean', 'coef_var', 'inverse_coef_var', 'kurtosis', 'skewness', 'quantile_01', 'quantile_05', 'quantile_25', 'quantile_75', 'quantile_95', 'quantile_99', 'label']\n",
    "\n",
    "# Print\n",
    "print(features_hourly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoona\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\ma\\core.py:5288: RuntimeWarning: Mean of empty slice.\n",
      "  result = super().mean(axis=axis, dtype=dtype, **kwargs)[()]\n",
      "c:\\Users\\zoona\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:182: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         mean       std  variance  trimmed_mean   coef_var  \\\n",
      "timestamp                                                                    \n",
      "2002-10-02 15:00:00  1.226905  1.213061  1.471516      1.122097   0.988716   \n",
      "2002-10-02 16:00:00  1.574949  1.221473  1.491997      1.543573   0.775564   \n",
      "2002-10-02 17:00:00  2.149760  1.191689  1.420123      2.106185   0.554336   \n",
      "2002-10-02 18:00:00  0.310868  1.101726  1.213801      0.145472   3.544034   \n",
      "2002-10-02 19:00:00  0.318620  1.047779  1.097842      0.157313   3.288494   \n",
      "...                       ...       ...       ...           ...        ...   \n",
      "2003-06-27 04:00:00 -0.471985  0.000000  0.000000     -0.471985  -0.000000   \n",
      "2003-06-27 05:00:00 -0.471985  0.000000  0.000000     -0.471985  -0.000000   \n",
      "2003-06-27 06:00:00 -0.471985  0.000000  0.000000     -0.471985  -0.000000   \n",
      "2003-06-27 07:00:00 -0.043854  0.710295  0.504518     -0.170510 -16.196911   \n",
      "2003-06-27 08:00:00 -0.433736  0.184898  0.034187     -0.471985  -0.426290   \n",
      "\n",
      "                     inverse_coef_var   kurtosis             skewness  \\\n",
      "timestamp                                                               \n",
      "2002-10-02 15:00:00          1.011413    0.91359    0.818143897401409   \n",
      "2002-10-02 16:00:00          1.289384  -0.851444  0.37448986646001314   \n",
      "2002-10-02 17:00:00          1.803961   0.655764   0.4032319566283226   \n",
      "2002-10-02 18:00:00          0.282164   5.408604    2.321853664834024   \n",
      "2002-10-02 19:00:00          0.304091    1.79589   1.5741168925565419   \n",
      "...                               ...        ...                  ...   \n",
      "2003-06-27 04:00:00              -inf       -3.0                  0.0   \n",
      "2003-06-27 05:00:00              -inf       -3.0                  0.0   \n",
      "2003-06-27 06:00:00              -inf       -3.0                  0.0   \n",
      "2003-06-27 07:00:00         -0.061740  14.558287   3.3483008819450073   \n",
      "2003-06-27 08:00:00         -2.345818  24.242867    4.976883629003061   \n",
      "\n",
      "                     quantile_01  quantile_05  quantile_25  quantile_75  \\\n",
      "timestamp                                                                 \n",
      "2002-10-02 15:00:00    -0.360834    -0.275852     0.222187     2.080553   \n",
      "2002-10-02 16:00:00    -0.239315    -0.095129     0.659285     2.425188   \n",
      "2002-10-02 17:00:00    -0.128135     0.285931     1.441718     2.923928   \n",
      "2002-10-02 18:00:00    -0.471985    -0.464000    -0.309475     0.325158   \n",
      "2002-10-02 19:00:00    -0.471985    -0.471985    -0.450271     0.775564   \n",
      "...                          ...          ...          ...          ...   \n",
      "2003-06-27 04:00:00    -0.471985    -0.471985    -0.471985    -0.471985   \n",
      "2003-06-27 05:00:00    -0.471985    -0.471985    -0.471985    -0.471985   \n",
      "2003-06-27 06:00:00    -0.471985    -0.471985    -0.471985    -0.471985   \n",
      "2003-06-27 07:00:00    -0.471985    -0.471985    -0.471985     0.158444   \n",
      "2003-06-27 08:00:00    -0.471985    -0.471985    -0.471985    -0.471985   \n",
      "\n",
      "                     quantile_95  quantile_99  \n",
      "timestamp                                      \n",
      "2002-10-02 15:00:00     3.160409     4.394593  \n",
      "2002-10-02 16:00:00     3.633651     4.009415  \n",
      "2002-10-02 17:00:00     4.069208     5.264558  \n",
      "2002-10-02 18:00:00     2.071027     4.346597  \n",
      "2002-10-02 19:00:00     2.642056     3.481732  \n",
      "...                          ...          ...  \n",
      "2003-06-27 04:00:00    -0.471985    -0.471985  \n",
      "2003-06-27 05:00:00    -0.471985    -0.471985  \n",
      "2003-06-27 06:00:00    -0.471985    -0.471985  \n",
      "2003-06-27 07:00:00     0.921124     2.616166  \n",
      "2003-06-27 08:00:00    -0.471985     0.393636  \n",
      "\n",
      "[6426 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# copy df to avoid changing the original\n",
    "normmal_feature_df = normal_df.copy()\n",
    "\n",
    "\n",
    "# 'timestamp' as index\n",
    "normmal_feature_df = normmal_feature_df.set_index('timestamp')\n",
    "\n",
    "# mean, standard deviation, and variance for each hour\n",
    "mean_value_hourly = normmal_feature_df.resample('H')['activity_norm'].mean()\n",
    "std_value_hourly = normmal_feature_df.resample('H')['activity_norm'].std()\n",
    "variance_value_hourly = normmal_feature_df.resample('H')['activity_norm'].var()\n",
    "\n",
    "# Trimmed Mean (using 5% trimming??) for each hour\n",
    "def calculate_trimmed_mean(x):\n",
    "    if len(x) > 0:\n",
    "        return np.mean(x[(x >= np.percentile(x, 5)) & (x <= np.percentile(x, 95))])\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "trimmed_mean_value_hourly = normmal_feature_df.resample('H')['activity_norm'].apply(calculate_trimmed_mean)\n",
    "\n",
    "# Coefficient of Variation and Inverse Coefficient of Variation for each hour\n",
    "coef_var_value_hourly = std_value_hourly / mean_value_hourly\n",
    "inverse_coef_var_value_hourly = mean_value_hourly / std_value_hourly\n",
    "\n",
    "#  Kurtosis and Skewness for each hour\n",
    "#kurtosis_value_hourly = normmal_feature_df.resample('H')['activity_norm'].apply(lambda x: kurtosis(x) if len(x) > 0 else np.nan)\n",
    "#skewness_value_hourly = normmal_feature_df.resample('H')['activity_norm'].apply(lambda x: skew(x) if len(x) > 0 else np.nan)\n",
    "kurtosis_value_hourly = normmal_feature_df.resample('H')['activity_norm'].apply(lambda x: mstats.kurtosis(x))\n",
    "skewness_value_hourly = normmal_feature_df.resample('H')['activity_norm'].apply(lambda x: mstats.skew(x))\n",
    "\n",
    "# Quantiles (1%, 5%, 25%, 75%, 95%, 99%) for each hour\n",
    "quantiles_value_hourly = normmal_feature_df.resample('H')['activity_norm'].quantile([0.01, 0.05, 0.25, 0.75, 0.95, 0.99])\n",
    "\n",
    "# df to store the features\n",
    "features_hourly = pd.DataFrame({\n",
    "    'mean': mean_value_hourly,\n",
    "    'std': std_value_hourly,\n",
    "    'variance': variance_value_hourly,\n",
    "    'trimmed_mean': trimmed_mean_value_hourly,\n",
    "    'coef_var': coef_var_value_hourly,\n",
    "    'inverse_coef_var': inverse_coef_var_value_hourly,\n",
    "    'kurtosis': kurtosis_value_hourly,\n",
    "    'skewness': skewness_value_hourly\n",
    "})\n",
    "\n",
    "# Add Quantiles\n",
    "features_hourly = pd.concat([features_hourly, quantiles_value_hourly.unstack(level=-1)], axis=1)\n",
    "\n",
    "# Rename columns\n",
    "features_hourly.columns = ['mean', 'std', 'variance', 'trimmed_mean', 'coef_var', 'inverse_coef_var', 'kurtosis', 'skewness', 'quantile_01', 'quantile_05', 'quantile_25', 'quantile_75', 'quantile_95', 'quantile_99']\n",
    "\n",
    "# Print\n",
    "print(features_hourly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features dataframe:\n",
      "DatetimeIndex(['2002-10-02 15:00:00', '2002-10-02 16:00:00',\n",
      "               '2002-10-02 17:00:00', '2002-10-02 18:00:00',\n",
      "               '2002-10-02 19:00:00', '2002-10-02 20:00:00',\n",
      "               '2002-10-02 21:00:00', '2002-10-02 22:00:00',\n",
      "               '2002-10-02 23:00:00', '2002-10-03 00:00:00',\n",
      "               ...\n",
      "               '2003-06-26 23:00:00', '2003-06-27 00:00:00',\n",
      "               '2003-06-27 01:00:00', '2003-06-27 02:00:00',\n",
      "               '2003-06-27 03:00:00', '2003-06-27 04:00:00',\n",
      "               '2003-06-27 05:00:00', '2003-06-27 06:00:00',\n",
      "               '2003-06-27 07:00:00', '2003-06-27 08:00:00'],\n",
      "              dtype='datetime64[ns]', name='timestamp', length=6426, freq='H')\n",
      "datetime64[ns]\n",
      "\n",
      "Label dataframe:\n",
      "DatetimeIndex(['2003-03-18 15:00:00', '2003-03-18 15:01:00',\n",
      "               '2003-03-18 15:02:00', '2003-03-18 15:03:00',\n",
      "               '2003-03-18 15:04:00', '2003-03-18 15:05:00',\n",
      "               '2003-03-18 15:06:00', '2003-03-18 15:07:00',\n",
      "               '2003-03-18 15:08:00', '2003-03-18 15:09:00',\n",
      "               ...\n",
      "               '2003-06-27 08:33:00', '2003-06-27 08:34:00',\n",
      "               '2003-06-27 08:35:00', '2003-06-27 08:36:00',\n",
      "               '2003-06-27 08:37:00', '2003-06-27 08:38:00',\n",
      "               '2003-06-27 08:39:00', '2003-06-27 08:40:00',\n",
      "               '2003-06-27 08:41:00', '2003-06-27 08:42:00'],\n",
      "              dtype='datetime64[ns]', name='timestamp', length=306813, freq=None)\n",
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the index type and format for features_hourly\n",
    "print(\"Features dataframe:\")\n",
    "print(features_hourly.index)\n",
    "print(features_hourly.index.dtype)\n",
    "\n",
    "# Check the index type and format for normal_df\n",
    "print(\"\\nLabel dataframe:\")\n",
    "print(normal_df.index)\n",
    "print(normal_df.index.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v1 first hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       mean  std  variance  quantile_01  quantile_05  quantile_25  \\\n",
      "0 -0.484596  0.0       0.0    -0.484596    -0.484596    -0.484596   \n",
      "1  0.354670  0.0       0.0     0.354670     0.354670     0.354670   \n",
      "2 -0.484596  0.0       0.0    -0.484596    -0.484596    -0.484596   \n",
      "3 -0.380374  0.0       0.0    -0.380374    -0.380374    -0.380374   \n",
      "4 -0.443456  0.0       0.0    -0.443456    -0.443456    -0.443456   \n",
      "\n",
      "   quantile_75  quantile_95  quantile_99 skewness  kurtosis  coef_var  \\\n",
      "0    -0.484596    -0.484596    -0.484596      0.0      -3.0      -0.0   \n",
      "1     0.354670     0.354670     0.354670      0.0      -3.0       0.0   \n",
      "2    -0.484596    -0.484596    -0.484596      0.0      -3.0      -0.0   \n",
      "3    -0.380374    -0.380374    -0.380374      0.0      -3.0      -0.0   \n",
      "4    -0.443456    -0.443456    -0.443456      0.0      -3.0      -0.0   \n",
      "\n",
      "   inverse_coef_var  \n",
      "0     -4.845962e+08  \n",
      "1      3.546704e+08  \n",
      "2     -4.845962e+08  \n",
      "3     -3.803736e+08  \n",
      "4     -4.434557e+08   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby to calculate other features\n",
    "features_first_hour = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].agg([np.mean,\n",
    "                                lambda x: mstats.trimmed_std(x, 0.1),  # Adjust the trim parameter as needed\n",
    "                                lambda x: mstats.trimmed_var(x, 0.1),\n",
    "                                lambda x: np.quantile(x, 0.01), \n",
    "                                lambda x: np.quantile(x, 0.05),\n",
    "                                lambda x: np.quantile(x, 0.25),\n",
    "                                lambda x: np.quantile(x, 0.75),\n",
    "                                lambda x: np.quantile(x, 0.95),\n",
    "                                lambda x: np.quantile(x, 0.99)]) \n",
    "\n",
    "features_first_hour.columns = ['mean', 'std', 'variance', 'quantile_01', 'quantile_05', 'quantile_25', 'quantile_75', 'quantile_95', 'quantile_99']\n",
    "\n",
    "# Calculate skewness and kurtosis separately\n",
    "skewness_values = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].apply(mstats.skew)\n",
    "kurtosis_values = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].apply(mstats.kurtosis)\n",
    "\n",
    "# Assign skewness and kurtosis to the dataframe\n",
    "features_first_hour['skewness'] = skewness_values\n",
    "features_first_hour['kurtosis'] = kurtosis_values\n",
    "\n",
    "# Calculate remaining features\n",
    "features_first_hour['coef_var'] = features_first_hour['std'] / features_first_hour['mean'] \n",
    "features_first_hour['inverse_coef_var'] = features_first_hour['mean'] / (np.abs(features_first_hour['std']) + 1e-9)\n",
    "\n",
    "\n",
    "# print the first 5 rows\n",
    "print(features_first_hour.head(5), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v2 mean hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       mean  std  variance  quantile_01  quantile_05  quantile_25  \\\n",
      "0  0.696000  0.0       0.0     0.696000     0.696000     0.696000   \n",
      "1  0.453893  0.0       0.0     0.453893     0.453893     0.453893   \n",
      "2  0.432865  0.0       0.0     0.432865     0.432865     0.432865   \n",
      "3  0.196942  0.0       0.0     0.196942     0.196942     0.196942   \n",
      "4  0.274151  0.0       0.0     0.274151     0.274151     0.274151   \n",
      "\n",
      "   quantile_75  quantile_95  quantile_99 skewness  kurtosis  coef_var  \\\n",
      "0     0.696000     0.696000     0.696000      0.0      -3.0       0.0   \n",
      "1     0.453893     0.453893     0.453893      0.0      -3.0       0.0   \n",
      "2     0.432865     0.432865     0.432865      0.0      -3.0       0.0   \n",
      "3     0.196942     0.196942     0.196942      0.0      -3.0       0.0   \n",
      "4     0.274151     0.274151     0.274151      0.0      -3.0       0.0   \n",
      "\n",
      "   inverse_coef_var  \n",
      "0      6.960005e+08  \n",
      "1      4.538928e+08  \n",
      "2      4.328654e+08  \n",
      "3      1.969423e+08  \n",
      "4      2.741511e+08   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby to calculate other features\n",
    "features_mean_hour = subset_mean_hour_df.groupby(subset_mean_hour_df.index)['activity_norm'].agg([np.mean,\n",
    "                                lambda x: mstats.trimmed_std(x, 0.1),  # Adjust the trim parameter as needed\n",
    "                                lambda x: mstats.trimmed_var(x, 0.1),\n",
    "                                lambda x: np.quantile(x, 0.01), \n",
    "                                lambda x: np.quantile(x, 0.05),\n",
    "                                lambda x: np.quantile(x, 0.25),\n",
    "                                lambda x: np.quantile(x, 0.75),\n",
    "                                lambda x: np.quantile(x, 0.95),\n",
    "                                lambda x: np.quantile(x, 0.99)]) \n",
    "\n",
    "features_mean_hour.columns = ['mean', 'std', 'variance', 'quantile_01', 'quantile_05', 'quantile_25', 'quantile_75', 'quantile_95', 'quantile_99']\n",
    "\n",
    "# skewness and kurtosis \n",
    "skewness_values = subset_mean_hour_df.groupby(subset_mean_hour_df.index)['activity_norm'].apply(mstats.skew)\n",
    "kurtosis_values = subset_mean_hour_df.groupby(subset_mean_hour_df.index)['activity_norm'].apply(mstats.kurtosis)\n",
    "\n",
    "# assign skewness and kurtosis to the dataframe\n",
    "features_mean_hour['skewness'] = skewness_values\n",
    "features_mean_hour['kurtosis'] = kurtosis_values\n",
    "\n",
    "# coef and inverse_coef features\n",
    "features_mean_hour['coef_var'] = features_mean_hour['std'] / features_mean_hour['mean'] \n",
    "features_mean_hour['inverse_coef_var'] = features_mean_hour['mean'] / (np.abs(features_mean_hour['std']) + 1e-9)\n",
    "\n",
    "\n",
    "# print the first 5 rows\n",
    "print(features_mean_hour.head(5), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5120, 13)\n",
      "(5120, 13)\n"
     ]
    }
   ],
   "source": [
    "print(features_first_hour.shape)\n",
    "print(features_mean_hour.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoona\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1649: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  f = lambda x: func(x, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       mean  std  variance  quantile_01  quantile_05  quantile_25  \\\n",
      "0 -0.484596  NaN       NaN    -0.484596    -0.484596    -0.484596   \n",
      "1  0.354670  NaN       NaN     0.354670     0.354670     0.354670   \n",
      "2 -0.484596  NaN       NaN    -0.484596    -0.484596    -0.484596   \n",
      "3 -0.380374  NaN       NaN    -0.380374    -0.380374    -0.380374   \n",
      "4 -0.443456  NaN       NaN    -0.443456    -0.443456    -0.443456   \n",
      "\n",
      "   quantile_75  quantile_95  quantile_99  skewness  kurtosis  coef_var  \\\n",
      "0    -0.484596    -0.484596    -0.484596       NaN       NaN       NaN   \n",
      "1     0.354670     0.354670     0.354670       NaN       NaN       NaN   \n",
      "2    -0.484596    -0.484596    -0.484596       NaN       NaN       NaN   \n",
      "3    -0.380374    -0.380374    -0.380374       NaN       NaN       NaN   \n",
      "4    -0.443456    -0.443456    -0.443456       NaN       NaN       NaN   \n",
      "\n",
      "   inverse_coef_var  \n",
      "0               NaN  \n",
      "1               NaN  \n",
      "2               NaN  \n",
      "3               NaN  \n",
      "4               NaN   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_first_hour = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].agg([np.mean, np.std, np.var, \n",
    "                                lambda x: np.quantile(x, 0.01), \n",
    "                                lambda x: np.quantile(x, 0.05),\n",
    "                                lambda x: np.quantile(x, 0.25),\n",
    "                                lambda x: np.quantile(x, 0.75),\n",
    "                                lambda x: np.quantile(x, 0.95),\n",
    "                                lambda x: np.quantile(x, 0.99),\n",
    "                                skew, kurtosis]) \n",
    "features_first_hour.columns = ['mean', 'std', 'variance', 'quantile_01', 'quantile_05', 'quantile_25', 'quantile_75', 'quantile_95', 'quantile_99','skewness', 'kurtosis']\n",
    "\n",
    "features_first_hour['coef_var'] = features_first_hour['std'] / features_first_hour['mean'] \n",
    "features_first_hour['inverse_coef_var'] = features_first_hour['mean'] / features_first_hour['std']\n",
    "\n",
    "# print the first 5 rows\n",
    "print(features_first_hour.head(5), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v1 first hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_44496\\3856034703.py:20: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  kurtosis_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].apply(lambda x: kurtosis(x))\n",
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_44496\\3856034703.py:23: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  skewness_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].apply(lambda x: skew(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          mean  std  variance  trimmed_mean  coef_var  inverse_coef_var  \\\n",
      "0    -0.484596  NaN       NaN     -0.484596       NaN               NaN   \n",
      "1     0.354670  NaN       NaN      0.354670       NaN               NaN   \n",
      "2    -0.484596  NaN       NaN     -0.484596       NaN               NaN   \n",
      "3    -0.380374  NaN       NaN     -0.380374       NaN               NaN   \n",
      "4    -0.443456  NaN       NaN     -0.443456       NaN               NaN   \n",
      "...        ...  ...       ...           ...       ...               ...   \n",
      "5115 -0.470883  NaN       NaN     -0.470883       NaN               NaN   \n",
      "5116 -0.193870  NaN       NaN     -0.193870       NaN               NaN   \n",
      "5117 -0.476368  NaN       NaN     -0.476368       NaN               NaN   \n",
      "5118 -0.476368  NaN       NaN     -0.476368       NaN               NaN   \n",
      "5119 -0.484596  NaN       NaN     -0.484596       NaN               NaN   \n",
      "\n",
      "      kurtosis  skewness  quantile_01  quantile_05  quantile_25  quantile_75  \\\n",
      "0          NaN       NaN    -0.484596    -0.484596    -0.484596    -0.484596   \n",
      "1          NaN       NaN     0.354670     0.354670     0.354670     0.354670   \n",
      "2          NaN       NaN    -0.484596    -0.484596    -0.484596    -0.484596   \n",
      "3          NaN       NaN    -0.380374    -0.380374    -0.380374    -0.380374   \n",
      "4          NaN       NaN    -0.443456    -0.443456    -0.443456    -0.443456   \n",
      "...        ...       ...          ...          ...          ...          ...   \n",
      "5115       NaN       NaN    -0.470883    -0.470883    -0.470883    -0.470883   \n",
      "5116       NaN       NaN    -0.193870    -0.193870    -0.193870    -0.193870   \n",
      "5117       NaN       NaN    -0.476368    -0.476368    -0.476368    -0.476368   \n",
      "5118       NaN       NaN    -0.476368    -0.476368    -0.476368    -0.476368   \n",
      "5119       NaN       NaN    -0.484596    -0.484596    -0.484596    -0.484596   \n",
      "\n",
      "      quantile_95  quantile_99  \n",
      "0       -0.484596    -0.484596  \n",
      "1        0.354670     0.354670  \n",
      "2       -0.484596    -0.484596  \n",
      "3       -0.380374    -0.380374  \n",
      "4       -0.443456    -0.443456  \n",
      "...           ...          ...  \n",
      "5115    -0.470883    -0.470883  \n",
      "5116    -0.193870    -0.193870  \n",
      "5117    -0.476368    -0.476368  \n",
      "5118    -0.476368    -0.476368  \n",
      "5119    -0.484596    -0.484596  \n",
      "\n",
      "[5120 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Mean\n",
    "mean_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].mean()\n",
    "\n",
    "# Calculate Standard Deviation\n",
    "std_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].std()\n",
    "\n",
    "# Calculate Variance\n",
    "variance_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].var()\n",
    "\n",
    "# Calculate Trimmed Mean (using 5% trimming as an example)\n",
    "trimmed_mean_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].apply(lambda x: np.mean(x[(x >= np.percentile(x, 5)) & (x <= np.percentile(x, 95))]))\n",
    "\n",
    "# Calculate Coefficient of Variation\n",
    "coef_var_value = std_value / mean_value\n",
    "\n",
    "# Calculate Inverse Coefficient of Variation\n",
    "inverse_coef_var_value = mean_value / std_value\n",
    "\n",
    "# Calculate Kurtosis\n",
    "kurtosis_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].apply(lambda x: kurtosis(x))\n",
    "\n",
    "# Calculate Skewness\n",
    "skewness_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].apply(lambda x: skew(x))\n",
    "\n",
    "# Calculate Quantiles (1%, 5%, 25%, 75%, 95%, 99%)\n",
    "quantiles_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].quantile([0.01, 0.05, 0.25, 0.75, 0.95, 0.99]).unstack(level=-1)\n",
    "\n",
    "# Create a dataframe to store the features\n",
    "features_first_hour = pd.DataFrame({\n",
    "    'mean': mean_value,\n",
    "    'std': std_value,\n",
    "    'variance': variance_value,\n",
    "    'trimmed_mean': trimmed_mean_value,\n",
    "    'coef_var': coef_var_value,\n",
    "    'inverse_coef_var': inverse_coef_var_value,\n",
    "    'kurtosis': kurtosis_value,\n",
    "    'skewness': skewness_value\n",
    "})\n",
    "\n",
    "# Add Quantiles to the dataframe\n",
    "features_first_hour = pd.concat([features_first_hour, quantiles_value], axis=1)\n",
    "\n",
    "# Rename columns for clarity\n",
    "features_first_hour.columns = ['mean', 'std', 'variance', 'trimmed_mean', 'coef_var', 'inverse_coef_var', 'kurtosis', 'skewness', 'quantile_01', 'quantile_05', 'quantile_25', 'quantile_75', 'quantile_95', 'quantile_99']\n",
    "\n",
    "# Print or further use features_hour\n",
    "print(features_first_hour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate features per time window\n",
    "features = []\n",
    "for idx, df_window in df.groupby(df.index): # grouped by time window\n",
    "    features_dict = {\n",
    "        'mean': df_window['activity_norm'].mean(),\n",
    "        'std': df_window['activity_norm'].std(),\n",
    "        'variance': df_window['activity_norm'].var(),\n",
    "        'variance': df_window['activity_norm'].var(),\n",
    "        'trimmed_mean': df_window['activity_norm'].quantile(0.05),\n",
    "        'coef_var': df_window['activity_norm'].std() / df_window['activity_norm'].mean(),\n",
    "        'inverse_coef_var': df_window['activity_norm'].mean() / df_window['activity_norm'].std(),\n",
    "        'kurtosis': df_window['activity_norm'].kurtosis(),\n",
    "        'skewness': df_window['activity_norm'].skew(),\n",
    "        'quantile_1': df_window['activity_norm'].quantile(0.01),\n",
    "        'quantile_5': df_window['activity_norm'].quantile(0.05),\n",
    "        'quantile_25': df_window['activity_norm'].quantile(0.25),\n",
    "        'quantile_75': df_window['activity_norm'].quantile(0.75),\n",
    "        'quantile_95': df_window['activity_norm'].quantile(0.95),\n",
    "        'quantile_99': df_window['activity_norm'].quantile(0.99)\n",
    "    }\n",
    "    \n",
    "    features.append(features_dict) \n",
    "\n",
    "features_df = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         mean  std  variance  quantile_01  quantile_05  \\\n",
      "timestamp                                                                \n",
      "2002-10-02 15:00:00  0.030959  NaN       NaN     0.030959     0.030959   \n",
      "2002-10-02 16:00:00  1.656566  NaN       NaN     1.656566     1.656566   \n",
      "2002-10-02 17:00:00  2.274023  NaN       NaN     2.274023     2.274023   \n",
      "2002-10-02 18:00:00  3.926951  NaN       NaN     3.926951     3.926951   \n",
      "2002-10-02 19:00:00  1.126536  NaN       NaN     1.126536     1.126536   \n",
      "\n",
      "                     quantile_25  quantile_75  quantile_95  quantile_99  \\\n",
      "timestamp                                                                 \n",
      "2002-10-02 15:00:00     0.030959     0.030959     0.030959     0.030959   \n",
      "2002-10-02 16:00:00     1.656566     1.656566     1.656566     1.656566   \n",
      "2002-10-02 17:00:00     2.274023     2.274023     2.274023     2.274023   \n",
      "2002-10-02 18:00:00     3.926951     3.926951     3.926951     3.926951   \n",
      "2002-10-02 19:00:00     1.126536     1.126536     1.126536     1.126536   \n",
      "\n",
      "                     skewness  kurtosis  coef_var  inverse_coef_var  \n",
      "timestamp                                                            \n",
      "2002-10-02 15:00:00       NaN       NaN       NaN               NaN  \n",
      "2002-10-02 16:00:00       NaN       NaN       NaN               NaN  \n",
      "2002-10-02 17:00:00       NaN       NaN       NaN               NaN  \n",
      "2002-10-02 18:00:00       NaN       NaN       NaN               NaN  \n",
      "2002-10-02 19:00:00       NaN       NaN       NaN               NaN  \n"
     ]
    }
   ],
   "source": [
    "print(features_hour.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
