{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "\n",
    "Possible explorations:\n",
    "\n",
    "* Rework Rodriguez - data leakage.\n",
    "* Male, Female, Full\n",
    "* Activity / Inactivity\n",
    "* Feature importance\n",
    "* Data leakage\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing dataset\n",
    "\n",
    "* extract data\n",
    "* keep full days only (1440 min per date)\n",
    "* reduce by 'days' on `scores_csv`, minimum days = 7, exact days = 7\n",
    "  * this means `condition_8` gets dropped (only 5 days)\n",
    "  * remaining 54 people have full week's worth of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load functions in python file with magic command\n",
    "%run ../code/final-1-rodriguez.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "folderpath = '../data/depresjon'\n",
    "output_csv_path = '../data/petter/final.csv'\n",
    "scores_csv_path = '../data/depresjon/scores.csv'\n",
    "\n",
    "# full ds, no csv\n",
    "df = extract_from_folder(folderpath)\n",
    "\n",
    "# keep full days only\n",
    "full_df = preprocess_full_days(df, print_info=False)\n",
    "\n",
    "# reduce to `num_days` in scores.csv\n",
    "reduce_df = extract_days_per_scores(full_df, scores_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add scores df\n",
    "final_scores_df = add_scores(reduce_df, scores_df = pd.read_csv(scores_csv_path))\n",
    "\n",
    "## drop cols 5, 6, 9-16\n",
    "final_df = final_scores_df.drop(columns=final_scores_df.columns[[5, 6, 9, 10, 11, 12, 13, 14, 15, 16]],axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "final_df.to_csv('../data/petter/final-rod-1-all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Rodriguez with no `data leakage`\n",
    "\n",
    "* Avoiding data leakage with 14 features used by Rodriguez\n",
    "* Split into separate datasets before engineering features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load functions in python file with magic command\n",
    "%run ../code/final-1-rodriguez.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset, timestamp, date as datetime\n",
    "df_rod = pd.read_csv('../data/petter/final-rod-1-all.csv', parse_dates=['timestamp', 'date'])\n",
    "\n",
    "# drop cols - gender and age\n",
    "df_rod = df_rod.drop(['gender', 'age'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New shape to dataframe - min/hour columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract hour and minute from timestamp\n",
    "df_rod['hour'] = df_rod['timestamp'].dt.hour\n",
    "df_rod['minute'] = df_rod['timestamp'].dt.minute\n",
    "\n",
    "# pivot the DataFrame\n",
    "df_pivot = df_rod.pivot(index=['date', 'id', 'label', 'hour'], columns='minute', values='activity')\n",
    "\n",
    "# rename columns\n",
    "df_pivot.columns = [f'min_{minute:02d}' for minute in range(60)]\n",
    "\n",
    "# reset index\n",
    "df_pivot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing = df_pivot[df_pivot.isnull().any(axis=1)]\n",
    "#print(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New dataframes - day, night, full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12348, 64)\n",
      "(10290, 64)\n",
      "(24696, 64)\n"
     ]
    }
   ],
   "source": [
    "#  subsets based on time ranges\n",
    "day = df_pivot[(df_pivot['hour'] >= 8) & (df_pivot['hour'] < 20)]  # day: 8 am to 8 pm\n",
    "night = df_pivot[(df_pivot['hour'] >= 21) | (df_pivot['hour'] < 7)]  # night: 9 pm to 7 am\n",
    "full = df_pivot  # full day:  24 hours\n",
    "\n",
    "# print shapes\n",
    "print(day.shape)\n",
    "print(night.shape)\n",
    "print(full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "* Ensure that split into test/train is done before calculating features\n",
    "* Functions below split the datasets first and then standardise and calculate 14 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Random Forest model\n",
    "\n",
    "`preprocess_and_calculate` - for feature generation\n",
    "`fit_and_evaluate` - for model (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing \"day\" dataset\n",
      "X_train shape: (10200, 23)\n",
      "y_train shape: (10200,)\n",
      "X_test shape: (2148, 23)\n",
      "y_test shape: (2148,)\n",
      "\n",
      "Fitting model for \"day\" dataset\n",
      "Accuracy: 0.5605, \n",
      "F1-score: 0.4010, \n",
      "Confusion Matrix:\n",
      "[[888 276]\n",
      " [668 316]]\n",
      "Recall: 0.3211, \n",
      "MCC: 0.0937\n",
      "\n",
      "\n",
      "Processing \"night\" dataset\n",
      "X_train shape: (8500, 23)\n",
      "y_train shape: (8500,)\n",
      "X_test shape: (1790, 23)\n",
      "y_test shape: (1790,)\n",
      "\n",
      "Fitting model for \"night\" dataset\n",
      "Accuracy: 0.5089, \n",
      "F1-score: 0.3644, \n",
      "Confusion Matrix:\n",
      "[[659 311]\n",
      " [568 252]]\n",
      "Recall: 0.3073, \n",
      "MCC: -0.0143\n",
      "\n",
      "\n",
      "Processing \"full\" dataset\n",
      "X_train shape: (20400, 23)\n",
      "y_train shape: (20400,)\n",
      "X_test shape: (4296, 23)\n",
      "y_test shape: (4296,)\n",
      "\n",
      "Fitting model for \"full\" dataset\n",
      "Accuracy: 0.5517, \n",
      "F1-score: 0.4056, \n",
      "Confusion Matrix:\n",
      "[[1713  615]\n",
      " [1311  657]]\n",
      "Recall: 0.3338, \n",
      "MCC: 0.0760\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs = [day, night, full]\n",
    "df_names = ['\"day\"', '\"night\"', '\"full\"']\n",
    "\n",
    "for df, df_names in zip(dfs, df_names):\n",
    "    print(f'Processing {df_names} dataset')\n",
    "    X_train, y_train, X_test, y_test = preprocess_and_calculate_features(df)\n",
    "    print(f'X_train shape: {X_train.shape}')\n",
    "    print(f'y_train shape: {y_train.shape}')\n",
    "    print(f'X_test shape: {X_test.shape}')\n",
    "    print(f'y_test shape: {y_test.shape}')\n",
    "    print('')\n",
    "\n",
    "    print(f'Fitting model for {df_names} dataset')\n",
    "    accuracy, f1, conf_matrix, recall, mcc, precision, roc_auc, specificity, support = fit_and_evaluate(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}, \\nF1-score: {f1:.4f}, \\nConfusion Matrix:\\n{conf_matrix}\\nRecall: {recall:.4f}, \\nMCC: {mcc:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The above model is a recreation of the Rodriguez model but making sure that there is no data leakage, by standardising and generating features after splitting the datasets into `train` and `test`.\n",
    "\n",
    "The data was reduced to the 'days' column on 'scores.csv' so there are an uneven number of days - but each day is full - that is, it contains 1440 rows.\n",
    "\n",
    "As a direct comparison to the same process with data leakage: \n",
    "\n",
    "**Day**\n",
    "* Accuracy dropped from 0.7289 to 0.5605\n",
    "* F1 stayed the same around 0.4\n",
    "* Recall dropped from 0.45 to 0.32\n",
    "* MCC dropped from 0.22 to 0.09\n",
    "\n",
    "**Night**\n",
    "* Accuracy dropped from 0.7095 to 0.5089\n",
    "* F1 stayed the same around 0.35\n",
    "* Recall dropped from 0.39 to 0.307\n",
    "* MCC dropped from 0.17 to -0.01\n",
    "\n",
    "**Full**\n",
    "* Accuracy dropped from 0.71 to 0.55\n",
    "* F1 improved slightly from 0.37 to 0.406\n",
    "* Recall dropped from 0.42 to 0.33\n",
    "* MCC dropped from 0.19 to 0.07\n",
    "\n",
    "\n",
    "**It is clear that the methodology employed by the authors of this study introduced significant data leakage, even more than I was able to introduce given their reported accuracy of 0.99**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with data leakage can be found here: \n",
    "\n",
    "[..\\petter\\Rodriguez-recreation.ipynb](..\\petter\\Rodriguez-recreation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igp5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
