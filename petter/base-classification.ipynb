{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic classification models\n",
    "\n",
    "* initial objectives:\n",
    "  * fit several base models\n",
    "  * candidates:  `logistic regression`, `decision tree`, `svm`, `random forest`, `gradient boosting`, `knn`, `naive bayes`, `neural networks`\n",
    "  * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions\n",
    "1. extract depresjon from folder\n",
    "2. extract full days (1440 rows) records\n",
    "3. resample, e.g. to hourly\n",
    "4. normalise data (mean = 0, std = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pl functions\n",
    "\n",
    "%run ../code/pl-functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = extract_folder('../data/depresjon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1571706 entries, 0 to 1571705\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count    Dtype         \n",
      "---  ------     --------------    -----         \n",
      " 0   timestamp  1571706 non-null  datetime64[ns]\n",
      " 1   date       1571706 non-null  datetime64[ns]\n",
      " 2   activity   1571706 non-null  int64         \n",
      " 3   id         1571706 non-null  object        \n",
      " 4   label      1571706 non-null  int64         \n",
      "dtypes: datetime64[ns](2), int64(2), object(1)\n",
      "memory usage: 60.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# print head\n",
    "#print(df.head())\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dataset to full days only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract full days\n",
    "df = extract_full_days(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample - \n",
    "\n",
    "* I am skipping this optional step for now...I want to see if it runs on the complete df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column to normalise\n",
    "col_to_normlise = ['activity']\n",
    "\n",
    "# normalise\n",
    "df = normalise_data(df, col_to_normlise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1527281 entries, 540 to 1570797\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count    Dtype         \n",
      "---  ------     --------------    -----         \n",
      " 0   timestamp  1527281 non-null  datetime64[ns]\n",
      " 1   date       1527281 non-null  object        \n",
      " 2   activity   1527281 non-null  float64       \n",
      " 3   id         1527281 non-null  object        \n",
      " 4   label      1527281 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(2)\n",
      "memory usage: 69.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# print head\n",
    "#print(df.head())\n",
    "\n",
    "# print info\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "Split the dataset into training and testing sets.\n",
    "\n",
    "Parameters:\n",
    "    X (DataFrame): The input features.\n",
    "    y (Series): The target variable.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): The seed used by the random number generator.\n",
    "\n",
    "Returns:\n",
    "    X_train (DataFrame): The training set of input features.\n",
    "    X_test (DataFrame): The testing set of input features.\n",
    "    y_train (Series): The training set of target variable.\n",
    "    y_test (Series): The testing set of target variable.\n",
    "\"\"\"\n",
    "X = df.drop(['label', 'timestamp', 'date', 'id'], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression:**\n",
    "LR is a statistical model used in binary classification by modelling the log-odds of the probability of an event.\n",
    "* Strengths: Simple, fast, and efficient for small dataset with limited features.\n",
    "* Weaknesses: Assumes a linear decision boundary, can underperform with complex datasets.\n",
    "\n",
    "**Random Forest:**\n",
    "* RF is an ensemble learning method constructs multiple decision trees at training time and outputting the class that is the mode of the classes of the individual trees.\n",
    "* Strengths: Handles categorical variables well, resistant to overfitting, can model non-linear decision boundaries.\n",
    "* Weaknesses: *Can be slow on large datasets*, not easily interpretable.\n",
    "\n",
    "**Support Vector Machine (SVM):**\n",
    "* SVM is a powerful and flexible class of supervised algorithms for both classification and regression.\n",
    "* Strengths: Effective in high dimensional spaces, or when number of dimensions is greater than the number of samples.\n",
    "* Weaknesses: Does not directly provide probability estimates, can be *inefficient to train*.\n",
    "  \n",
    "**Decision Tree:**\n",
    "* DT are a type of simple flowchart-like structure in which internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome.\n",
    "* Strengths: Simple to understand and interpret, requires little data preparation.\n",
    "* Weaknesses: Prone to overfitting, can create biased trees if some classes dominate.\n",
    "\n",
    "**K-Nearest Neighbors (KNN):**\n",
    "* KNN is a type of *instance-based learning, or lazy learning*, where the function is only approximated locally and all computation is deferred until function evaluation.\n",
    "* Strengths: Simple, effective for datasets with complex decision boundaries.\n",
    "* Weaknesses: *Slow on large datasets*, sensitive to irrelevant features.\n",
    "\n",
    "**Naive Bayes:**\n",
    "* NB classifiers are a family of simple “probabilistic classifiers” based on applying Bayes’ theorem with strong (naïve) independence assumptions between the features.\n",
    "* Strengths: Fast, simple, performs well with small datasets.\n",
    "* Weaknesses: *Assumes that all features are independent*, which is rarely the case.\n",
    "\n",
    "**Neural Network (MLPClassifier):**\n",
    "* MLPClassifier stands for Multi-layer Perceptron classifier connects to a Neural Network. MLPClassifier relies on an underlying Neural Network to perform the task of classification.\n",
    "* Strengths: Can model complex, non-linear patterns.\n",
    "* Weaknesses: *Requires a lot of data and computational resources, not easily interpretable.*\n",
    "\n",
    "**XGBoost**:\n",
    "* XGBoost is an optimised distributed gradient boosting library designed to be highly efficient, flexible and portable.\n",
    "* Strengths: Fast, accurate, supports regularisation to avoid overfitting.\n",
    "* Weaknesses: *Can be slow to train*, not easily interpretable.\n",
    "\n",
    "**LightGBM**:\n",
    "* LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages: Faster training speed and higher efficiency, lower memory usage, better accuracy, support of parallel and GPU learning, capable of handling large-scale data.\n",
    "* Strengths: Fast, uses less memory, higher accuracy.\n",
    "* Weaknesses: Can overfit on small datasets, not easily interpretable.\n",
    "\n",
    "**AdaBoost**:\n",
    "* AdaBoost is a boosting algorithm which constructs a classifier. Adaboost creates a strong classifier from number of weak classifiers.\n",
    "* Strengths: Fast, simple, less prone to overfitting.\n",
    "* Weaknesses: Sensitive to noisy data and outliers.\n",
    "\n",
    "**Quadratic Discriminant Analysis (QDA):**\n",
    "* QDA is a classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.\n",
    "* Strengths: Can model more complex decision boundaries than linear methods.\n",
    "* Weaknesses: *Assumes that the observations are normally distributed*, which is rarely the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost\n",
    "%pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.6533927963449078\n",
      "Random Forest: 0.6671468227028695\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier \n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Random Forest', RandomForestClassifier()),\n",
    "    ('SVM', SVC()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Neural Network', MLPClassifier()),\n",
    "    ('XGBoost', XGBClassifier()),\n",
    "    ('LightGBM', LGBMClassifier()),\n",
    "    ('AdaBoost', AdaBoostClassifier()),\n",
    "    ('QDA', QuadraticDiscriminantAnalysis()), \n",
    "    ('Gradient Boosting', GradientBoostingClassifier())\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "def evaluate_models(models, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of different models on a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - models (list): A list of tuples containing the name and model object.\n",
    "    - X_train (array-like): Training data features.\n",
    "    - y_train (array-like): Training data labels.\n",
    "    - X_test (array-like): Test data features.\n",
    "    - y_test (array-like): Test data labels.\n",
    "\n",
    "    Returns:\n",
    "    - results (list): A list of dictionaries containing the evaluation results for each model.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "    for name, model in models:\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cr = classification_report(y_test, y_pred)\n",
    "\n",
    "        print(f'{name}:')\n",
    "        print(f'Time taken: {elapsed_time} seconds')\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        print(f'Precision: {precision}')\n",
    "        print(f'Recall: {recall}')\n",
    "        print(f'F1 Score: {f1}')\n",
    "        print(f'Matthews Correlation Coefficient: {mcc}')\n",
    "        print(f'Confusion Matrix: {cm}')\n",
    "        print(f'Classification Report: {cr}')\n",
    "        print(cr)\n",
    "\n",
    "        results.append({\n",
    "            'name': name,\n",
    "            'time': elapsed_time,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'mcc': mcc,\n",
    "            'confusion_matrix': cm,\n",
    "            'classification_report': cr\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* understanding /summarising each model, strengths/weakness, description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation option (not run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_models(models, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models using cross-validation and generate predictions on the test data.\n",
    "\n",
    "    Parameters:\n",
    "    - models (list): A list of tuples containing the name and model object for each model to be evaluated.\n",
    "    - X_train (array-like): The training data features.\n",
    "    - y_train (array-like): The training data labels.\n",
    "    - X_test (array-like): The test data features.\n",
    "    - y_test (array-like): The test data labels.\n",
    "\n",
    "    Returns:\n",
    "    - results (list): A list of dictionaries containing the evaluation results for each model.\n",
    "    Each dictionary contains the following keys:\n",
    "        - 'name': The name of the model.\n",
    "        - 'time': The time taken to train and predict.\n",
    "        - 'accuracy': The accuracy score.\n",
    "        - 'precision': The precision score.\n",
    "        - 'recall': The recall score.\n",
    "        - 'f1': The F1 score.\n",
    "        - 'mcc': The Matthews Correlation Coefficient.\n",
    "        - 'confusion_matrix': The confusion matrix.\n",
    "        - 'classification_report': The classification report.\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sklearn.metrics import matthews_corrcoef, confusion_matrix, classification_report\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for name, model in models:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "        print(f'{name} cross-validation score: {cv_scores.mean()} ± {cv_scores.std()}')\n",
    "\n",
    "        # Fit the model on the training data and generate predictions on the test data\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f'{name} took {elapsed_time} seconds to train and predict\\n')\n",
    "\n",
    "        # Calculate metrics here (accuracy, precision, recall, etc.)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cr = classification_report(y_test, y_pred)\n",
    "\n",
    "        print(f'{name}:')\n",
    "        print(f'Time taken: {elapsed_time} seconds')\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        print(f'Precision: {precision}')\n",
    "        print(f'Recall: {recall}')\n",
    "        print(f'F1 Score: {f1}')\n",
    "        print(f'Matthews Correlation Coefficient: {mcc}')\n",
    "        print(f'Confusion Matrix: {cm}')\n",
    "        print(f'Classification Report: {cr}')\n",
    "\n",
    "        results.append({\n",
    "            'name': name,\n",
    "            'time': elapsed_time,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'mcc': mcc,\n",
    "            'confusion_matrix': cm,\n",
    "            'classification_report': cr\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create a bar plot of the accuracy of each model\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='accuracy', y='name', data=results_df.sort_values('accuracy'))\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Model')\n",
    "plt.title('Model Performance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `cm` is the confusion matrix and `model_name` is the name of the model\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix for {model_name}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian processes\n",
    "\n",
    "In this code, RBF(1.0) creates an RBF kernel with a length scale of 1.0, and GaussianProcessClassifier(kernel=kernel, random_state=0) creates a Gaussian process classifier using this kernel.\n",
    "\n",
    "* an replace RBF(1.0) with `DotProduct()`, `Matern()`, `RationalQuadratic()`, or `WhiteKernel()` to use a different kernel.\n",
    "* computationally intensive, especially on large datasets -> start with a subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, DotProduct, Matern, RationalQuadratic, WhiteKernel\n",
    "\n",
    "# models\n",
    "random_state = 0\n",
    "model = [\n",
    "    ('Gaussian Process with RBF kernel', GaussianProcessClassifier(kernel=RBF())),\n",
    "    ('Gaussian Process with DotProduct kernel', GaussianProcessClassifier(kernel=DotProduct())),\n",
    "    ('Gaussian Process with Matern kernel', GaussianProcessClassifier(kernel=Matern())),\n",
    "    ('Gaussian Process with RationalQuadratic kernel', GaussianProcessClassifier(kernel=RationalQuadratic())),\n",
    "    ('Gaussian Process with WhiteKernel', GaussianProcessClassifier(kernel=WhiteKernel()))\n",
    "]\n",
    "\n",
    "# evaluate models\n",
    "for name, model in model:\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cr = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f'{name}:')\n",
    "    print(f'Time taken: {elapsed_time} seconds')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(f'Matthews Correlation Coefficient: {mcc}')\n",
    "    print(f'Confusion Matrix: {cm}')\n",
    "    print(f'Classification Report: {cr}')\n",
    "    print(cr)\n",
    "\n",
    "    results.append({\n",
    "        'name': name,\n",
    "        'time': elapsed_time,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'mcc': mcc,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': cr\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=0)\n",
    "gpc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gpc.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras, TensorFlow, Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNN - recurrent neural networks\n",
    "* CNN - convolutional neural network\n",
    "\n",
    "* **Bidirectional** - wrapper for RNN layer like LSTM or GRU\n",
    "* **Flatten**: This layer is used to flatten the structure into a one-dimensional vector.\n",
    "* **TimeDistributed**: This wrapper allows a layer to be applied to every temporal slice of an input. \n",
    "* **Conv1D**: This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. It’s often used in tasks involving sequences, such as text classification or time-series analysis.\n",
    "* **MaxPooling1D**: This layer applies a max pooling operation for temporal data to reduce the dimensionality of the data.\n",
    "* **ConvLSTM2D**: This layer is a type of LSTM layer where the recurrent connections are convolutional. \n",
    "* **backend** (K): This is a utility to provide backend-agnostic operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **sequence** from `keras.preprocessing`: This module provides functions to prepare time-series actigraphy data for the model.\n",
    "* **ConfigProto** and **Session** from `tensorflow`: These are used to configure TensorFlow and create a TensorFlow session - for gpu use\n",
    "* **Sequential** from `keras.models`: This is a linear stack of layers that to create a neural network. add layers to the network using the .add() method.\n",
    "* **Dense**, **LSTM**, and **Dropout** from `keras.layers`: These are different types of layers can add to your neural network. Dense is a fully connected layer, LSTM is a type of recurrent layer suitable for sequence data, and Dropout is a regularization layer that helps prevent overfitting.\n",
    "* **Adam** from `keras.optimizers`: This is an optimisation algorithm to train your neural network.  efficiency and low memory requirements\n",
    "* **load_model** from `keras.models`: This function allows you to load a saved model - saves retraining\n",
    "* **ModelCheckpoint** from `keras.callbacks`: This is a callback that you can use to save the model at regular intervals during training. This can be useful if training takes a long time and you don’t want to lose your progress if it’s interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, TimeDistributed, Conv1D, MaxPooling1D\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, 1)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
