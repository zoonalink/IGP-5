{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreating Rodriguez-Ruiz \n",
    "\n",
    "* Using Tom's code/plan\n",
    "* And my code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_from_folder(folderpath, downsample=None, save_to_csv=False, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Extract CSV data from folder and subfolders into a dataframe.\n",
    "\n",
    "    Args:\n",
    "      folderpath (str): folder containing CSV files.\n",
    "      downsample (int, optional): number of rows to downsample CSVs to. Defaults to None.\n",
    "      save_to_csv (bool, optional): save the updated df to a CSV file? defaults to False.\n",
    "      output_csv_path (str, optional): csv filepath. required if save_to_csv is True.\n",
    "\n",
    "    Returns:\n",
    "      pandas.DataFrame: DataFrame of concatenated CSV data.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    # dict to store dataframes by condition  \n",
    "    dfs = {'control': [], 'condition': []}\n",
    "\n",
    "    try:\n",
    "        # subfolders\n",
    "        subfolders = [f for f in os.listdir(folderpath) if os.path.isdir(os.path.join(folderpath, f))]\n",
    "\n",
    "        for subfolder in subfolders:\n",
    "            subfolderpath = os.path.join(folderpath, subfolder)  \n",
    "\n",
    "            # list of CSV files\n",
    "            files = os.listdir(subfolderpath)\n",
    "\n",
    "            for file in files:\n",
    "                filepath = os.path.join(subfolderpath, file)\n",
    "\n",
    "                # extract ID from filename \n",
    "                id = file.split('.')[0]\n",
    "\n",
    "                df = pd.read_csv(filepath)\n",
    "\n",
    "                # optional downsample \n",
    "                if downsample:\n",
    "                    df = df.sample(downsample)\n",
    "\n",
    "                # ID column - this is the filename without the extension\n",
    "                df['id'] = id\n",
    "\n",
    "                # 'condition' column\n",
    "                df['condition'] = subfolder\n",
    "\n",
    "                # convert 'timestamp' and 'date' to datetime\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "                # append to dict by condition\n",
    "                if subfolder == 'control':\n",
    "                    dfs['control'].append(df)\n",
    "                else:  \n",
    "                    dfs['condition'].append(df)\n",
    "\n",
    "    except OSError:\n",
    "        print(f\"Error reading folder: {folderpath}\")\n",
    "\n",
    "    # concatenate dfs for each condition\n",
    "    dfs['control'] = pd.concat(dfs['control'])\n",
    "    dfs['condition'] = pd.concat(dfs['condition'])\n",
    "\n",
    "    # reset index on the final df\n",
    "    df = pd.concat([dfs['control'], dfs['condition']]).reset_index(drop=True)\n",
    "\n",
    "    # add label column\n",
    "    df['label'] = 0\n",
    "    df.loc[df['condition'] == 'condition', 'label'] = 1\n",
    "    \n",
    "    # remove old 'condition' column\n",
    "    df.drop('condition', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    try:\n",
    "        if save_to_csv:\n",
    "            if output_csv_path:\n",
    "                df.to_csv(output_csv_path, index=False)\n",
    "                print(f\"df saved to {output_csv_path}\")\n",
    "            else:\n",
    "                print(\"Error: Please provide an output CSV path.\")\n",
    "        \n",
    "        \n",
    "        return df\n",
    "    except OSError:\n",
    "        print(\"Error saving to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction of all the activity data into one data frame\n",
    "folderpath = '../data/depresjon'\n",
    "# full ds, no csv\n",
    "df = extract_from_folder(folderpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.head())\n",
    "#print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "1. equal observations per subject\n",
    "2. segmentation into hourly\n",
    "3. day, night, full \n",
    "4. NaNs and standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Reduction\n",
    "\n",
    ">\"For the pre-processing stage, the next step are proposed. Since the total amount of data recorded for each subject is different, a new subset of data is extracted, adjusting the number of observations to be equal for each subject.\"\n",
    "\n",
    "This step is not adequately described as there are many ways to approach this.\n",
    "\n",
    "Below, I have tried three approaches: \n",
    "\n",
    "1. Reducing data to the maximum viable number of rows - that is, finding the minimum of all ids and reducing all other id's rows to this value. \n",
    "   * this can be done by `head()` or `sample()` methods\n",
    "2. Reducing to 'full days' first and then minimising the dataset\n",
    "3. Reducing to match 'num_days' in scores.csv and then minimising the dataset. \n",
    "\n",
    "Finally, I did none of the above and simply used the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min rows per id\n",
    "\n",
    "* this is reducing to maximum viable number of rows, no other processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce df to min number of rows per id - so each id has the same number of rows\n",
    "min_rows = df['id'].value_counts().min()\n",
    "trim = df.groupby('id').apply(lambda x: x.head(min_rows), include_groups=False).reset_index()\n",
    "\n",
    "# drop the 'level_1' column\n",
    "trim = trim.drop(columns='level_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19299]\n"
     ]
    }
   ],
   "source": [
    "# print unique row counts per id\n",
    "print(trim['id'].value_counts().unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(trim.info())\n",
    "#print(trim.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full days\n",
    "\n",
    "This includes the 'preprocess to full days' step before reducing to max viable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_full_days(df, save_to_csv=False, output_csv_path=None, print_info=True):\n",
    "    \"\"\"\n",
    "    Extracts full days from a dataframe.\n",
    "\n",
    "    Args::\n",
    "    df (DataFrame): input df.\n",
    "    save_to_csv (bool, optional): save the updated df to a CSV file? defaults to False.\n",
    "    output_csv_path (str, optional): csv filepath. required if save_to_csv is True.\n",
    "    print_info (bool, optional): print info about the df. defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: df containing only full days (1440 rows per day).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # group by id and date, count rows, and filter where count equals 1440\n",
    "    full_days_df = df.groupby(['id', 'date']).filter(lambda x: len(x) == 1440)\n",
    "\n",
    "    # set index to timestamp\n",
    "    #full_days_df.set_index(['timestamp'], inplace=True)\n",
    "    \n",
    "    if print_info:\n",
    "        # print id and date combinations that don't have 1440 rows\n",
    "        not_full_days = df.groupby(['id', 'date']).size().reset_index(name='count').query('count != 1440')\n",
    "        print(\"\\nid and date combinations that don't have 1440 rows and have been removed:\\n\")\n",
    "        print(not_full_days)\n",
    "\n",
    "        # print info\n",
    "        print(\"\\nfull_days_df info:\\n\")\n",
    "        print(full_days_df.info())\n",
    "\n",
    "        #print full days per id\n",
    "        print(\"\\nfull days per id:\\n\")\n",
    "        print(full_days_df.groupby('id').size()/1440)\n",
    "\n",
    "        # print min number of days\n",
    "        print(\"\\nmin number of days per id:\\n\")\n",
    "        print(full_days_df.groupby('id').size().min()/1440)\n",
    "        \n",
    "\n",
    "    try:\n",
    "        if save_to_csv:\n",
    "            if output_csv_path:\n",
    "                full_days_df.to_csv(output_csv_path, index=False)\n",
    "                print(f\"df saved to {output_csv_path}\")\n",
    "            else:\n",
    "                print(\"Error: Please provide an output CSV path.\")\n",
    "        \n",
    "        \n",
    "        return full_days_df\n",
    "    except OSError:\n",
    "        print(\"Error saving to CSV.\")\n",
    "\n",
    "    return full_days_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17280]\n"
     ]
    }
   ],
   "source": [
    "# full days\n",
    "full = preprocess_full_days(df, print_info=False)\n",
    "#print(full.info())\n",
    "\n",
    "# reduce df to min number of rows per id - so each id has the same number of rows\n",
    "min_rows = full['id'].value_counts().min()\n",
    "trim2 = full.groupby('id').apply(lambda x: x.head(min_rows), include_groups=False).reset_index()\n",
    "\n",
    "# drop the 'level_1' column\n",
    "trim2 = trim2.drop(columns='level_1')\n",
    "\n",
    "# print unique row counts per id\n",
    "print(trim2['id'].value_counts().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce to 'number of days' from scores\n",
    "\n",
    "This step reduces to num_days as per scores.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_days_per_scores(df, scores_csv_path='..\\data\\depresjon\\scores.csv', save_to_csv=True, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Extract the number of days per ID from the 'scores' data.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): df containing the 'id' column.\n",
    "        scores_csv_path (str, optional): path to the 'scores' CSV file. Defaults to '..\\data\\depresjon\\scores.csv'.\n",
    "        save_to_csv (bool, optional): save the updated df to a CSV file? Defaults to True.\n",
    "        output_csv_path (str, optional): csv filepath. Required if save_to_csv is True.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df with the specified number of days per ID based on 'scores'.\n",
    "    \"\"\"\n",
    "    # scores from the CSV file\n",
    "    scores_df = pd.read_csv(scores_csv_path)\n",
    "\n",
    "    # merge scores with the df based on the 'id' column\n",
    "    merged_df = pd.merge(df, scores_df, left_on='id', right_on='number', how='left')\n",
    "\n",
    "    # filter rows to keep the specified number of days\n",
    "    df_filtered = merged_df.groupby('id', group_keys=False, as_index=False, sort=False).apply(lambda group: group.head(group['days'].min() * 1440)).reset_index(drop=True)\n",
    "\n",
    "    # drop cols number, days, gender, age, afftype, melanch, inpatient, edu, marriage, work, madrs1, madrs2\n",
    "    cols = ['number', 'number', 'days', 'gender', 'age', 'afftype', 'melanch', 'inpatient', 'edu', 'marriage', 'work', 'madrs1', 'madrs2']\n",
    "    df_filtered.drop(cols, axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    # save to CSV\n",
    "    if save_to_csv:\n",
    "        if output_csv_path:\n",
    "            df_filtered.to_csv(output_csv_path, index=False)\n",
    "            print(f\"\\n\\ndf saved to {output_csv_path}\")\n",
    "        else:\n",
    "            print(\"Error: Please provide an output CSV path.\")\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_33264\\242992910.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_filtered = merged_df.groupby('id', group_keys=False, as_index=False, sort=False).apply(lambda group: group.head(group['days'].min() * 1440)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7200]\n"
     ]
    }
   ],
   "source": [
    "num_days = extract_days_per_scores(df, save_to_csv=False, output_csv_path=None)\n",
    "\n",
    "#print(num_days.info())\n",
    "\n",
    "# reduce df to min number of rows per id - so each id has the same number of rows\n",
    "min_rows = num_days['id'].value_counts().min()\n",
    "trim3 = num_days.groupby('id').apply(lambda x: x.head(min_rows), include_groups=False).reset_index()\n",
    "\n",
    "# drop the 'level_1' column\n",
    "trim3 = trim3.drop(columns='level_1')\n",
    "\n",
    "# print unique row counts per id\n",
    "print(trim3['id'].value_counts().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment into hourly intervals\n",
    "\n",
    "\"The structure of the data for every observation is contained by 61 columns; one column for the monitored hour and one column for each minute (60 columns) of motor activity. This segmentation allowed the classification of depressive episodes per hour.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26230, 64)\n"
     ]
    }
   ],
   "source": [
    "# copy df\n",
    "no_trim = df.copy()\n",
    "\n",
    "# extract hour and minute from timestamp\n",
    "no_trim['hour'] = no_trim['timestamp'].dt.hour\n",
    "no_trim['minute'] = no_trim['timestamp'].dt.minute\n",
    "\n",
    "# pivot the DataFrame\n",
    "no_trim_piv = no_trim.pivot(index=['date', 'id', 'label', 'hour'], columns='minute', values='activity')\n",
    "\n",
    "# rename columns\n",
    "no_trim_piv.columns = [f'min_{minute:02d}' for minute in range(60)]\n",
    "\n",
    "# Reset index\n",
    "no_trim_piv.reset_index(inplace=True)\n",
    "\n",
    "#  NaN with 0 (for missing minute values)\n",
    "no_trim = no_trim_piv.fillna(0)\n",
    "\n",
    "# print hourly_data shape\n",
    "print(no_trim.shape)\n",
    "\n",
    "# print info\n",
    "#print(no_trim.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date            id  label  hour  min_00  min_01  min_02  min_03  \\\n",
      "0 2002-05-24  condition_20      1    11     0.0     0.0     0.0     0.0   \n",
      "\n",
      "   min_04  min_05  ...  min_50  min_51  min_52  min_53  min_54  min_55  \\\n",
      "0     0.0     0.0  ...    83.0     0.0     0.0     0.0     3.0     0.0   \n",
      "\n",
      "   min_56  min_57  min_58  min_59  \n",
      "0   249.0   209.0   360.0    36.0  \n",
      "\n",
      "[1 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "print(no_trim.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim 1 - no processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract hour and minute from timestamp\n",
    "trim['hour'] = trim['timestamp'].dt.hour\n",
    "trim['minute'] = trim['timestamp'].dt.minute\n",
    "\n",
    "# pivot the DataFrame\n",
    "df_pivot = trim.pivot(index=['date', 'id', 'label', 'hour'], columns='minute', values='activity')\n",
    "\n",
    "# rename columns\n",
    "df_pivot.columns = [f'min_{minute:02d}' for minute in range(60)]\n",
    "\n",
    "# Reset index\n",
    "df_pivot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_pivot.head(5))\n",
    "#print(df_pivot.info())\n",
    "#print(df_pivot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print any missing minute data\n",
    "missing = df_pivot[df_pivot.isnull().any(axis=1)]\n",
    "#print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17722, 64)\n"
     ]
    }
   ],
   "source": [
    "#  NaN with 0 (for missing minute values)\n",
    "trim1_piv = df_pivot.fillna(0)\n",
    "\n",
    "# print hourly_data shape\n",
    "print(trim1_piv.shape)\n",
    "\n",
    "# print info\n",
    "#print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim2 - full days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15840, 64)\n"
     ]
    }
   ],
   "source": [
    "# extract hour and minute from timestamp\n",
    "trim2['hour'] = trim2['timestamp'].dt.hour\n",
    "trim2['minute'] = trim2['timestamp'].dt.minute\n",
    "\n",
    "# pivot the DataFrame\n",
    "df_pivot2 = trim2.pivot(index=['date', 'id', 'label', 'hour'], columns='minute', values='activity')\n",
    "\n",
    "# rename columns\n",
    "df_pivot2.columns = [f'min_{minute:02d}' for minute in range(60)]\n",
    "\n",
    "# Reset index\n",
    "df_pivot2.reset_index(inplace=True)\n",
    "\n",
    "#  NaN with 0 (for missing minute values)\n",
    "trim2_piv = df_pivot2.fillna(0)\n",
    "\n",
    "# print hourly_data shape\n",
    "print(trim2_piv.shape)\n",
    "\n",
    "# print info\n",
    "#print(df2.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim 3 - 'num_days'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6613, 64)\n"
     ]
    }
   ],
   "source": [
    "# extract hour and minute from timestamp\n",
    "trim3['hour'] = trim3['timestamp'].dt.hour\n",
    "trim3['minute'] = trim3['timestamp'].dt.minute\n",
    "\n",
    "# pivot the DataFrame\n",
    "df_pivot3 = trim3.pivot(index=['date', 'id', 'label', 'hour'], columns='minute', values='activity')\n",
    "\n",
    "# rename columns\n",
    "df_pivot3.columns = [f'min_{minute:02d}' for minute in range(60)]\n",
    "\n",
    "# Reset index\n",
    "df_pivot3.reset_index(inplace=True)\n",
    "\n",
    "#  NaN with 0 (for missing minute values)\n",
    "trim3_piv = df_pivot3.fillna(0)\n",
    "\n",
    "# print hourly_data shape\n",
    "print(trim3_piv.shape)\n",
    "\n",
    "# print info\n",
    "#print(df3.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, these dataframes are the following lengths:\n",
    "\n",
    "* no trim - 26230\n",
    "* trim to min - 17722\n",
    "* trim to min of full days - 15840\n",
    "* trim to min of num_days - 6613"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new dataframes - day, night, full\n",
    "\n",
    "\"Therefore, based on the hourly segmentation, three different subsets are constructed; night motor activity (from 21 to 7 h taking into account the sunrise standard hours) [21], day motor activity (from 8 to 20 h) and finally all day motor activity with the total day hours.\"\n",
    "\n",
    "* 8 am - 8 pm (12 hours)\n",
    "* 9 pm - 7 am (10 hours)\n",
    "\n",
    "Why do it this way? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1061445 entries, 0 to 1061444\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count    Dtype         \n",
      "---  ------     --------------    -----         \n",
      " 0   id         1061445 non-null  object        \n",
      " 1   timestamp  1061445 non-null  datetime64[ns]\n",
      " 2   date       1061445 non-null  datetime64[ns]\n",
      " 3   activity   1061445 non-null  int64         \n",
      " 4   label      1061445 non-null  int64         \n",
      " 5   hour       1061445 non-null  int32         \n",
      " 6   minute     1061445 non-null  int32         \n",
      "dtypes: datetime64[ns](2), int32(2), int64(2), object(1)\n",
      "memory usage: 48.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(trim.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9055, 64)\n",
      "(7213, 64)\n",
      "(17722, 64)\n"
     ]
    }
   ],
   "source": [
    "#  subsets based on time ranges\n",
    "trim1_day_df = trim1_piv[(trim1_piv['hour'] >= 8) & (trim1_piv['hour'] < 20)]  # day: 8 am to 8 pm\n",
    "trim1_night_df = trim1_piv[(trim1_piv['hour'] >= 21) | (trim1_piv['hour'] < 7)]  # night: 9 pm to 7 am\n",
    "trim1_full_day_df = trim1_piv  # full day:  24 hours\n",
    "\n",
    "# print shapes\n",
    "print(trim1_day_df.shape)\n",
    "print(trim1_night_df.shape)\n",
    "print(trim1_full_day_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not match Rodriquez dataframe lengths: \n",
    "\n",
    "* day = 14168 obs\n",
    "* night = 11945 obs\n",
    "* full day = 26113 obs\n",
    "\n",
    "![](2024-03-23-22-15-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim 2 and 3 \n",
    "\n",
    "* will also not match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No trim\n",
    "\n",
    "This is much closer to to the dataset lengths reported by Rodgriguez.\n",
    "\n",
    "Questions:\n",
    "\n",
    "* Did they actually do what they have written?\n",
    "* What exactly did they do?\n",
    "* How have they created the day/night segments? Are hours inclusive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13172, 64)\n",
      "(10880, 64)\n",
      "(26230, 64)\n"
     ]
    }
   ],
   "source": [
    "#  subsets based on time ranges\n",
    "no_trim_day_df = no_trim[(no_trim['hour'] >= 8) & (no_trim['hour'] < 20)]  # day: 8 am to 8 pm\n",
    "no_trim_night_df = no_trim[(no_trim['hour'] >= 21) | (no_trim['hour'] < 7)]  # night: 9 pm to 7 am\n",
    "no_trim_full_day_df = no_trim  # full day:  24 hours\n",
    "\n",
    "# print shapes\n",
    "print(no_trim_day_df.shape)\n",
    "print(no_trim_night_df.shape)\n",
    "print(no_trim_full_day_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [date, id, label, hour, min_00, min_01, min_02, min_03, min_04, min_05, min_06, min_07, min_08, min_09, min_10, min_11, min_12, min_13, min_14, min_15, min_16, min_17, min_18, min_19, min_20, min_21, min_22, min_23, min_24, min_25, min_26, min_27, min_28, min_29, min_30, min_31, min_32, min_33, min_34, min_35, min_36, min_37, min_38, min_39, min_40, min_41, min_42, min_43, min_44, min_45, min_46, min_47, min_48, min_49, min_50, min_51, min_52, min_53, min_54, min_55, min_56, min_57, min_58, min_59]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 64 columns]\n",
      "Empty DataFrame\n",
      "Columns: [date, id, label, hour, min_00, min_01, min_02, min_03, min_04, min_05, min_06, min_07, min_08, min_09, min_10, min_11, min_12, min_13, min_14, min_15, min_16, min_17, min_18, min_19, min_20, min_21, min_22, min_23, min_24, min_25, min_26, min_27, min_28, min_29, min_30, min_31, min_32, min_33, min_34, min_35, min_36, min_37, min_38, min_39, min_40, min_41, min_42, min_43, min_44, min_45, min_46, min_47, min_48, min_49, min_50, min_51, min_52, min_53, min_54, min_55, min_56, min_57, min_58, min_59]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 64 columns]\n",
      "Empty DataFrame\n",
      "Columns: [date, id, label, hour, min_00, min_01, min_02, min_03, min_04, min_05, min_06, min_07, min_08, min_09, min_10, min_11, min_12, min_13, min_14, min_15, min_16, min_17, min_18, min_19, min_20, min_21, min_22, min_23, min_24, min_25, min_26, min_27, min_28, min_29, min_30, min_31, min_32, min_33, min_34, min_35, min_36, min_37, min_38, min_39, min_40, min_41, min_42, min_43, min_44, min_45, min_46, min_47, min_48, min_49, min_50, min_51, min_52, min_53, min_54, min_55, min_56, min_57, min_58, min_59]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# print missing data from no_trim dfs\n",
    "missing_no_trim_day = no_trim_day_df[no_trim_day_df.isnull().any(axis=1)]\n",
    "missing_no_trim_night = no_trim_night_df[no_trim_night_df.isnull().any(axis=1)]\n",
    "missing_no_trim_full_day = no_trim_full_day_df[no_trim_full_day_df.isnull().any(axis=1)]\n",
    "print(missing_no_trim_day)\n",
    "print(missing_no_trim_night)\n",
    "print(missing_no_trim_full_day)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation of motor activity\n",
    "\n",
    "* Going to proceed with `trim1_` day/night/full and `no_trim_` day/night/full dataframes.\n",
    "\n",
    "\n",
    "* standardisation: $[ z_i = \\frac{{x_i - \\bar{x}}}{{s}} ]$\n",
    "\n",
    "Where:\n",
    "* $(z_i)$ is the standardized value for observation $(i)$.\n",
    "* $(x_i)$ is the original value for observation $(i)$.\n",
    "* $(\\bar{x})$ is the mean (average) of the entire dataset.\n",
    "* $(s)$ is the standard deviation of the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date            id  label  hour  min_00  min_01  min_02  min_03  \\\n",
      "0 2002-05-24  condition_20      1    11     0.0     0.0     0.0     0.0   \n",
      "\n",
      "   min_04  min_05  ...  min_50  min_51  min_52  min_53  min_54  min_55  \\\n",
      "0     0.0     0.0  ...    83.0     0.0     0.0     0.0     3.0     0.0   \n",
      "\n",
      "   min_56  min_57  min_58  min_59  \n",
      "0   249.0   209.0   360.0    36.0  \n",
      "\n",
      "[1 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# print head(1) of trim1_day_df\n",
    "print(trim1_day_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_33264\\4051920383.py:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  new_df[column_name] = (df[column_name] - mean_values[minute]) / std_values[minute]\n"
     ]
    }
   ],
   "source": [
    "# list of df\n",
    "dataframes = [no_trim_day_df, no_trim_night_df, no_trim_full_day_df,\n",
    "              trim1_day_df, trim1_night_df, trim1_full_day_df]\n",
    "\n",
    "# list of new df names\n",
    "new_df_names = ['no_trim_day_stand', 'no_trim_night_stand', 'no_trim_full_day_stand',\n",
    "                'trim1_day_stand', 'trim1_night_stand', 'trim1_full_day_stand']\n",
    "\n",
    "# standardise each df\n",
    "for df, new_df_name in zip(dataframes, new_df_names):\n",
    "    # mean and standard deviation for the entire dataset\n",
    "    mean_values = df.loc[:, 'min_00':'min_59'].mean()\n",
    "    std_values = df.loc[:, 'min_00':'min_59'].std()\n",
    "\n",
    "    # create new df\n",
    "    new_df = df.copy()\n",
    "\n",
    "    # standardise each minute column\n",
    "    for minute in range(60):\n",
    "        column_name = f'min_{minute:02d}'\n",
    "        new_df[column_name] = (df[column_name] - mean_values[minute]) / std_values[minute]\n",
    "    \n",
    "    # assign new df to variable with new df name\n",
    "    globals()[new_df_name] = new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Features:\n",
    "\n",
    "* mean\n",
    "* median\n",
    "* std\n",
    "* variance\n",
    "* kurtosis\n",
    "* coefficient of variance\n",
    "* interquartile range \n",
    "* min\n",
    "* max\n",
    "* trimmed mean\n",
    "\n",
    "Frequency Features: \n",
    "\n",
    "* Spectral density\n",
    "* Entropy\n",
    "* Skewness\n",
    "* Spectral flatness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_trim_day_stand: 13172\n",
      "no_trim_night_stand: 10880\n",
      "no_trim_full_day_stand: 26230\n",
      "trim1_day_stand: 9055\n",
      "trim1_night_stand: 7213\n",
      "trim1_full_day_stand: 17722\n"
     ]
    }
   ],
   "source": [
    "# print lengths of new dataframes\n",
    "for df_name in new_df_names:\n",
    "    print(f\"{df_name}: {len(globals()[df_name])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "\n",
    "def calculate_all_features(df):\n",
    "    # Fast Fourier Transform (FFT) to each row\n",
    "    fft_columns = df.iloc[:, 4:].apply(lambda row: np.fft.fft(row), axis=1)\n",
    "    \n",
    "    # power spectral density (PSD)\n",
    "    psd = fft_columns.apply(lambda row: np.abs(row) ** 2)\n",
    "    \n",
    "    # time domain features\n",
    "    df['TDmean'] = df.iloc[:, 4:].mean(axis=1)\n",
    "    df['TDmedian'] = df.iloc[:, 4:].median(axis=1)\n",
    "    df['TDstd'] = df.iloc[:, 4:].std(axis=1)\n",
    "    df['TDvariance'] = df.iloc[:, 4:].var(axis=1)\n",
    "    df['TDmin'] = df.iloc[:, 4:].min(axis=1)\n",
    "    df['TDmax'] = df.iloc[:, 4:].max(axis=1)\n",
    "    # trimmed mean 25\n",
    "    df['TDtrimmed_mean'] = df.iloc[:, 4:].apply(lambda row: np.mean(row[(row >= np.percentile(row, 5)) & (row <= np.percentile(row, 95))]), axis=1)\n",
    "    df['TDkurtosis'] = df.iloc[:, 4:].apply(lambda row: kurtosis(row), axis=1)\n",
    "    #df['TDskewness'] = df.iloc[:, 4:].apply(lambda row: skew(row), axis=1)\n",
    "    df['TDcoefficient_of_variance'] = df['TDstd'] / df['TDmean']\n",
    "    df['TDinterquartile_range'] = df.iloc[:, 4:].apply(lambda row: np.percentile(row, 75) - np.percentile(row, 25), axis=1)\n",
    "    \n",
    "    # frequency domain features\n",
    "    df['FDmean'] = psd.apply(np.mean)\n",
    "    df['FDmedian'] = psd.apply(np.median)\n",
    "    df['FDstd'] = psd.apply(np.std)\n",
    "    df['FDvariance'] = psd.apply(np.var)\n",
    "    df['FDmin'] = psd.apply(np.min)\n",
    "    df['FDmax'] = psd.apply(np.max)\n",
    "    df['FDtrimmed_mean'] = psd.apply(lambda row: np.mean(row[(row >= np.percentile(row, 5)) & (row <= np.percentile(row, 95))]))\n",
    "    df['FDkurtosis'] = psd.apply(lambda row: kurtosis(row))\n",
    "    df['FDentropy'] = fft_columns.apply(lambda row: entropy(np.abs(row)))\n",
    "    df['FDskewness'] = fft_columns.apply(lambda row: skew(np.abs(row)))\n",
    "    df['FDspectral_flatness'] = fft_columns.apply(lambda row: np.exp(np.mean(np.log(np.abs(row) + 1e-10))))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  all features for each dataframe\n",
    "trim_day = calculate_all_features(trim1_day_stand)\n",
    "trim_night = calculate_all_features(trim1_night_stand)\n",
    "trim_full = calculate_all_features(trim1_full_day_stand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trim_day: (9055, 86)\n",
      "trim_night: (7213, 85)\n",
      "trim_full: (17722, 85)\n"
     ]
    }
   ],
   "source": [
    "# print shapes of new dataframes\n",
    "for df_name in ['trim_day', 'trim_night', 'trim_full']:\n",
    "    print(f\"{df_name}: {globals()[df_name].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEXT Steps\n",
    "\n",
    "TODO\n",
    "\n",
    "1. new no trim dataframes\n",
    "2. train rf model for 'trim' and 'no trim' dataframes - by rows (as per Rodriguez)\n",
    "3. train rf model for 'trim' and 'no trom' dataframes - to predict persons\n",
    "4. compare results to Tom's and article results\n",
    "5. discussion - what is this all about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrap - to be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Fourier Transform (FFT) to each row\n",
    "fft_columns = trim1_day_stand.iloc[:, 4:].apply(lambda row: np.fft.fft(row), axis=1)\n",
    "    \n",
    "# power spectral density (PSD)\n",
    "psd = fft_columns.apply(lambda row: np.abs(row) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency domain features\n",
    "df['FDmean'] = psd.mean(axis=1)\n",
    "df['FDmedian'] = psd.median(axis=1)\n",
    "df['FDmin'] = psd.min(axis=1)\n",
    "df['FDmax'] = psd.max(axis=1)\n",
    "df['FDentropy'] = fft_columns.apply(lambda row: entropy(np.abs(row)))\n",
    "df['FDskewness'] = fft_columns.apply(lambda row: skew(np.abs(row)))\n",
    "df['FDspectral_flatness'] = fft_columns.apply(lambda row: np.exp(np.mean(np.log(np.abs(row) + 1e-10))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igp5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
