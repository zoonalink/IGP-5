{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreating Rodriguez-Ruiz \n",
    "\n",
    "* Using Tom's code/plan\n",
    "* And my code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_from_folder(folderpath, downsample=None, save_to_csv=False, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Extract CSV data from folder and subfolders into a dataframe.\n",
    "\n",
    "    Args:\n",
    "      folderpath (str): folder containing CSV files.\n",
    "      downsample (int, optional): number of rows to downsample CSVs to. Defaults to None.\n",
    "      save_to_csv (bool, optional): save the updated df to a CSV file? defaults to False.\n",
    "      output_csv_path (str, optional): csv filepath. required if save_to_csv is True.\n",
    "\n",
    "    Returns:\n",
    "      pandas.DataFrame: DataFrame of concatenated CSV data.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    # dict to store dataframes by condition  \n",
    "    dfs = {'control': [], 'condition': []}\n",
    "\n",
    "    try:\n",
    "        # subfolders\n",
    "        subfolders = [f for f in os.listdir(folderpath) if os.path.isdir(os.path.join(folderpath, f))]\n",
    "\n",
    "        for subfolder in subfolders:\n",
    "            subfolderpath = os.path.join(folderpath, subfolder)  \n",
    "\n",
    "            # list of CSV files\n",
    "            files = os.listdir(subfolderpath)\n",
    "\n",
    "            for file in files:\n",
    "                filepath = os.path.join(subfolderpath, file)\n",
    "\n",
    "                # extract ID from filename \n",
    "                id = file.split('.')[0]\n",
    "\n",
    "                df = pd.read_csv(filepath)\n",
    "\n",
    "                # optional downsample \n",
    "                if downsample:\n",
    "                    df = df.sample(downsample)\n",
    "\n",
    "                # ID column - this is the filename without the extension\n",
    "                df['id'] = id\n",
    "\n",
    "                # 'condition' column\n",
    "                df['condition'] = subfolder\n",
    "\n",
    "                # convert 'timestamp' and 'date' to datetime\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "                # append to dict by condition\n",
    "                if subfolder == 'control':\n",
    "                    dfs['control'].append(df)\n",
    "                else:  \n",
    "                    dfs['condition'].append(df)\n",
    "\n",
    "    except OSError:\n",
    "        print(f\"Error reading folder: {folderpath}\")\n",
    "\n",
    "    # concatenate dfs for each condition\n",
    "    dfs['control'] = pd.concat(dfs['control'])\n",
    "    dfs['condition'] = pd.concat(dfs['condition'])\n",
    "\n",
    "    # reset index on the final df\n",
    "    df = pd.concat([dfs['control'], dfs['condition']]).reset_index(drop=True)\n",
    "\n",
    "    # add label column\n",
    "    df['label'] = 0\n",
    "    df.loc[df['condition'] == 'condition', 'label'] = 1\n",
    "    \n",
    "    # remove old 'condition' column\n",
    "    df.drop('condition', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    try:\n",
    "        if save_to_csv:\n",
    "            if output_csv_path:\n",
    "                df.to_csv(output_csv_path, index=False)\n",
    "                print(f\"df saved to {output_csv_path}\")\n",
    "            else:\n",
    "                print(\"Error: Please provide an output CSV path.\")\n",
    "        \n",
    "        \n",
    "        return df\n",
    "    except OSError:\n",
    "        print(\"Error saving to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction of all the activity data into one data frame\n",
    "folderpath = '../data/depresjon'\n",
    "# full ds, no csv\n",
    "df = extract_from_folder(folderpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.head())\n",
    "#print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "1. equal observations per subject\n",
    "2. segmentation into hourly\n",
    "3. day, night, full \n",
    "4. NaNs and standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Reduction\n",
    "\n",
    ">\"For the pre-processing stage, the next step are proposed. Since the total amount of data recorded for each subject is different, a new subset of data is extracted, adjusting the number of observations to be equal for each subject.\"\n",
    "\n",
    "This step is not adequately described as there are many ways to approach this.\n",
    "\n",
    "Below, I have tried three approaches: \n",
    "\n",
    "1. Reducing data to the maximum viable number of rows - that is, finding the minimum of all ids and reducing all other id's rows to this value. \n",
    "   * this can be done by `head()` or `sample()` methods\n",
    "2. Reducing to 'full days' first and then minimising the dataset\n",
    "3. Reducing to match 'num_days' in scores.csv and then minimising the dataset. \n",
    "\n",
    "Finally, I did none of the above and simply used the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min rows per id\n",
    "\n",
    "* this is reducing to maximum viable number of rows, no other processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce df to min number of rows per id - so each id has the same number of rows\n",
    "min_rows = df['id'].value_counts().min()\n",
    "trim = df.groupby('id').apply(lambda x: x.head(min_rows), include_groups=False).reset_index()\n",
    "\n",
    "# drop the 'level_1' column\n",
    "trim = trim.drop(columns='level_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19299]\n"
     ]
    }
   ],
   "source": [
    "# print unique row counts per id\n",
    "print(trim['id'].value_counts().unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(trim.info())\n",
    "#print(trim.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full days\n",
    "\n",
    "This includes the 'preprocess to full days' step before reducing to max viable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_full_days(df, save_to_csv=False, output_csv_path=None, print_info=True):\n",
    "    \"\"\"\n",
    "    Extracts full days from a dataframe.\n",
    "\n",
    "    Args::\n",
    "    df (DataFrame): input df.\n",
    "    save_to_csv (bool, optional): save the updated df to a CSV file? defaults to False.\n",
    "    output_csv_path (str, optional): csv filepath. required if save_to_csv is True.\n",
    "    print_info (bool, optional): print info about the df. defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: df containing only full days (1440 rows per day).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # group by id and date, count rows, and filter where count equals 1440\n",
    "    full_days_df = df.groupby(['id', 'date']).filter(lambda x: len(x) == 1440)\n",
    "\n",
    "    # set index to timestamp\n",
    "    #full_days_df.set_index(['timestamp'], inplace=True)\n",
    "    \n",
    "    if print_info:\n",
    "        # print id and date combinations that don't have 1440 rows\n",
    "        not_full_days = df.groupby(['id', 'date']).size().reset_index(name='count').query('count != 1440')\n",
    "        print(\"\\nid and date combinations that don't have 1440 rows and have been removed:\\n\")\n",
    "        print(not_full_days)\n",
    "\n",
    "        # print info\n",
    "        print(\"\\nfull_days_df info:\\n\")\n",
    "        print(full_days_df.info())\n",
    "\n",
    "        #print full days per id\n",
    "        print(\"\\nfull days per id:\\n\")\n",
    "        print(full_days_df.groupby('id').size()/1440)\n",
    "\n",
    "        # print min number of days\n",
    "        print(\"\\nmin number of days per id:\\n\")\n",
    "        print(full_days_df.groupby('id').size().min()/1440)\n",
    "        \n",
    "\n",
    "    try:\n",
    "        if save_to_csv:\n",
    "            if output_csv_path:\n",
    "                full_days_df.to_csv(output_csv_path, index=False)\n",
    "                print(f\"df saved to {output_csv_path}\")\n",
    "            else:\n",
    "                print(\"Error: Please provide an output CSV path.\")\n",
    "        \n",
    "        \n",
    "        return full_days_df\n",
    "    except OSError:\n",
    "        print(\"Error saving to CSV.\")\n",
    "\n",
    "    return full_days_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17280]\n"
     ]
    }
   ],
   "source": [
    "# full days\n",
    "full = preprocess_full_days(df, print_info=False)\n",
    "#print(full.info())\n",
    "\n",
    "# reduce df to min number of rows per id - so each id has the same number of rows\n",
    "min_rows = full['id'].value_counts().min()\n",
    "trim2 = full.groupby('id').apply(lambda x: x.head(min_rows), include_groups=False).reset_index()\n",
    "\n",
    "# drop the 'level_1' column\n",
    "trim2 = trim2.drop(columns='level_1')\n",
    "\n",
    "# print unique row counts per id\n",
    "print(trim2['id'].value_counts().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce to 'number of days' from scores\n",
    "\n",
    "This step reduces to num_days as per scores.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_days_per_scores(df, scores_csv_path='..\\data\\depresjon\\scores.csv', save_to_csv=True, output_csv_path=None):\n",
    "    \"\"\"\n",
    "    Extract the number of days per ID from the 'scores' data.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): df containing the 'id' column.\n",
    "        scores_csv_path (str, optional): path to the 'scores' CSV file. Defaults to '..\\data\\depresjon\\scores.csv'.\n",
    "        save_to_csv (bool, optional): save the updated df to a CSV file? Defaults to True.\n",
    "        output_csv_path (str, optional): csv filepath. Required if save_to_csv is True.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df with the specified number of days per ID based on 'scores'.\n",
    "    \"\"\"\n",
    "    # scores from the CSV file\n",
    "    scores_df = pd.read_csv(scores_csv_path)\n",
    "\n",
    "    # merge scores with the df based on the 'id' column\n",
    "    merged_df = pd.merge(df, scores_df, left_on='id', right_on='number', how='left')\n",
    "\n",
    "    # filter rows to keep the specified number of days\n",
    "    df_filtered = merged_df.groupby('id', group_keys=False, as_index=False, sort=False).apply(lambda group: group.head(group['days'].min() * 1440)).reset_index(drop=True)\n",
    "\n",
    "    # drop cols number, days, gender, age, afftype, melanch, inpatient, edu, marriage, work, madrs1, madrs2\n",
    "    cols = ['number', 'number', 'days', 'gender', 'age', 'afftype', 'melanch', 'inpatient', 'edu', 'marriage', 'work', 'madrs1', 'madrs2']\n",
    "    df_filtered.drop(cols, axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    # save to CSV\n",
    "    if save_to_csv:\n",
    "        if output_csv_path:\n",
    "            df_filtered.to_csv(output_csv_path, index=False)\n",
    "            print(f\"\\n\\ndf saved to {output_csv_path}\")\n",
    "        else:\n",
    "            print(\"Error: Please provide an output CSV path.\")\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_8976\\242992910.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_filtered = merged_df.groupby('id', group_keys=False, as_index=False, sort=False).apply(lambda group: group.head(group['days'].min() * 1440)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7200]\n"
     ]
    }
   ],
   "source": [
    "num_days = extract_days_per_scores(df, save_to_csv=False, output_csv_path=None)\n",
    "\n",
    "#print(num_days.info())\n",
    "\n",
    "# reduce df to min number of rows per id - so each id has the same number of rows\n",
    "min_rows = num_days['id'].value_counts().min()\n",
    "trim3 = num_days.groupby('id').apply(lambda x: x.head(min_rows), include_groups=False).reset_index()\n",
    "\n",
    "# drop the 'level_1' column\n",
    "trim3 = trim3.drop(columns='level_1')\n",
    "\n",
    "# print unique row counts per id\n",
    "print(trim3['id'].value_counts().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment into hourly intervals\n",
    "\n",
    "\"The structure of the data for every observation is contained by 61 columns; one column for the monitored hour and one column for each minute (60 columns) of motor activity. This segmentation allowed the classification of depressive episodes per hour.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26230, 64)\n"
     ]
    }
   ],
   "source": [
    "# copy df\n",
    "no_trim = df.copy()\n",
    "\n",
    "# extract hour and minute from timestamp\n",
    "no_trim['hour'] = no_trim['timestamp'].dt.hour\n",
    "no_trim['minute'] = no_trim['timestamp'].dt.minute\n",
    "\n",
    "# pivot the DataFrame\n",
    "no_trim_piv = no_trim.pivot(index=['date', 'id', 'label', 'hour'], columns='minute', values='activity')\n",
    "\n",
    "# rename columns\n",
    "no_trim_piv.columns = [f'min_{minute:02d}' for minute in range(60)]\n",
    "\n",
    "# Reset index\n",
    "no_trim_piv.reset_index(inplace=True)\n",
    "\n",
    "#  NaN with 0 (for missing minute values)\n",
    "no_trim = no_trim_piv.fillna(0)\n",
    "\n",
    "# print hourly_data shape\n",
    "print(no_trim.shape)\n",
    "\n",
    "# print info\n",
    "#print(no_trim.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date            id  label  hour  min_00  min_01  min_02  min_03  \\\n",
      "0 2002-05-24  condition_20      1    11     0.0     0.0     0.0     0.0   \n",
      "\n",
      "   min_04  min_05  ...  min_50  min_51  min_52  min_53  min_54  min_55  \\\n",
      "0     0.0     0.0  ...    83.0     0.0     0.0     0.0     3.0     0.0   \n",
      "\n",
      "   min_56  min_57  min_58  min_59  \n",
      "0   249.0   209.0   360.0    36.0  \n",
      "\n",
      "[1 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "print(no_trim.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim 1 - no processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract hour and minute from timestamp\n",
    "trim['hour'] = trim['timestamp'].dt.hour\n",
    "trim['minute'] = trim['timestamp'].dt.minute\n",
    "\n",
    "# pivot the DataFrame\n",
    "df_pivot = trim.pivot(index=['date', 'id', 'label', 'hour'], columns='minute', values='activity')\n",
    "\n",
    "# rename columns\n",
    "df_pivot.columns = [f'min_{minute:02d}' for minute in range(60)]\n",
    "\n",
    "# Reset index\n",
    "df_pivot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_pivot.head(5))\n",
    "#print(df_pivot.info())\n",
    "#print(df_pivot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print any missing minute data\n",
    "missing = df_pivot[df_pivot.isnull().any(axis=1)]\n",
    "#print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17722, 64)\n"
     ]
    }
   ],
   "source": [
    "#  NaN with 0 (for missing minute values)\n",
    "trim1_piv = df_pivot.fillna(0)\n",
    "\n",
    "# print hourly_data shape\n",
    "print(trim1_piv.shape)\n",
    "\n",
    "# print info\n",
    "#print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim2 - full days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15840, 64)\n"
     ]
    }
   ],
   "source": [
    "# extract hour and minute from timestamp\n",
    "trim2['hour'] = trim2['timestamp'].dt.hour\n",
    "trim2['minute'] = trim2['timestamp'].dt.minute\n",
    "\n",
    "# pivot the DataFrame\n",
    "df_pivot2 = trim2.pivot(index=['date', 'id', 'label', 'hour'], columns='minute', values='activity')\n",
    "\n",
    "# rename columns\n",
    "df_pivot2.columns = [f'min_{minute:02d}' for minute in range(60)]\n",
    "\n",
    "# Reset index\n",
    "df_pivot2.reset_index(inplace=True)\n",
    "\n",
    "#  NaN with 0 (for missing minute values)\n",
    "trim2_piv = df_pivot2.fillna(0)\n",
    "\n",
    "# print hourly_data shape\n",
    "print(trim2_piv.shape)\n",
    "\n",
    "# print info\n",
    "#print(df2.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim 3 - 'num_days'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6613, 64)\n"
     ]
    }
   ],
   "source": [
    "# extract hour and minute from timestamp\n",
    "trim3['hour'] = trim3['timestamp'].dt.hour\n",
    "trim3['minute'] = trim3['timestamp'].dt.minute\n",
    "\n",
    "# pivot the DataFrame\n",
    "df_pivot3 = trim3.pivot(index=['date', 'id', 'label', 'hour'], columns='minute', values='activity')\n",
    "\n",
    "# rename columns\n",
    "df_pivot3.columns = [f'min_{minute:02d}' for minute in range(60)]\n",
    "\n",
    "# Reset index\n",
    "df_pivot3.reset_index(inplace=True)\n",
    "\n",
    "#  NaN with 0 (for missing minute values)\n",
    "trim3_piv = df_pivot3.fillna(0)\n",
    "\n",
    "# print hourly_data shape\n",
    "print(trim3_piv.shape)\n",
    "\n",
    "# print info\n",
    "#print(df3.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, these dataframes are the following lengths:\n",
    "\n",
    "* no trim - 26230\n",
    "* trim to min - 17722\n",
    "* trim to min of full days - 15840\n",
    "* trim to min of num_days - 6613"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new dataframes - day, night, full\n",
    "\n",
    "\"Therefore, based on the hourly segmentation, three different subsets are constructed; night motor activity (from 21 to 7 h taking into account the sunrise standard hours) [21], day motor activity (from 8 to 20 h) and finally all day motor activity with the total day hours.\"\n",
    "\n",
    "* 8 am - 8 pm (12 hours)\n",
    "* 9 pm - 7 am (10 hours)\n",
    "\n",
    "Why do it this way? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1061445 entries, 0 to 1061444\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count    Dtype         \n",
      "---  ------     --------------    -----         \n",
      " 0   id         1061445 non-null  object        \n",
      " 1   timestamp  1061445 non-null  datetime64[ns]\n",
      " 2   date       1061445 non-null  datetime64[ns]\n",
      " 3   activity   1061445 non-null  int64         \n",
      " 4   label      1061445 non-null  int64         \n",
      " 5   hour       1061445 non-null  int32         \n",
      " 6   minute     1061445 non-null  int32         \n",
      "dtypes: datetime64[ns](2), int32(2), int64(2), object(1)\n",
      "memory usage: 48.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(trim.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9055, 64)\n",
      "(7213, 64)\n",
      "(17722, 64)\n"
     ]
    }
   ],
   "source": [
    "#  subsets based on time ranges\n",
    "trim1_day_df = trim1_piv[(trim1_piv['hour'] >= 8) & (trim1_piv['hour'] < 20)]  # day: 8 am to 8 pm\n",
    "trim1_night_df = trim1_piv[(trim1_piv['hour'] >= 21) | (trim1_piv['hour'] < 7)]  # night: 9 pm to 7 am\n",
    "trim1_full_day_df = trim1_piv  # full day:  24 hours\n",
    "\n",
    "# print shapes\n",
    "print(trim1_day_df.shape)\n",
    "print(trim1_night_df.shape)\n",
    "print(trim1_full_day_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not match Rodriquez dataframe lengths: \n",
    "\n",
    "* day = 14168 obs\n",
    "* night = 11945 obs\n",
    "* full day = 26113 obs\n",
    "\n",
    "![](2024-03-23-22-15-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim 2 and 3 \n",
    "\n",
    "* will also not match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No trim\n",
    "\n",
    "This is much closer to to the dataset lengths reported by Rodgriguez.\n",
    "\n",
    "Questions:\n",
    "\n",
    "* Did they actually do what they have written?\n",
    "* What exactly did they do?\n",
    "* How have they created the day/night segments? Are hours inclusive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13172, 64)\n",
      "(10880, 64)\n",
      "(26230, 64)\n"
     ]
    }
   ],
   "source": [
    "#  subsets based on time ranges\n",
    "no_trim_day_df = no_trim[(no_trim['hour'] >= 8) & (no_trim['hour'] < 20)]  # day: 8 am to 8 pm\n",
    "no_trim_night_df = no_trim[(no_trim['hour'] >= 21) | (no_trim['hour'] < 7)]  # night: 9 pm to 7 am\n",
    "no_trim_full_day_df = no_trim  # full day:  24 hours\n",
    "\n",
    "# print shapes\n",
    "print(no_trim_day_df.shape)\n",
    "print(no_trim_night_df.shape)\n",
    "print(no_trim_full_day_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [date, id, label, hour, min_00, min_01, min_02, min_03, min_04, min_05, min_06, min_07, min_08, min_09, min_10, min_11, min_12, min_13, min_14, min_15, min_16, min_17, min_18, min_19, min_20, min_21, min_22, min_23, min_24, min_25, min_26, min_27, min_28, min_29, min_30, min_31, min_32, min_33, min_34, min_35, min_36, min_37, min_38, min_39, min_40, min_41, min_42, min_43, min_44, min_45, min_46, min_47, min_48, min_49, min_50, min_51, min_52, min_53, min_54, min_55, min_56, min_57, min_58, min_59]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 64 columns]\n",
      "Empty DataFrame\n",
      "Columns: [date, id, label, hour, min_00, min_01, min_02, min_03, min_04, min_05, min_06, min_07, min_08, min_09, min_10, min_11, min_12, min_13, min_14, min_15, min_16, min_17, min_18, min_19, min_20, min_21, min_22, min_23, min_24, min_25, min_26, min_27, min_28, min_29, min_30, min_31, min_32, min_33, min_34, min_35, min_36, min_37, min_38, min_39, min_40, min_41, min_42, min_43, min_44, min_45, min_46, min_47, min_48, min_49, min_50, min_51, min_52, min_53, min_54, min_55, min_56, min_57, min_58, min_59]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 64 columns]\n",
      "Empty DataFrame\n",
      "Columns: [date, id, label, hour, min_00, min_01, min_02, min_03, min_04, min_05, min_06, min_07, min_08, min_09, min_10, min_11, min_12, min_13, min_14, min_15, min_16, min_17, min_18, min_19, min_20, min_21, min_22, min_23, min_24, min_25, min_26, min_27, min_28, min_29, min_30, min_31, min_32, min_33, min_34, min_35, min_36, min_37, min_38, min_39, min_40, min_41, min_42, min_43, min_44, min_45, min_46, min_47, min_48, min_49, min_50, min_51, min_52, min_53, min_54, min_55, min_56, min_57, min_58, min_59]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# print missing data from no_trim dfs\n",
    "missing_no_trim_day = no_trim_day_df[no_trim_day_df.isnull().any(axis=1)]\n",
    "missing_no_trim_night = no_trim_night_df[no_trim_night_df.isnull().any(axis=1)]\n",
    "missing_no_trim_full_day = no_trim_full_day_df[no_trim_full_day_df.isnull().any(axis=1)]\n",
    "print(missing_no_trim_day)\n",
    "print(missing_no_trim_night)\n",
    "print(missing_no_trim_full_day)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation of motor activity\n",
    "\n",
    "* Going to proceed with `trim1_` day/night/full and `no_trim_` day/night/full dataframes.\n",
    "\n",
    "\n",
    "* standardisation: $[ z_i = \\frac{{x_i - \\bar{x}}}{{s}} ]$\n",
    "\n",
    "Where:\n",
    "* $(z_i)$ is the standardized value for observation $(i)$.\n",
    "* $(x_i)$ is the original value for observation $(i)$.\n",
    "* $(\\bar{x})$ is the mean (average) of the entire dataset.\n",
    "* $(s)$ is the standard deviation of the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date            id  label  hour  min_00  min_01  min_02  min_03  \\\n",
      "0 2002-05-24  condition_20      1    11     0.0     0.0     0.0     0.0   \n",
      "\n",
      "   min_04  min_05  ...  min_50  min_51  min_52  min_53  min_54  min_55  \\\n",
      "0     0.0     0.0  ...    83.0     0.0     0.0     0.0     3.0     0.0   \n",
      "\n",
      "   min_56  min_57  min_58  min_59  \n",
      "0   249.0   209.0   360.0    36.0  \n",
      "\n",
      "[1 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# print head(1) of trim1_day_df\n",
    "print(trim1_day_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_8976\\4051920383.py:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  new_df[column_name] = (df[column_name] - mean_values[minute]) / std_values[minute]\n"
     ]
    }
   ],
   "source": [
    "# list of df\n",
    "dataframes = [no_trim_day_df, no_trim_night_df, no_trim_full_day_df,\n",
    "              trim1_day_df, trim1_night_df, trim1_full_day_df]\n",
    "\n",
    "# list of new df names\n",
    "new_df_names = ['no_trim_day_stand', 'no_trim_night_stand', 'no_trim_full_day_stand',\n",
    "                'trim1_day_stand', 'trim1_night_stand', 'trim1_full_day_stand']\n",
    "\n",
    "# standardise each df\n",
    "for df, new_df_name in zip(dataframes, new_df_names):\n",
    "    # mean and standard deviation for the entire dataset\n",
    "    mean_values = df.loc[:, 'min_00':'min_59'].mean()\n",
    "    std_values = df.loc[:, 'min_00':'min_59'].std()\n",
    "\n",
    "    # create new df\n",
    "    new_df = df.copy()\n",
    "\n",
    "    # standardise each minute column\n",
    "    for minute in range(60):\n",
    "        column_name = f'min_{minute:02d}'\n",
    "        new_df[column_name] = (df[column_name] - mean_values[minute]) / std_values[minute]\n",
    "    \n",
    "    # assign new df to variable with new df name\n",
    "    globals()[new_df_name] = new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Features:\n",
    "\n",
    "* mean\n",
    "* median\n",
    "* std\n",
    "* variance\n",
    "* kurtosis\n",
    "* coefficient of variance\n",
    "* interquartile range \n",
    "* min\n",
    "* max\n",
    "* trimmed mean\n",
    "\n",
    "Frequency Features: \n",
    "\n",
    "* Spectral density\n",
    "* Entropy\n",
    "* Skewness\n",
    "* Spectral flatness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_trim_day_stand: 13172\n",
      "no_trim_night_stand: 10880\n",
      "no_trim_full_day_stand: 26230\n",
      "trim1_day_stand: 9055\n",
      "trim1_night_stand: 7213\n",
      "trim1_full_day_stand: 17722\n"
     ]
    }
   ],
   "source": [
    "# print lengths of new dataframes\n",
    "for df_name in new_df_names:\n",
    "    print(f\"{df_name}: {len(globals()[df_name])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "\n",
    "def calculate_all_features(df):\n",
    "    # Fast Fourier Transform (FFT) to each row\n",
    "    fft_columns = df.iloc[:, 4:].apply(lambda row: np.fft.fft(row), axis=1)\n",
    "    \n",
    "    # power spectral density (PSD)\n",
    "    psd = fft_columns.apply(lambda row: np.abs(row) ** 2)\n",
    "    \n",
    "    # time domain features\n",
    "    df['TDmean'] = df.iloc[:, 4:].mean(axis=1)\n",
    "    df['TDmedian'] = df.iloc[:, 4:].median(axis=1)\n",
    "    df['TDstd'] = df.iloc[:, 4:].std(axis=1)\n",
    "    df['TDvariance'] = df.iloc[:, 4:].var(axis=1)\n",
    "    df['TDmin'] = df.iloc[:, 4:].min(axis=1)\n",
    "    df['TDmax'] = df.iloc[:, 4:].max(axis=1)\n",
    "    # trimmed mean 20%\n",
    "    df['TDtrimmed_mean'] = df.iloc[:, 4:].apply(lambda row: np.mean(row[(row >= np.percentile(row, 10)) & (row <= np.percentile(row, 90))]), axis=1)\n",
    "    df['TDkurtosis'] = df.iloc[:, 4:].apply(lambda row: kurtosis(row), axis=1)\n",
    "    #df['TDskewness'] = df.iloc[:, 4:].apply(lambda row: skew(row), axis=1)\n",
    "    df['TDcoefficient_of_variance'] = df['TDstd'] / df['TDmean']\n",
    "    df['TDinterquartile_range'] = df.iloc[:, 4:].apply(lambda row: np.percentile(row, 75) - np.percentile(row, 25), axis=1)\n",
    "    \n",
    "    # frequency domain features\n",
    "    df['FDmean'] = psd.apply(np.mean)\n",
    "    df['FDmedian'] = psd.apply(np.median)\n",
    "    df['FDstd'] = psd.apply(np.std)\n",
    "    df['FDvariance'] = psd.apply(np.var)\n",
    "    df['FDmin'] = psd.apply(np.min)\n",
    "    df['FDmax'] = psd.apply(np.max)\n",
    "    df['FDtrimmed_mean'] = psd.apply(lambda row: np.mean(row[(row >= np.percentile(row, 10)) & (row <= np.percentile(row, 90))]))\n",
    "    df['FDkurtosis'] = psd.apply(lambda row: kurtosis(row))\n",
    "    df['FDskewness'] = psd.apply(lambda row: skew(row))\n",
    "    df['FDcoefficient_of_variance'] = df['FDstd'] / df['FDmean']\n",
    "    df['FDinterquartile_range'] = psd.apply(lambda row: np.percentile(row, 75) - np.percentile(row, 25))\n",
    "    df['FDentropy'] = fft_columns.apply(lambda row: entropy(np.abs(row)))\n",
    "    df['FDskewness'] = fft_columns.apply(lambda row: skew(np.abs(row)))\n",
    "    df['FDspectral_flatness'] = fft_columns.apply(lambda row: np.exp(np.mean(np.log(np.abs(row) + 1e-10))))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  all features for each dataframe\n",
    "trim_day = calculate_all_features(trim1_day_stand)\n",
    "trim_night = calculate_all_features(trim1_night_stand)\n",
    "trim_full = calculate_all_features(trim1_full_day_stand)\n",
    "\n",
    "# all features for each no trim dataframe\n",
    "no_trim_day = calculate_all_features(no_trim_day_stand)\n",
    "no_trim_night = calculate_all_features(no_trim_night_stand)\n",
    "no_trim_full = calculate_all_features(no_trim_full_day_stand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trim_day: (9055, 87)\n",
      "trim_night: (7213, 87)\n",
      "trim_full: (17722, 87)\n",
      "no_trim_day: (13172, 87)\n",
      "no_trim_night: (10880, 87)\n",
      "no_trim_full: (26230, 87)\n"
     ]
    }
   ],
   "source": [
    "# print shapes of new dataframes\n",
    "for df_name in ['trim_day', 'trim_night', 'trim_full', 'no_trim_day', 'no_trim_night', 'no_trim_full']:\n",
    "    print(f\"{df_name}: {globals()[df_name].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of dataframes\n",
    "df_names = ['trim_day', 'trim_night', 'trim_full', 'no_trim_day', 'no_trim_night', 'no_trim_full']\n",
    "\n",
    "# drop minute (min_00 to min_59) and 'hour', 'date' columns from dfs\n",
    "for df_name in df_names:\n",
    "    # drop minute columns\n",
    "    globals()[df_name].drop(columns=[f'min_{minute:02d}' for minute in range(60)], inplace=True)\n",
    "    # drop 'hour', 'date' columns\n",
    "    globals()[df_name].drop(columns=['hour', 'date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trim_day.csv saved\n",
      "trim_night.csv saved\n",
      "trim_full.csv saved\n",
      "no_trim_day.csv saved\n",
      "no_trim_night.csv saved\n",
      "no_trim_full.csv saved\n"
     ]
    }
   ],
   "source": [
    "# save all dfs to csv\n",
    "for df_name in df_names:\n",
    "    globals()[df_name].to_csv(f'../data/petter/{df_name}.csv', index=False)\n",
    "    print(f\"{df_name}.csv saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD CSVs HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "trim_day = pd.read_csv('../data/petter/trim_day.csv')\n",
    "trim_night = pd.read_csv('../data/petter/trim_night.csv')\n",
    "trim_full = pd.read_csv('../data/petter/trim_full.csv')\n",
    "no_trim_day = pd.read_csv('../data/petter/no_trim_day.csv')\n",
    "no_trim_night = pd.read_csv('../data/petter/no_trim_night.csv')\n",
    "no_trim_full = pd.read_csv('../data/petter/no_trim_full.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, matthews_corrcoef\n",
    "\n",
    "def split(df, group_by_id=True):\n",
    "    \"\"\"\n",
    "    Split the given DataFrame into training and testing datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame to be split.\n",
    "    - group_by_id (bool): If True, group the data by 'id' before splitting. Default is True.  This will allow prediction of depression state by person.\n",
    "\n",
    "    Returns:\n",
    "    - X_train (DataFrame): The features of the training dataset.\n",
    "    - y_train (Series): The labels of the training dataset.\n",
    "    - X_test (DataFrame): The features of the testing dataset.\n",
    "    - y_test (Series): The labels of the testing dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    if group_by_id:\n",
    "        # group by 'id' and split\n",
    "        train_ids, test_ids = train_test_split(df['id'].unique(), test_size=0.2, random_state=42)\n",
    "        train_data = df[df['id'].isin(train_ids)]\n",
    "        test_data = df[df['id'].isin(test_ids)]\n",
    "    else:\n",
    "        # split without grouping by 'id'\n",
    "        train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # separate features and labels\n",
    "    X_train = train_data.drop(['label', 'id'], axis=1)\n",
    "    y_train = train_data['label']\n",
    "    X_test = test_data.drop(['label', 'id'], axis=1)\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def fit_and_evaluate(X_train, y_train, X_test, y_test):\n",
    "    # separate numerical and categorical columns\n",
    "    numeric_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # one-hot encode categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', 'passthrough', numeric_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "        ])\n",
    "\n",
    "    # preprocess the data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # initialize \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # fit \n",
    "    rf.fit(X_train_processed, y_train)\n",
    "\n",
    "    # predictions\n",
    "    y_pred = rf.predict(X_test_processed)\n",
    "\n",
    "    # evaluate \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    specificity = report['0']['recall']\n",
    "    support = report['1']['support']\n",
    "\n",
    "    return accuracy, f1, conf_matrix, recall, mcc, precision, roc_auc, specificity, support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset used: trim_day\n",
      "Predicting 'label' by 'id':\n",
      "Accuracy: 0.6715, \n",
      "F1-score: 0.4255, \n",
      "Confusion Matrix:\n",
      "[[994 319]\n",
      " [275 220]]\n",
      "Recall: 0.4444, \n",
      "MCC: 0.1964\n"
     ]
    }
   ],
   "source": [
    "# split, fit and eval - trim_day\n",
    "X_train, y_train, X_test, y_test = split(trim_day, group_by_id=True)\n",
    "accuracy, f1, conf_matrix, recall, mcc, precision, roc_auc, specificity, support = fit_and_evaluate(X_train, y_train, X_test, y_test)\n",
    "print(\"Dataset used: trim_day\")\n",
    "print(\"Predicting 'label' by 'id':\")\n",
    "print(f\"Accuracy: {accuracy:.4f}, \\nF1-score: {f1:.4f}, \\nConfusion Matrix:\\n{conf_matrix}\\nRecall: {recall:.4f}, \\nMCC: {mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset used: trim_night\n",
      "Predicting 'label' by 'id':\n",
      "Accuracy: 0.6235, \n",
      "F1-score: 0.4087, \n",
      "Confusion Matrix:\n",
      "[[713 339]\n",
      " [205 188]]\n",
      "Recall: 0.4784, \n",
      "MCC: 0.1443\n"
     ]
    }
   ],
   "source": [
    "# split, fit and eval - trim_night\n",
    "X_train, y_train, X_test, y_test = split(trim_night, group_by_id=True)\n",
    "accuracy, f1, conf_matrix, recall, mcc, precision, roc_auc, specificity, support = fit_and_evaluate(X_train, y_train, X_test, y_test)\n",
    "print(\"Dataset used: trim_night\")\n",
    "print(\"Predicting 'label' by 'id':\")\n",
    "print(f\"Accuracy: {accuracy:.4f}, \\nF1-score: {f1:.4f}, \\nConfusion Matrix:\\n{conf_matrix}\\nRecall: {recall:.4f}, \\nMCC: {mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset used: trim_full\n",
      "Predicting 'label' by 'id':\n",
      "Accuracy: 0.6402, \n",
      "F1-score: 0.4017, \n",
      "Confusion Matrix:\n",
      "[[1841  735]\n",
      " [ 540  428]]\n",
      "Recall: 0.4421, \n",
      "MCC: 0.1488\n"
     ]
    }
   ],
   "source": [
    "# split, fit and eval - trim_full\n",
    "X_train, y_train, X_test, y_test = split(trim_full, group_by_id=True)\n",
    "accuracy, f1, conf_matrix, recall, mcc, precision, roc_auc, specificity, support = fit_and_evaluate(X_train, y_train, X_test, y_test)\n",
    "print(\"Dataset used: trim_full\")\n",
    "print(\"Predicting 'label' by 'id':\")\n",
    "print(f\"Accuracy: {accuracy:.4f}, \\nF1-score: {f1:.4f}, \\nConfusion Matrix:\\n{conf_matrix}\\nRecall: {recall:.4f}, \\nMCC: {mcc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset used: no_trim_day\n",
      "Predicting 'label' for 'id':\n",
      "Accuracy: 0.7289, \n",
      "F1-score: 0.3984, \n",
      "Confusion Matrix:\n",
      "[[1745  443]\n",
      " [ 297  245]]\n",
      "Recall: 0.4520, \n",
      "MCC: 0.2293\n",
      "\n",
      "\n",
      "Dataset used: no_trim_night\n",
      "Predicting 'label' for 'id':\n",
      "Accuracy: 0.7095, \n",
      "F1-score: 0.3531, \n",
      "Confusion Matrix:\n",
      "[[1423  385]\n",
      " [ 271  179]]\n",
      "Recall: 0.3978, \n",
      "MCC: 0.1706\n",
      "\n",
      "\n",
      "Dataset used: no_trim_full\n",
      "Predicting 'label' for 'id':\n",
      "Accuracy: 0.7131, \n",
      "F1-score: 0.3672, \n",
      "Confusion Matrix:\n",
      "[[3426  932]\n",
      " [ 629  453]]\n",
      "Recall: 0.4187, \n",
      "MCC: 0.1877\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_trim_dfs = [no_trim_day, no_trim_night, no_trim_full]\n",
    "df_names = ['no_trim_day', 'no_trim_night', 'no_trim_full']\n",
    "\n",
    "group_by_id = True  \n",
    "\n",
    "for df, df_name in zip(no_trim_dfs, df_names):\n",
    "    X_train, y_train, X_test, y_test = split(df, group_by_id=group_by_id)\n",
    "    accuracy, f1, conf_matrix, recall, mcc, precision, roc_auc, specificity, support = fit_and_evaluate(X_train, y_train, X_test, y_test)\n",
    "    print(f\"Dataset used: {df_name}\")\n",
    "    if group_by_id:\n",
    "        print(\"Predicting 'label' for 'id':\")\n",
    "    else: \n",
    "        print(\"Predicting 'label' for rows:\")\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}, \\nF1-score: {f1:.4f}, \\nConfusion Matrix:\\n{conf_matrix}\\nRecall: {recall:.4f}, \\nMCC: {mcc:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [trim_day, trim_night, trim_full, no_trim_day, no_trim_night, no_trim_full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Empty DataFrame\n",
      "Columns: [id, label, TDmean, TDmedian, TDstd, TDvariance, TDmin, TDmax, TDtrimmed_mean, TDkurtosis, TDcoefficient_of_variance, TDinterquartile_range, FDmean, FDmedian, FDstd, FDvariance, FDmin, FDmax, FDtrimmed_mean, FDkurtosis, FDentropy, FDskewness, FDspectral_flatness, FDcoefficient_of_variance, FDinterquartile_range]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 25 columns]\n",
      "Predicting 'label' by 'id':\n",
      "Accuracy: 0.6715, F1-score: 0.4255, Confusion Matrix:\n",
      "[[994 319]\n",
      " [275 220]]\n",
      "Recall: 0.4444, MCC: 0.1964\n",
      "\n",
      "Predicting 'label' by row:\n",
      "Accuracy: 0.6433, F1-score: 0.5305, Confusion Matrix:\n",
      "[[800 270]\n",
      " [376 365]]\n",
      "Recall: 0.4926, MCC: 0.2475\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluating: Empty DataFrame\n",
      "Columns: [id, label, TDmean, TDmedian, TDstd, TDvariance, TDmin, TDmax, TDtrimmed_mean, TDkurtosis, TDcoefficient_of_variance, TDinterquartile_range, FDmean, FDmedian, FDstd, FDvariance, FDmin, FDmax, FDtrimmed_mean, FDkurtosis, FDentropy, FDskewness, FDspectral_flatness, FDcoefficient_of_variance, FDinterquartile_range]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 25 columns]\n",
      "Predicting 'label' by 'id':\n",
      "Accuracy: 0.6235, F1-score: 0.4087, Confusion Matrix:\n",
      "[[713 339]\n",
      " [205 188]]\n",
      "Recall: 0.4784, MCC: 0.1443\n",
      "\n",
      "Predicting 'label' by row:\n",
      "Accuracy: 0.5897, F1-score: 0.4807, Confusion Matrix:\n",
      "[[577 241]\n",
      " [351 274]]\n",
      "Recall: 0.4384, MCC: 0.1487\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluating: Empty DataFrame\n",
      "Columns: [id, label, TDmean, TDmedian, TDstd, TDvariance, TDmin, TDmax, TDtrimmed_mean, TDkurtosis, TDcoefficient_of_variance, TDinterquartile_range, FDmean, FDmedian, FDstd, FDvariance, FDmin, FDmax, FDtrimmed_mean, FDkurtosis, FDentropy, FDskewness, FDspectral_flatness, FDcoefficient_of_variance, FDinterquartile_range]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 25 columns]\n",
      "Predicting 'label' by 'id':\n",
      "Accuracy: 0.6402, F1-score: 0.4017, Confusion Matrix:\n",
      "[[1841  735]\n",
      " [ 540  428]]\n",
      "Recall: 0.4421, MCC: 0.1488\n",
      "\n",
      "Predicting 'label' by row:\n",
      "Accuracy: 0.6262, F1-score: 0.4975, Confusion Matrix:\n",
      "[[1564  535]\n",
      " [ 790  656]]\n",
      "Recall: 0.4537, MCC: 0.2068\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluating: Empty DataFrame\n",
      "Columns: [id, label, TDmean, TDmedian, TDstd, TDvariance, TDmin, TDmax, TDtrimmed_mean, TDkurtosis, TDcoefficient_of_variance, TDinterquartile_range, FDmean, FDmedian, FDstd, FDvariance, FDmin, FDmax, FDtrimmed_mean, FDkurtosis, FDentropy, FDskewness, FDspectral_flatness, FDcoefficient_of_variance, FDinterquartile_range]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 25 columns]\n",
      "Predicting 'label' by 'id':\n",
      "Accuracy: 0.7289, F1-score: 0.3984, Confusion Matrix:\n",
      "[[1745  443]\n",
      " [ 297  245]]\n",
      "Recall: 0.4520, MCC: 0.2293\n",
      "\n",
      "Predicting 'label' by row:\n",
      "Accuracy: 0.6915, F1-score: 0.4805, Confusion Matrix:\n",
      "[[1446  291]\n",
      " [ 522  376]]\n",
      "Recall: 0.4187, MCC: 0.2738\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluating: Empty DataFrame\n",
      "Columns: [id, label, TDmean, TDmedian, TDstd, TDvariance, TDmin, TDmax, TDtrimmed_mean, TDkurtosis, TDcoefficient_of_variance, TDinterquartile_range, FDmean, FDmedian, FDstd, FDvariance, FDmin, FDmax, FDtrimmed_mean, FDkurtosis, FDentropy, FDskewness, FDspectral_flatness, FDcoefficient_of_variance, FDinterquartile_range]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 25 columns]\n",
      "Predicting 'label' by 'id':\n",
      "Accuracy: 0.7095, F1-score: 0.3531, Confusion Matrix:\n",
      "[[1423  385]\n",
      " [ 271  179]]\n",
      "Recall: 0.3978, MCC: 0.1706\n",
      "\n",
      "Predicting 'label' by row:\n",
      "Accuracy: 0.6448, F1-score: 0.4077, Confusion Matrix:\n",
      "[[1137  253]\n",
      " [ 520  266]]\n",
      "Recall: 0.3384, MCC: 0.1763\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluating: Empty DataFrame\n",
      "Columns: [id, label, TDmean, TDmedian, TDstd, TDvariance, TDmin, TDmax, TDtrimmed_mean, TDkurtosis, TDcoefficient_of_variance, TDinterquartile_range, FDmean, FDmedian, FDstd, FDvariance, FDmin, FDmax, FDtrimmed_mean, FDkurtosis, FDentropy, FDskewness, FDspectral_flatness, FDcoefficient_of_variance, FDinterquartile_range]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 25 columns]\n",
      "Predicting 'label' by 'id':\n",
      "Accuracy: 0.7131, F1-score: 0.3672, Confusion Matrix:\n",
      "[[3426  932]\n",
      " [ 629  453]]\n",
      "Recall: 0.4187, MCC: 0.1877\n",
      "\n",
      "Predicting 'label' by row:\n",
      "Accuracy: 0.6687, F1-score: 0.4468, Confusion Matrix:\n",
      "[[2806  606]\n",
      " [1132  702]]\n",
      "Recall: 0.3828, MCC: 0.2261\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for df in dataframes:\n",
    "    print(f\"Evaluating: {df.head(0)}\")\n",
    "    \n",
    "    # Predict the 'label' by 'id'\n",
    "    X_train, y_train, X_test, y_test = split(df, group_by_id=True)\n",
    "    accuracy, f1, conf_matrix, recall, mcc = fit_and_evaluate(X_train, y_train, X_test, y_test)\n",
    "    print(\"Predicting 'label' by 'id':\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, F1-score: {f1:.4f}, Confusion Matrix:\\n{conf_matrix}\\nRecall: {recall:.4f}, MCC: {mcc:.4f}\")\n",
    "\n",
    "    # Predict the 'label' by row\n",
    "    X_train, y_train, X_test, y_test = split(df, group_by_id=False)\n",
    "    accuracy, f1, conf_matrix, recall, mcc = fit_and_evaluate(X_train, y_train, X_test, y_test)\n",
    "    print(\"\\nPredicting 'label' by row:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, F1-score: {f1:.4f}, Confusion Matrix:\\n{conf_matrix}\\nRecall: {recall:.4f}, MCC: {mcc:.4f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEXT Steps\n",
    "\n",
    "TODO\n",
    "\n",
    "1. ~~new no trim dataframes~~\n",
    "2. ~~drop cols (minutes, hour)~~\n",
    "3. ~~prep dataframes for predicting person depression and by row~~\n",
    "4. train rf model for 'trim' and 'no trim' dataframes - by rows (as per Rodriguez)\n",
    "5. train rf model for 'trim' and 'no trom' dataframes - to predict persons\n",
    "6. compare results to Tom's and article results\n",
    "7. discussion - what is this all about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(X_train, y_train, X_test, y_test):\n",
    "    # model\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # fit the model\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # predictions\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    # evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, recall, mcc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igp5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
