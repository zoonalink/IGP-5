{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - baseline\n",
    "\n",
    "* Referencing: \n",
    "> **Bibliography:** Zanella-Calzada, L.A., Galván-Tejada, C.E., Chávez-Lamas, N.M., Gracia-Cortés, M. del C., Magallanes-Quintanar, R., Celaya-Padilla, J.M., Galván-Tejada, J.I. and Gamboa-Rosales, H. (2019) Feature Extraction in Motor Activity Signal: Towards a Depression Episodes Detection in Unipolar and Bipolar Patients. Diagnostics [online]. 9 (1), p. 8. Available from: https://www.mdpi.com/2075-4418/9/1/8 [Accessed 28 November 2023].\n",
    "* [article notes](../literature/Zanella-FeatureExtraction.md)\n",
    "\n",
    "Objective: \n",
    "\n",
    "* To fit a modified Random Forest model, taking inspiration from the above article\n",
    "\n",
    "## Plan \n",
    "\n",
    "1. Load and process `depresjon`\n",
    "   * load into pandas df\n",
    "   * select `control` and `condition` -> it seems that they used first 4 control and first 5 condition participants\n",
    "   * normalise data (mean = 0, std = 1)\n",
    "   * remove incomplete cases\n",
    "   * take only first value of each hour??  maybe mean for each hour\n",
    "\n",
    "2. Extract features - 14 features\n",
    "   * mean\n",
    "   * standard deviation\n",
    "   * variance\n",
    "   * trimmed mean\n",
    "   * coefficient of variation\n",
    "   * inversse coefficient of variation\n",
    "   * kurtosis\n",
    "   * skewness\n",
    "   * quantailes (1, 5, 25, 75, 95, 99)\n",
    "\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis, mstats\n",
    "from statsmodels.robust import mad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data\n",
    "\n",
    "### Load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_folder(folderpath, add_scores=False, downsample=None):\n",
    "    \"\"\"\n",
    "    Extract CSV data from folder and subfolders into a dataframe.\n",
    "\n",
    "    Args:\n",
    "      folderpath (str): Path to the folder containing CSV files.\n",
    "      add_scores (bool, optional): Boolean to add scores.csv to the dataframe. Defaults to False.\n",
    "      downsample (int, optional): Number of rows to downsample CSVs to. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "      pandas.DataFrame: DataFrame of concatenated CSV data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dict to store dataframes by condition  \n",
    "    dfs = {'control': [], 'condition': []}\n",
    "\n",
    "    try:\n",
    "        # Handle top-level scores CSV\n",
    "        if add_scores and 'scores.csv' in os.listdir(folderpath):\n",
    "            scores_path = os.path.join(folderpath, 'scores.csv')  \n",
    "            dfs['scores'] = pd.read_csv(scores_path)\n",
    "\n",
    "        # Get subfolders\n",
    "        subfolders = [f for f in os.listdir(folderpath) if os.path.isdir(os.path.join(folderpath, f))]\n",
    "\n",
    "        for subfolder in subfolders:\n",
    "            subfolderpath = os.path.join(folderpath, subfolder)  \n",
    "\n",
    "            # Get list of CSV files\n",
    "            files = os.listdir(subfolderpath)\n",
    "\n",
    "            for file in files:\n",
    "                filepath = os.path.join(subfolderpath, file)\n",
    "\n",
    "                # Extract ID from filename \n",
    "                id = file.split('.')[0]\n",
    "\n",
    "                df = pd.read_csv(filepath)\n",
    "\n",
    "                # Downsample if needed\n",
    "                if downsample:\n",
    "                    df = df.sample(downsample)\n",
    "\n",
    "                # Add ID column - this is the filename without the extension\n",
    "                df['id'] = id\n",
    "\n",
    "                # Add 'condition' column\n",
    "                df['condition'] = subfolder\n",
    "\n",
    "                # Convert 'timestamp' and 'date' to datetime\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "                # Append to dict by condition\n",
    "                if subfolder == 'control':\n",
    "                    dfs['control'].append(df)\n",
    "                else:  \n",
    "                    dfs['condition'].append(df)\n",
    "\n",
    "    except OSError:\n",
    "        print(f\"Error reading folder: {folderpath}\")\n",
    "\n",
    "    # concatenate dfs for each condition\n",
    "    dfs['control'] = pd.concat(dfs['control'])\n",
    "    dfs['condition'] = pd.concat(dfs['condition'])\n",
    "\n",
    "    # Reset index on the final df\n",
    "    df = pd.concat([dfs['control'], dfs['condition']]).reset_index(drop=True)\n",
    "\n",
    "    # add label column\n",
    "    df['label'] = 0\n",
    "    df.loc[df['condition'] == 'condition', 'label'] = 1\n",
    "    \n",
    "    # remove old 'condition' column\n",
    "    df.drop('condition', axis=1, inplace=True)\n",
    "\n",
    "    # Final concat\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  timestamp       date  activity           id  label\n",
      "0       2003-03-18 15:00:00 2003-03-18        60    control_1      0\n",
      "1       2003-03-18 15:01:00 2003-03-18         0    control_1      0\n",
      "2       2003-03-18 15:02:00 2003-03-18       264    control_1      0\n",
      "3       2003-03-18 15:03:00 2003-03-18       662    control_1      0\n",
      "4       2003-03-18 15:04:00 2003-03-18       293    control_1      0\n",
      "...                     ...        ...       ...          ...    ...\n",
      "1571696 2004-06-10 14:58:00 2004-06-10         0  condition_9      1\n",
      "1571697 2004-06-10 14:59:00 2004-06-10         0  condition_9      1\n",
      "1571698 2004-06-10 15:00:00 2004-06-10         0  condition_9      1\n",
      "1571699 2004-06-10 15:01:00 2004-06-10         5  condition_9      1\n",
      "1571700 2004-06-10 15:02:00 2004-06-10         0  condition_9      1\n",
      "\n",
      "[1571701 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# set folder path\n",
    "folderpath = '../data/depresjon/'\n",
    "# extract all files\n",
    "all_files = extract_folder(folderpath)\n",
    "# print rows 21-24\n",
    "#print(all_files.iloc[21:25])\n",
    "print(all_files.head(-5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp       date  activity         id  label\n",
      "0 2003-03-18 15:00:00 2003-03-18        60  control_1      0\n",
      "1 2003-03-18 15:01:00 2003-03-18         0  control_1      0\n",
      "2 2003-03-18 15:02:00 2003-03-18       264  control_1      0\n",
      "3 2003-03-18 15:03:00 2003-03-18       662  control_1      0\n",
      "4 2003-03-18 15:04:00 2003-03-18       293  control_1      0 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 306813 entries, 0 to 1488480\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype         \n",
      "---  ------     --------------   -----         \n",
      " 0   timestamp  306813 non-null  datetime64[ns]\n",
      " 1   date       306813 non-null  datetime64[ns]\n",
      " 2   activity   306813 non-null  int64         \n",
      " 3   id         306813 non-null  object        \n",
      " 4   label      306813 non-null  int64         \n",
      "dtypes: datetime64[ns](2), int64(2), object(1)\n",
      "memory usage: 14.0+ MB\n",
      "None \n",
      "\n",
      "                  timestamp       date  activity           id  label\n",
      "307776  2002-10-04 11:01:00 2002-10-04       106    control_2      0\n",
      "1483793 2003-06-24 02:35:00 2003-06-24        72  condition_5      1\n",
      "14611   2003-03-28 18:31:00 2003-03-28       116    control_1      0\n",
      "803330  2002-11-12 13:43:00 2002-11-12         5    control_4      0\n",
      "1429946 2003-05-23 21:42:00 2003-05-23         5  condition_3      1\n",
      "1478019 2003-06-20 02:21:00 2003-06-20         9  condition_5      1\n",
      "37708   2003-04-13 20:28:00 2003-04-13         0    control_1      0\n",
      "596967  2002-11-11 06:24:00 2002-11-11       531    control_3      0\n",
      "598491  2002-11-12 07:48:00 2002-11-12         0    control_3      0\n",
      "628884  2002-12-03 10:21:00 2002-12-03         0    control_3      0 \n",
      "\n",
      "control_3      65407\n",
      "control_1      51611\n",
      "condition_2    38926\n",
      "control_2      31473\n",
      "control_4      31455\n",
      "condition_1    23244\n",
      "condition_3    21648\n",
      "condition_4    21556\n",
      "condition_5    21493\n",
      "Name: id, dtype: int64 \n",
      "\n",
      "0    179946\n",
      "1    126867\n",
      "Name: label, dtype: int64\n",
      "0    0.586501\n",
      "1    0.413499\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "control_subjects = ['control_1', 'control_2', 'control_3', 'control_4']\n",
    "condition_subjects = ['condition_1', 'condition_2', 'condition_3', 'condition_4', 'condition_5']\n",
    "\n",
    "# Filter for control subjects\n",
    "control_df = all_files[all_files['id'].isin(control_subjects)] \n",
    "\n",
    "# Filter for condition subjects\n",
    "condition_df = all_files[all_files['id'].isin(condition_subjects)]\n",
    "\n",
    "# Concatenate \n",
    "subset_df = pd.concat([control_df, condition_df])\n",
    "\n",
    "# print the first 5 rows\n",
    "print(subset_df.head(5), '\\n')\n",
    "\n",
    "# print info\n",
    "print(subset_df.info(), '\\n')\n",
    "\n",
    "# print random 10 rows\n",
    "print(subset_df.sample(10), '\\n')\n",
    "\n",
    "# print number of rows by 'id'\n",
    "print(subset_df['id'].value_counts(), '\\n')\n",
    "\n",
    "# print number of rows by 'label'\n",
    "print(subset_df['label'].value_counts())\n",
    "\n",
    "# print proportion of 'label' column\n",
    "print(subset_df['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample to hourly \n",
    "\n",
    "\"In the selection of the samples, only the first value of the 60 values acquired during 1 h was kept, equivalent to the minutes correspondent to that time lapse, counting now the activity in intervals of 1 h. This procedure was performed for each hour of the total data.\"\n",
    "\n",
    "-> take first value of each hour\n",
    "-> take mean of hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1 - first value on the hour (as per article?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               id           timestamp       date  activity  label\n",
      "0     condition_1 2003-05-07 12:00:00 2003-05-07         0      1\n",
      "1     condition_1 2003-05-07 13:00:00 2003-05-07       306      1\n",
      "2     condition_1 2003-05-07 14:00:00 2003-05-07         0      1\n",
      "3     condition_1 2003-05-07 15:00:00 2003-05-07        38      1\n",
      "4     condition_1 2003-05-07 16:00:00 2003-05-07        15      1\n",
      "...           ...                 ...        ...       ...    ...\n",
      "5115    control_4 2002-11-18 07:00:00 2002-11-18         5      0\n",
      "5116    control_4 2002-11-18 08:00:00 2002-11-18       106      0\n",
      "5117    control_4 2002-11-18 09:00:00 2002-11-18         3      0\n",
      "5118    control_4 2002-11-18 10:00:00 2002-11-18         3      0\n",
      "5119    control_4 2002-11-18 11:00:00 2002-11-18         0      0\n",
      "\n",
      "[5120 rows x 5 columns] \n",
      "\n",
      "0    0.586328\n",
      "1    0.413672\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Copy the subset_df DataFrame\n",
    "subset_first_hour_df = subset_df.copy()\n",
    "\n",
    "# Set 'timestamp' as index\n",
    "#subset_first_hour_df = subset_first_hour_df.set_index('timestamp')\n",
    "\n",
    "# Convert index to datetime \n",
    "#subset_first_hour_df.index = pd.to_datetime(subset_first_hour_df.index)\n",
    "\n",
    "# floor and resample\n",
    "#subset_first_hour_df.index = subset_first_hour_df.index.floor('H')\n",
    "#subset_first_hour_df = subset_first_hour_df.resample('H').first()\n",
    "\n",
    "# floor and resample\n",
    "subset_first_hour_df['timestamp'] = subset_first_hour_df['timestamp'].dt.floor('H')\n",
    "subset_first_hour_df = subset_first_hour_df.groupby(['id', 'timestamp']).first().reset_index()\n",
    "\n",
    "# print the first 5 rows\n",
    "print(subset_first_hour_df, '\\n')\n",
    "\n",
    "# print distribution of 'label' column\n",
    "print(subset_first_hour_df['label'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2 - mean of hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id           timestamp    activity  label\n",
      "0  condition_1 2003-05-07 12:00:00  346.550000    1.0\n",
      "1  condition_1 2003-05-07 13:00:00  284.566667    1.0\n",
      "2  condition_1 2003-05-07 14:00:00  279.183333    1.0\n",
      "3  condition_1 2003-05-07 15:00:00  218.783333    1.0\n",
      "4  condition_1 2003-05-07 16:00:00  238.550000    1.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_44496\\442291410.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  subset_mean_hour_df = subset_mean_hour_df.groupby(['id', 'timestamp']).mean().reset_index()\n"
     ]
    }
   ],
   "source": [
    "# copy dataframe\n",
    "subset_mean_hour_df = subset_df.copy()\n",
    "\n",
    "# resample to hourly mean\n",
    "subset_mean_hour_df['timestamp'] = subset_mean_hour_df['timestamp'].dt.floor('H')\n",
    "subset_mean_hour_df = subset_mean_hour_df.groupby(['id', 'timestamp']).mean().reset_index()\n",
    "\n",
    "# print the first 5 rows\n",
    "print(subset_mean_hour_df.head(5), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise subset (z-score)\n",
    "\n",
    "Could use: \n",
    "\n",
    "* `sklearn.preprocessing.scale` - Standardises features by removing the mean and scaling to unit variance (similar to manual z-score normalisation)\n",
    "* `sklearn.preprocessing.minmax_scale` - Transforms features to a given range (often 0-1 for minmax scaling)\n",
    "* `sklearn.preprocessing.normalize` - L2 vector normalisation\n",
    "\n",
    "* pandas -> df.normalize(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v1 first hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          activity        label  activity_norm\n",
      "count  5120.000000  5120.000000   5.120000e+03\n",
      "mean    176.685742     0.413672  -1.110223e-17\n",
      "std     364.604038     0.492539   1.000000e+00\n",
      "min       0.000000     0.000000  -4.845962e-01\n",
      "25%       0.000000     0.000000  -4.845962e-01\n",
      "50%       4.000000     0.000000  -4.736254e-01\n",
      "75%     184.000000     1.000000   2.006083e-02\n",
      "max    4009.000000     1.000000   1.051089e+01\n"
     ]
    }
   ],
   "source": [
    "# calculate mean and standard deviation\n",
    "mu = subset_first_hour_df['activity'].mean()\n",
    "sigma = subset_first_hour_df['activity'].std()\n",
    "\n",
    "# normalise\n",
    "subset_first_hour_df['activity_norm'] = (subset_first_hour_df['activity'] - mu)/sigma\n",
    "\n",
    "# print summary statistics\n",
    "print(subset_first_hour_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v2 hour mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          activity        label  activity_norm\n",
      "count  5120.000000  5120.000000   5.120000e+03\n",
      "mean    168.363036     0.413672  -2.220446e-17\n",
      "std     256.015579     0.492539   1.000000e+00\n",
      "min       0.000000     0.000000  -6.576281e-01\n",
      "25%       1.495833     0.000000  -6.517853e-01\n",
      "50%      19.616667     0.000000  -5.810051e-01\n",
      "75%     269.012500     1.000000   3.931380e-01\n",
      "max    2095.983333     1.000000   7.529309e+00\n"
     ]
    }
   ],
   "source": [
    "# calculate mean and standard deviation\n",
    "mu = subset_mean_hour_df['activity'].mean()\n",
    "sigma = subset_mean_hour_df['activity'].std()\n",
    "\n",
    "# normalise\n",
    "subset_mean_hour_df['activity_norm'] = (subset_mean_hour_df['activity'] - mu)/sigma\n",
    "\n",
    "# print summary statistics\n",
    "print(subset_mean_hour_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False \n",
      "\n",
      "id               0\n",
      "timestamp        0\n",
      "date             0\n",
      "activity         0\n",
      "label            0\n",
      "activity_norm    0\n",
      "dtype: int64 \n",
      "\n",
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# Check if dataframe has any NaN \n",
    "print(subset_first_hour_df.isnull().values.any(), '\\n')\n",
    "\n",
    "# Count number of NaN per column\n",
    "print(subset_first_hour_df.isnull().sum(), '\\n')\n",
    "\n",
    "# See indices of NaN values \n",
    "print(subset_first_hour_df[subset_first_hour_df.isnull().any(axis=1)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False \n",
      "\n",
      "id               0\n",
      "timestamp        0\n",
      "activity         0\n",
      "label            0\n",
      "activity_norm    0\n",
      "dtype: int64 \n",
      "\n",
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# Check if dataframe has any NaN \n",
    "print(subset_mean_hour_df.isnull().values.any(), '\\n')\n",
    "\n",
    "# Count number of NaN per column\n",
    "print(subset_mean_hour_df.isnull().sum(), '\\n')\n",
    "\n",
    "# See indices of NaN values \n",
    "print(subset_mean_hour_df[subset_mean_hour_df.isnull().any(axis=1)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3938 entries, 2002-10-02 15:00:00 to 2003-06-27 08:00:00\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   date           3938 non-null   datetime64[ns]\n",
      " 1   activity       3938 non-null   float64       \n",
      " 2   id             3938 non-null   object        \n",
      " 3   label          3938 non-null   float64       \n",
      " 4   activity_norm  3938 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(3), object(1)\n",
      "memory usage: 184.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# drop NaN values rows\n",
    "subset_hour_df.dropna(inplace=True)\n",
    "\n",
    "# Check if dataframe has any NaN\n",
    "print(subset_hour_df.isnull().values.any())\n",
    "\n",
    "# print info\n",
    "print(subset_hour_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features (14)\n",
    "\n",
    "**Mean**:\n",
    "\n",
    "* Calculation: Average value of the activity data.\n",
    "* Significance: Represents the central tendency of the data.\n",
    "\n",
    "**Trimmed Standard Deviation:**\n",
    "* Calculation: A measure of the amount of variation in the data, with a specified percentage of outliers removed.\n",
    "* Significance: More robust to outliers than the regular standard deviation.\n",
    "\n",
    "**Trimmed Variance:**\n",
    "\n",
    "* Calculation: Similar to trimmed standard deviation but squared, providing a measure of the spread of the data.\n",
    "* Significance: Robust measure of data spread with specified outliers removed.\n",
    "\n",
    "**Quantiles (1st, 5th, 25th, 75th, 95th, 99th):**\n",
    "\n",
    "* Calculation: Values below which a given percentage of the data falls.\n",
    "* Significance: Describes the distribution of the data and helps identify potential outliers.\n",
    "\n",
    "**Skewness:**\n",
    "\n",
    "* Calculation: A measure of the asymmetry of the data distribution.\n",
    "* Significance: Positive skewness indicates a right-skewed distribution (tail to the right), negative skewness indicates a left-skewed distribution (tail to the left).\n",
    "\n",
    "**Kurtosis:**\n",
    "\n",
    "* Calculation: A measure of the \"tailedness\" of the data distribution.\n",
    "* Significance: High kurtosis indicates heavy tails and more outliers compared to a normal distribution. Low kurtosis indicates light tails.\n",
    "\n",
    "**Coefficient of Variation (Coef_Var):**\n",
    "\n",
    "* Calculation: Standard deviation divided by the mean, expressing the relative variability of the data.\n",
    "* Significance: Useful for comparing the variability of datasets with different units or scales.\n",
    "\n",
    "**Inverse Coefficient of Variation (Inverse_Coef_Var):**\n",
    "\n",
    "* Calculation: The reciprocal of the coefficient of variation.\n",
    "* Significance: Measures the efficiency of data representation; a higher value suggests more efficient data representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'timestamp', 'date', 'activity', 'label', 'activity_norm'], dtype='object') \n",
      "\n",
      "            id           timestamp       date  activity  label  activity_norm\n",
      "0  condition_1 2003-05-07 12:00:00 2003-05-07         0      1      -0.484596\n",
      "1  condition_1 2003-05-07 13:00:00 2003-05-07       306      1       0.354670\n",
      "2  condition_1 2003-05-07 14:00:00 2003-05-07         0      1      -0.484596\n",
      "3  condition_1 2003-05-07 15:00:00 2003-05-07        38      1      -0.380374\n",
      "4  condition_1 2003-05-07 16:00:00 2003-05-07        15      1      -0.443456 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5120 entries, 0 to 5119\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   id             5120 non-null   object        \n",
      " 1   timestamp      5120 non-null   datetime64[ns]\n",
      " 2   date           5120 non-null   datetime64[ns]\n",
      " 3   activity       5120 non-null   int64         \n",
      " 4   label          5120 non-null   int64         \n",
      " 5   activity_norm  5120 non-null   float64       \n",
      "dtypes: datetime64[ns](2), float64(1), int64(2), object(1)\n",
      "memory usage: 240.1+ KB\n",
      "None \n",
      "\n",
      "Index(['id', 'timestamp', 'activity', 'label', 'activity_norm'], dtype='object') \n",
      "\n",
      "            id           timestamp    activity  label  activity_norm\n",
      "0  condition_1 2003-05-07 12:00:00  346.550000    1.0       0.696000\n",
      "1  condition_1 2003-05-07 13:00:00  284.566667    1.0       0.453893\n",
      "2  condition_1 2003-05-07 14:00:00  279.183333    1.0       0.432865\n",
      "3  condition_1 2003-05-07 15:00:00  218.783333    1.0       0.196942\n",
      "4  condition_1 2003-05-07 16:00:00  238.550000    1.0       0.274151 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5120 entries, 0 to 5119\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   id             5120 non-null   object        \n",
      " 1   timestamp      5120 non-null   datetime64[ns]\n",
      " 2   activity       5120 non-null   float64       \n",
      " 3   label          5120 non-null   float64       \n",
      " 4   activity_norm  5120 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(3), object(1)\n",
      "memory usage: 200.1+ KB\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(subset_first_hour_df.columns, '\\n')\n",
    "print(subset_first_hour_df.head(5), '\\n')\n",
    "print(subset_first_hour_df.info(), '\\n')\n",
    "print(subset_mean_hour_df.columns, '\\n')\n",
    "print(subset_mean_hour_df.head(5), '\\n')\n",
    "print(subset_mean_hour_df.info(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRAPS - to be deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v1 first hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       mean  std  variance  quantile_01  quantile_05  quantile_25  \\\n",
      "0 -0.484596  0.0       0.0    -0.484596    -0.484596    -0.484596   \n",
      "1  0.354670  0.0       0.0     0.354670     0.354670     0.354670   \n",
      "2 -0.484596  0.0       0.0    -0.484596    -0.484596    -0.484596   \n",
      "3 -0.380374  0.0       0.0    -0.380374    -0.380374    -0.380374   \n",
      "4 -0.443456  0.0       0.0    -0.443456    -0.443456    -0.443456   \n",
      "\n",
      "   quantile_75  quantile_95  quantile_99 skewness  kurtosis  coef_var  \\\n",
      "0    -0.484596    -0.484596    -0.484596      0.0      -3.0      -0.0   \n",
      "1     0.354670     0.354670     0.354670      0.0      -3.0       0.0   \n",
      "2    -0.484596    -0.484596    -0.484596      0.0      -3.0      -0.0   \n",
      "3    -0.380374    -0.380374    -0.380374      0.0      -3.0      -0.0   \n",
      "4    -0.443456    -0.443456    -0.443456      0.0      -3.0      -0.0   \n",
      "\n",
      "   inverse_coef_var  \n",
      "0     -4.845962e+08  \n",
      "1      3.546704e+08  \n",
      "2     -4.845962e+08  \n",
      "3     -3.803736e+08  \n",
      "4     -4.434557e+08   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby to calculate other features\n",
    "features_first_hour = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].agg([np.mean,\n",
    "                                lambda x: mstats.trimmed_std(x, 0.1),  # Adjust the trim parameter as needed\n",
    "                                lambda x: mstats.trimmed_var(x, 0.1),\n",
    "                                lambda x: np.quantile(x, 0.01), \n",
    "                                lambda x: np.quantile(x, 0.05),\n",
    "                                lambda x: np.quantile(x, 0.25),\n",
    "                                lambda x: np.quantile(x, 0.75),\n",
    "                                lambda x: np.quantile(x, 0.95),\n",
    "                                lambda x: np.quantile(x, 0.99)]) \n",
    "\n",
    "features_first_hour.columns = ['mean', 'std', 'variance', 'quantile_01', 'quantile_05', 'quantile_25', 'quantile_75', 'quantile_95', 'quantile_99']\n",
    "\n",
    "# Calculate skewness and kurtosis separately\n",
    "skewness_values = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].apply(mstats.skew)\n",
    "kurtosis_values = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].apply(mstats.kurtosis)\n",
    "\n",
    "# Assign skewness and kurtosis to the dataframe\n",
    "features_first_hour['skewness'] = skewness_values\n",
    "features_first_hour['kurtosis'] = kurtosis_values\n",
    "\n",
    "# Calculate remaining features\n",
    "features_first_hour['coef_var'] = features_first_hour['std'] / features_first_hour['mean'] \n",
    "features_first_hour['inverse_coef_var'] = features_first_hour['mean'] / (np.abs(features_first_hour['std']) + 1e-9)\n",
    "\n",
    "\n",
    "# print the first 5 rows\n",
    "print(features_first_hour.head(5), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v2 mean hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       mean  std  variance  quantile_01  quantile_05  quantile_25  \\\n",
      "0  0.696000  0.0       0.0     0.696000     0.696000     0.696000   \n",
      "1  0.453893  0.0       0.0     0.453893     0.453893     0.453893   \n",
      "2  0.432865  0.0       0.0     0.432865     0.432865     0.432865   \n",
      "3  0.196942  0.0       0.0     0.196942     0.196942     0.196942   \n",
      "4  0.274151  0.0       0.0     0.274151     0.274151     0.274151   \n",
      "\n",
      "   quantile_75  quantile_95  quantile_99 skewness  kurtosis  coef_var  \\\n",
      "0     0.696000     0.696000     0.696000      0.0      -3.0       0.0   \n",
      "1     0.453893     0.453893     0.453893      0.0      -3.0       0.0   \n",
      "2     0.432865     0.432865     0.432865      0.0      -3.0       0.0   \n",
      "3     0.196942     0.196942     0.196942      0.0      -3.0       0.0   \n",
      "4     0.274151     0.274151     0.274151      0.0      -3.0       0.0   \n",
      "\n",
      "   inverse_coef_var  \n",
      "0      6.960005e+08  \n",
      "1      4.538928e+08  \n",
      "2      4.328654e+08  \n",
      "3      1.969423e+08  \n",
      "4      2.741511e+08   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby to calculate other features\n",
    "features_mean_hour = subset_mean_hour_df.groupby(subset_mean_hour_df.index)['activity_norm'].agg([np.mean,\n",
    "                                lambda x: mstats.trimmed_std(x, 0.1),  # Adjust the trim parameter as needed\n",
    "                                lambda x: mstats.trimmed_var(x, 0.1),\n",
    "                                lambda x: np.quantile(x, 0.01), \n",
    "                                lambda x: np.quantile(x, 0.05),\n",
    "                                lambda x: np.quantile(x, 0.25),\n",
    "                                lambda x: np.quantile(x, 0.75),\n",
    "                                lambda x: np.quantile(x, 0.95),\n",
    "                                lambda x: np.quantile(x, 0.99)]) \n",
    "\n",
    "features_mean_hour.columns = ['mean', 'std', 'variance', 'quantile_01', 'quantile_05', 'quantile_25', 'quantile_75', 'quantile_95', 'quantile_99']\n",
    "\n",
    "# skewness and kurtosis \n",
    "skewness_values = subset_mean_hour_df.groupby(subset_mean_hour_df.index)['activity_norm'].apply(mstats.skew)\n",
    "kurtosis_values = subset_mean_hour_df.groupby(subset_mean_hour_df.index)['activity_norm'].apply(mstats.kurtosis)\n",
    "\n",
    "# assign skewness and kurtosis to the dataframe\n",
    "features_mean_hour['skewness'] = skewness_values\n",
    "features_mean_hour['kurtosis'] = kurtosis_values\n",
    "\n",
    "# coef and inverse_coef features\n",
    "features_mean_hour['coef_var'] = features_mean_hour['std'] / features_mean_hour['mean'] \n",
    "features_mean_hour['inverse_coef_var'] = features_mean_hour['mean'] / (np.abs(features_mean_hour['std']) + 1e-9)\n",
    "\n",
    "\n",
    "# print the first 5 rows\n",
    "print(features_mean_hour.head(5), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5120, 13)\n",
      "(5120, 13)\n"
     ]
    }
   ],
   "source": [
    "print(features_first_hour.shape)\n",
    "print(features_mean_hour.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zoona\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1649: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  f = lambda x: func(x, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       mean  std  variance  quantile_01  quantile_05  quantile_25  \\\n",
      "0 -0.484596  NaN       NaN    -0.484596    -0.484596    -0.484596   \n",
      "1  0.354670  NaN       NaN     0.354670     0.354670     0.354670   \n",
      "2 -0.484596  NaN       NaN    -0.484596    -0.484596    -0.484596   \n",
      "3 -0.380374  NaN       NaN    -0.380374    -0.380374    -0.380374   \n",
      "4 -0.443456  NaN       NaN    -0.443456    -0.443456    -0.443456   \n",
      "\n",
      "   quantile_75  quantile_95  quantile_99  skewness  kurtosis  coef_var  \\\n",
      "0    -0.484596    -0.484596    -0.484596       NaN       NaN       NaN   \n",
      "1     0.354670     0.354670     0.354670       NaN       NaN       NaN   \n",
      "2    -0.484596    -0.484596    -0.484596       NaN       NaN       NaN   \n",
      "3    -0.380374    -0.380374    -0.380374       NaN       NaN       NaN   \n",
      "4    -0.443456    -0.443456    -0.443456       NaN       NaN       NaN   \n",
      "\n",
      "   inverse_coef_var  \n",
      "0               NaN  \n",
      "1               NaN  \n",
      "2               NaN  \n",
      "3               NaN  \n",
      "4               NaN   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_first_hour = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].agg([np.mean, np.std, np.var, \n",
    "                                lambda x: np.quantile(x, 0.01), \n",
    "                                lambda x: np.quantile(x, 0.05),\n",
    "                                lambda x: np.quantile(x, 0.25),\n",
    "                                lambda x: np.quantile(x, 0.75),\n",
    "                                lambda x: np.quantile(x, 0.95),\n",
    "                                lambda x: np.quantile(x, 0.99),\n",
    "                                skew, kurtosis]) \n",
    "features_first_hour.columns = ['mean', 'std', 'variance', 'quantile_01', 'quantile_05', 'quantile_25', 'quantile_75', 'quantile_95', 'quantile_99','skewness', 'kurtosis']\n",
    "\n",
    "features_first_hour['coef_var'] = features_first_hour['std'] / features_first_hour['mean'] \n",
    "features_first_hour['inverse_coef_var'] = features_first_hour['mean'] / features_first_hour['std']\n",
    "\n",
    "# print the first 5 rows\n",
    "print(features_first_hour.head(5), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v1 first hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_44496\\3856034703.py:20: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  kurtosis_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].apply(lambda x: kurtosis(x))\n",
      "C:\\Users\\zoona\\AppData\\Local\\Temp\\ipykernel_44496\\3856034703.py:23: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  skewness_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].apply(lambda x: skew(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          mean  std  variance  trimmed_mean  coef_var  inverse_coef_var  \\\n",
      "0    -0.484596  NaN       NaN     -0.484596       NaN               NaN   \n",
      "1     0.354670  NaN       NaN      0.354670       NaN               NaN   \n",
      "2    -0.484596  NaN       NaN     -0.484596       NaN               NaN   \n",
      "3    -0.380374  NaN       NaN     -0.380374       NaN               NaN   \n",
      "4    -0.443456  NaN       NaN     -0.443456       NaN               NaN   \n",
      "...        ...  ...       ...           ...       ...               ...   \n",
      "5115 -0.470883  NaN       NaN     -0.470883       NaN               NaN   \n",
      "5116 -0.193870  NaN       NaN     -0.193870       NaN               NaN   \n",
      "5117 -0.476368  NaN       NaN     -0.476368       NaN               NaN   \n",
      "5118 -0.476368  NaN       NaN     -0.476368       NaN               NaN   \n",
      "5119 -0.484596  NaN       NaN     -0.484596       NaN               NaN   \n",
      "\n",
      "      kurtosis  skewness  quantile_01  quantile_05  quantile_25  quantile_75  \\\n",
      "0          NaN       NaN    -0.484596    -0.484596    -0.484596    -0.484596   \n",
      "1          NaN       NaN     0.354670     0.354670     0.354670     0.354670   \n",
      "2          NaN       NaN    -0.484596    -0.484596    -0.484596    -0.484596   \n",
      "3          NaN       NaN    -0.380374    -0.380374    -0.380374    -0.380374   \n",
      "4          NaN       NaN    -0.443456    -0.443456    -0.443456    -0.443456   \n",
      "...        ...       ...          ...          ...          ...          ...   \n",
      "5115       NaN       NaN    -0.470883    -0.470883    -0.470883    -0.470883   \n",
      "5116       NaN       NaN    -0.193870    -0.193870    -0.193870    -0.193870   \n",
      "5117       NaN       NaN    -0.476368    -0.476368    -0.476368    -0.476368   \n",
      "5118       NaN       NaN    -0.476368    -0.476368    -0.476368    -0.476368   \n",
      "5119       NaN       NaN    -0.484596    -0.484596    -0.484596    -0.484596   \n",
      "\n",
      "      quantile_95  quantile_99  \n",
      "0       -0.484596    -0.484596  \n",
      "1        0.354670     0.354670  \n",
      "2       -0.484596    -0.484596  \n",
      "3       -0.380374    -0.380374  \n",
      "4       -0.443456    -0.443456  \n",
      "...           ...          ...  \n",
      "5115    -0.470883    -0.470883  \n",
      "5116    -0.193870    -0.193870  \n",
      "5117    -0.476368    -0.476368  \n",
      "5118    -0.476368    -0.476368  \n",
      "5119    -0.484596    -0.484596  \n",
      "\n",
      "[5120 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Mean\n",
    "mean_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].mean()\n",
    "\n",
    "# Calculate Standard Deviation\n",
    "std_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].std()\n",
    "\n",
    "# Calculate Variance\n",
    "variance_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].var()\n",
    "\n",
    "# Calculate Trimmed Mean (using 5% trimming as an example)\n",
    "trimmed_mean_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].apply(lambda x: np.mean(x[(x >= np.percentile(x, 5)) & (x <= np.percentile(x, 95))]))\n",
    "\n",
    "# Calculate Coefficient of Variation\n",
    "coef_var_value = std_value / mean_value\n",
    "\n",
    "# Calculate Inverse Coefficient of Variation\n",
    "inverse_coef_var_value = mean_value / std_value\n",
    "\n",
    "# Calculate Kurtosis\n",
    "kurtosis_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].apply(lambda x: kurtosis(x))\n",
    "\n",
    "# Calculate Skewness\n",
    "skewness_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].apply(lambda x: skew(x))\n",
    "\n",
    "# Calculate Quantiles (1%, 5%, 25%, 75%, 95%, 99%)\n",
    "quantiles_value = subset_first_hour_df.groupby(subset_first_hour_df.index)['activity_norm'].quantile([0.01, 0.05, 0.25, 0.75, 0.95, 0.99]).unstack(level=-1)\n",
    "\n",
    "# Create a dataframe to store the features\n",
    "features_first_hour = pd.DataFrame({\n",
    "    'mean': mean_value,\n",
    "    'std': std_value,\n",
    "    'variance': variance_value,\n",
    "    'trimmed_mean': trimmed_mean_value,\n",
    "    'coef_var': coef_var_value,\n",
    "    'inverse_coef_var': inverse_coef_var_value,\n",
    "    'kurtosis': kurtosis_value,\n",
    "    'skewness': skewness_value\n",
    "})\n",
    "\n",
    "# Add Quantiles to the dataframe\n",
    "features_first_hour = pd.concat([features_first_hour, quantiles_value], axis=1)\n",
    "\n",
    "# Rename columns for clarity\n",
    "features_first_hour.columns = ['mean', 'std', 'variance', 'trimmed_mean', 'coef_var', 'inverse_coef_var', 'kurtosis', 'skewness', 'quantile_01', 'quantile_05', 'quantile_25', 'quantile_75', 'quantile_95', 'quantile_99']\n",
    "\n",
    "# Print or further use features_hour\n",
    "print(features_first_hour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate features per time window\n",
    "features = []\n",
    "for idx, df_window in df.groupby(df.index): # grouped by time window\n",
    "    features_dict = {\n",
    "        'mean': df_window['activity_norm'].mean(),\n",
    "        'std': df_window['activity_norm'].std(),\n",
    "        'variance': df_window['activity_norm'].var(),\n",
    "        'variance': df_window['activity_norm'].var(),\n",
    "        'trimmed_mean': df_window['activity_norm'].quantile(0.05),\n",
    "        'coef_var': df_window['activity_norm'].std() / df_window['activity_norm'].mean(),\n",
    "        'inverse_coef_var': df_window['activity_norm'].mean() / df_window['activity_norm'].std(),\n",
    "        'kurtosis': df_window['activity_norm'].kurtosis(),\n",
    "        'skewness': df_window['activity_norm'].skew(),\n",
    "        'quantile_1': df_window['activity_norm'].quantile(0.01),\n",
    "        'quantile_5': df_window['activity_norm'].quantile(0.05),\n",
    "        'quantile_25': df_window['activity_norm'].quantile(0.25),\n",
    "        'quantile_75': df_window['activity_norm'].quantile(0.75),\n",
    "        'quantile_95': df_window['activity_norm'].quantile(0.95),\n",
    "        'quantile_99': df_window['activity_norm'].quantile(0.99)\n",
    "    }\n",
    "    \n",
    "    features.append(features_dict) \n",
    "\n",
    "features_df = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         mean  std  variance  quantile_01  quantile_05  \\\n",
      "timestamp                                                                \n",
      "2002-10-02 15:00:00  0.030959  NaN       NaN     0.030959     0.030959   \n",
      "2002-10-02 16:00:00  1.656566  NaN       NaN     1.656566     1.656566   \n",
      "2002-10-02 17:00:00  2.274023  NaN       NaN     2.274023     2.274023   \n",
      "2002-10-02 18:00:00  3.926951  NaN       NaN     3.926951     3.926951   \n",
      "2002-10-02 19:00:00  1.126536  NaN       NaN     1.126536     1.126536   \n",
      "\n",
      "                     quantile_25  quantile_75  quantile_95  quantile_99  \\\n",
      "timestamp                                                                 \n",
      "2002-10-02 15:00:00     0.030959     0.030959     0.030959     0.030959   \n",
      "2002-10-02 16:00:00     1.656566     1.656566     1.656566     1.656566   \n",
      "2002-10-02 17:00:00     2.274023     2.274023     2.274023     2.274023   \n",
      "2002-10-02 18:00:00     3.926951     3.926951     3.926951     3.926951   \n",
      "2002-10-02 19:00:00     1.126536     1.126536     1.126536     1.126536   \n",
      "\n",
      "                     skewness  kurtosis  coef_var  inverse_coef_var  \n",
      "timestamp                                                            \n",
      "2002-10-02 15:00:00       NaN       NaN       NaN               NaN  \n",
      "2002-10-02 16:00:00       NaN       NaN       NaN               NaN  \n",
      "2002-10-02 17:00:00       NaN       NaN       NaN               NaN  \n",
      "2002-10-02 18:00:00       NaN       NaN       NaN               NaN  \n",
      "2002-10-02 19:00:00       NaN       NaN       NaN               NaN  \n"
     ]
    }
   ],
   "source": [
    "print(features_hour.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
