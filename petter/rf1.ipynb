{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - baseline\n",
    "\n",
    "* Referencing: \n",
    "> **Bibliography:** Zanella-Calzada, L.A., Galván-Tejada, C.E., Chávez-Lamas, N.M., Gracia-Cortés, M. del C., Magallanes-Quintanar, R., Celaya-Padilla, J.M., Galván-Tejada, J.I. and Gamboa-Rosales, H. (2019) Feature Extraction in Motor Activity Signal: Towards a Depression Episodes Detection in Unipolar and Bipolar Patients. Diagnostics [online]. 9 (1), p. 8. Available from: https://www.mdpi.com/2075-4418/9/1/8 [Accessed 28 November 2023].\n",
    "* [article notes](../literature/Zanella-FeatureExtraction.md)\n",
    "\n",
    "\n",
    "## Plan \n",
    "\n",
    "1. Load and process `depresjon`\n",
    "   * load into pandas df\n",
    "   * select `control` and `condition` -> it seems that they used first 4 control and first 5 condition participants\n",
    "   * normalise data (mean = 0, std = 1)\n",
    "   * remove incomplete cases\n",
    "\n",
    "2. Extract features - 14 features\n",
    "   * mean\n",
    "   * standard deviation\n",
    "   * variance\n",
    "   * trimmed mean\n",
    "   * coefficient of variation\n",
    "   * inversse coefficient of variation\n",
    "   * kurtosis\n",
    "   * skewness\n",
    "   * quantailes (1, 5, 25, 75, 95, 99)\n",
    "\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data\n",
    "\n",
    "### Load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_folder(folderpath, add_scores=False, downsample=None):\n",
    "    \"\"\"\n",
    "    Extract CSV data from folder and subfolders into a dataframe.\n",
    "\n",
    "    Args:\n",
    "      folderpath (str): Path to the folder containing CSV files.\n",
    "      add_scores (bool, optional): Boolean to add scores.csv to the dataframe. Defaults to False.\n",
    "      downsample (int, optional): Number of rows to downsample CSVs to. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "      pandas.DataFrame: DataFrame of concatenated CSV data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dict to store dataframes by condition  \n",
    "    dfs = {'control': [], 'condition': []}\n",
    "\n",
    "    try:\n",
    "        # Handle top-level scores CSV\n",
    "        if add_scores and 'scores.csv' in os.listdir(folderpath):\n",
    "            scores_path = os.path.join(folderpath, 'scores.csv')  \n",
    "            dfs['scores'] = pd.read_csv(scores_path)\n",
    "\n",
    "        # Get subfolders\n",
    "        subfolders = [f for f in os.listdir(folderpath) if os.path.isdir(os.path.join(folderpath, f))]\n",
    "\n",
    "        for subfolder in subfolders:\n",
    "            subfolderpath = os.path.join(folderpath, subfolder)  \n",
    "\n",
    "            # Get list of CSV files\n",
    "            files = os.listdir(subfolderpath)\n",
    "\n",
    "            for file in files:\n",
    "                filepath = os.path.join(subfolderpath, file)\n",
    "\n",
    "                # Extract ID from filename \n",
    "                id = file.split('.')[0]\n",
    "\n",
    "                df = pd.read_csv(filepath)\n",
    "\n",
    "                # Downsample if needed\n",
    "                if downsample:\n",
    "                    df = df.sample(downsample)\n",
    "\n",
    "                # Add ID column - this is the filename without the extension\n",
    "                df['id'] = id\n",
    "\n",
    "                # Add 'condition' column\n",
    "                df['condition'] = subfolder\n",
    "\n",
    "                # Convert 'timestamp' and 'date' to datetime\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "                # Append to dict by condition\n",
    "                if subfolder == 'control':\n",
    "                    dfs['control'].append(df)\n",
    "                else:  \n",
    "                    dfs['condition'].append(df)\n",
    "\n",
    "    except OSError:\n",
    "        print(f\"Error reading folder: {folderpath}\")\n",
    "\n",
    "    # concatenate dfs for each condition\n",
    "    dfs['control'] = pd.concat(dfs['control'])\n",
    "    dfs['condition'] = pd.concat(dfs['condition'])\n",
    "\n",
    "    # Reset index on the final df\n",
    "    df = pd.concat([dfs['control'], dfs['condition']]).reset_index(drop=True)\n",
    "\n",
    "    # add label column\n",
    "    df['label'] = 0\n",
    "    df.loc[df['condition'] == 'condition', 'label'] = 1\n",
    "    \n",
    "    # remove old 'condition' column\n",
    "    df.drop('condition', axis=1, inplace=True)\n",
    "\n",
    "    # Final concat\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  timestamp       date  activity           id  label\n",
      "0       2003-03-18 15:00:00 2003-03-18        60    control_1      0\n",
      "1       2003-03-18 15:01:00 2003-03-18         0    control_1      0\n",
      "2       2003-03-18 15:02:00 2003-03-18       264    control_1      0\n",
      "3       2003-03-18 15:03:00 2003-03-18       662    control_1      0\n",
      "4       2003-03-18 15:04:00 2003-03-18       293    control_1      0\n",
      "...                     ...        ...       ...          ...    ...\n",
      "1571696 2004-06-10 14:58:00 2004-06-10         0  condition_9      1\n",
      "1571697 2004-06-10 14:59:00 2004-06-10         0  condition_9      1\n",
      "1571698 2004-06-10 15:00:00 2004-06-10         0  condition_9      1\n",
      "1571699 2004-06-10 15:01:00 2004-06-10         5  condition_9      1\n",
      "1571700 2004-06-10 15:02:00 2004-06-10         0  condition_9      1\n",
      "\n",
      "[1571701 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# set folder path\n",
    "folderpath = '../data/depresjon/'\n",
    "# extract all files\n",
    "all_files = extract_folder(folderpath)\n",
    "# print rows 21-24\n",
    "#print(all_files.iloc[21:25])\n",
    "print(all_files.head(-5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp       date  activity         id  label\n",
      "0 2003-03-18 15:00:00 2003-03-18        60  control_1      0\n",
      "1 2003-03-18 15:01:00 2003-03-18         0  control_1      0\n",
      "2 2003-03-18 15:02:00 2003-03-18       264  control_1      0\n",
      "3 2003-03-18 15:03:00 2003-03-18       662  control_1      0\n",
      "4 2003-03-18 15:04:00 2003-03-18       293  control_1      0 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 306813 entries, 0 to 1488480\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype         \n",
      "---  ------     --------------   -----         \n",
      " 0   timestamp  306813 non-null  datetime64[ns]\n",
      " 1   date       306813 non-null  datetime64[ns]\n",
      " 2   activity   306813 non-null  int64         \n",
      " 3   id         306813 non-null  object        \n",
      " 4   label      306813 non-null  int64         \n",
      "dtypes: datetime64[ns](2), int64(2), object(1)\n",
      "memory usage: 14.0+ MB\n",
      "None \n",
      "\n",
      "                  timestamp       date  activity           id  label\n",
      "1293564 2003-05-13 09:17:00 2003-05-13      1073  condition_2      1\n",
      "1041287 2003-05-22 06:57:00 2003-05-22         0  condition_1      1\n",
      "312855  2002-10-07 23:40:00 2002-10-07       103    control_2      0\n",
      "320968  2002-10-13 14:53:00 2002-10-13         0    control_2      0\n",
      "45465   2003-04-19 05:45:00 2003-04-19         3    control_1      0\n",
      "1483038 2003-06-23 14:00:00 2003-06-23       354  condition_5      1\n",
      "1471231 2003-06-15 09:13:00 2003-06-15         0  condition_5      1\n",
      "1469805 2003-06-14 09:27:00 2003-06-14        18  condition_5      1\n",
      "47020   2003-04-20 07:40:00 2003-04-20         3    control_1      0\n",
      "27851   2003-04-07 00:11:00 2003-04-07         0    control_1      0 \n",
      "\n",
      "control_3      65407\n",
      "control_1      51611\n",
      "condition_2    38926\n",
      "control_2      31473\n",
      "control_4      31455\n",
      "condition_1    23244\n",
      "condition_3    21648\n",
      "condition_4    21556\n",
      "condition_5    21493\n",
      "Name: id, dtype: int64 \n",
      "\n",
      "0    179946\n",
      "1    126867\n",
      "Name: label, dtype: int64\n",
      "0    0.586501\n",
      "1    0.413499\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "control_subjects = ['control_1', 'control_2', 'control_3', 'control_4']\n",
    "condition_subjects = ['condition_1', 'condition_2', 'condition_3', 'condition_4', 'condition_5']\n",
    "\n",
    "# Filter for control subjects\n",
    "control_df = all_files[all_files['id'].isin(control_subjects)] \n",
    "\n",
    "# Filter for condition subjects\n",
    "condition_df = all_files[all_files['id'].isin(condition_subjects)]\n",
    "\n",
    "# Concatenate \n",
    "df = pd.concat([control_df, condition_df])\n",
    "\n",
    "# print the first 5 rows\n",
    "print(df.head(5), '\\n')\n",
    "\n",
    "# print info\n",
    "print(df.info(), '\\n')\n",
    "\n",
    "# print random 10 rows\n",
    "print(df.sample(10), '\\n')\n",
    "\n",
    "# print number of rows by 'id'\n",
    "print(df['id'].value_counts(), '\\n')\n",
    "\n",
    "# print number of rows by 'label'\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# print proportion of 'label' column\n",
    "print(df['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise subset (z-score)\n",
    "\n",
    "Could use: \n",
    "\n",
    "* `sklearn.preprocessing.scale` - Standardises features by removing the mean and scaling to unit variance (similar to manual z-score normalisation)\n",
    "* `sklearn.preprocessing.minmax_scale` - Transforms features to a given range (often 0-1 for minmax scaling)\n",
    "* `sklearn.preprocessing.normalize` - L2 vector normalisation\n",
    "\n",
    "* pandas -> df.normalize(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            activity          label  activity_norm\n",
      "count  306813.000000  306813.000000   3.068130e+05\n",
      "mean      168.451369       0.413499  -2.501153e-17\n",
      "std       356.899613       0.492462   1.000000e+00\n",
      "min         0.000000       0.000000  -4.719853e-01\n",
      "25%         0.000000       0.000000  -4.719853e-01\n",
      "50%         3.000000       0.000000  -4.635796e-01\n",
      "75%       160.000000       1.000000  -2.367996e-02\n",
      "max      6776.000000       1.000000   1.851375e+01\n"
     ]
    }
   ],
   "source": [
    "# calculate mean and standard deviation\n",
    "mu = df['activity'].mean()\n",
    "sigma = df['activity'].std()\n",
    "\n",
    "# normalise\n",
    "df['activity_norm'] = (df['activity'] - mu)/sigma\n",
    "\n",
    "# print summary statistics\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "timestamp        0\n",
      "date             0\n",
      "activity         0\n",
      "id               0\n",
      "label            0\n",
      "activity_norm    0\n",
      "dtype: int64\n",
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# Check if dataframe has any NaN \n",
    "print(df.isnull().values.any())\n",
    "\n",
    "# Count number of NaN per column\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# See indices of NaN values \n",
    "print(df[df.isnull().any(axis=1)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features (14)\n",
    "\n",
    "\n",
    "TODO - can this be made more efficient\n",
    "fix the above to take the first value from each hour for each respondent - too much data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate features per time window\n",
    "features = []\n",
    "for idx, df_window in df.groupby(df.index): # grouped by time window\n",
    "    features_dict = {\n",
    "        'mean': df_window['activity_norm'].mean(),\n",
    "        'std': df_window['activity_norm'].std(),\n",
    "        'variance': df_window['activity_norm'].var(),\n",
    "        'variance': df_window['activity_norm'].var(),\n",
    "        'trimmed_mean': df_window['activity_norm'].quantile(0.05),\n",
    "        'coef_var': df_window['activity_norm'].std() / df_window['activity_norm'].mean(),\n",
    "        'inverse_coef_var': df_window['activity_norm'].mean() / df_window['activity_norm'].std(),\n",
    "        'kurtosis': df_window['activity_norm'].kurtosis(),\n",
    "        'skewness': df_window['activity_norm'].skew(),\n",
    "        'quantile_1': df_window['activity_norm'].quantile(0.01),\n",
    "        'quantile_5': df_window['activity_norm'].quantile(0.05),\n",
    "        'quantile_25': df_window['activity_norm'].quantile(0.25),\n",
    "        'quantile_75': df_window['activity_norm'].quantile(0.75),\n",
    "        'quantile_95': df_window['activity_norm'].quantile(0.95),\n",
    "        'quantile_99': df_window['activity_norm'].quantile(0.99)\n",
    "    }\n",
    "    \n",
    "    features.append(features_dict) \n",
    "\n",
    "features_df = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 306813 entries, 0 to 306812\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   mean      306813 non-null  float64\n",
      " 1   std       0 non-null       float64\n",
      " 2   variance  0 non-null       float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 7.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(features_df.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
